\documentclass[twoside,english]{uiofysmaster}
\usepackage[T1]{fontenc} %for å bruke æøå
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage[font=scriptsize]{caption}
\usepackage{subcaption}
\usepackage{mdwlist}
\usepackage[toc,page]{appendix}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{mathtools}

\newcommand*\dif{\mathop{}\!\mathrm{d}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\bibliographystyle{ieeetr}
%\bibliography{references}

\author{Vilde Moe Flugsrud}
\title{Solving Quantum Mechanical Problems with Machine Learning}
\date{June 2018}

\begin{document}

\maketitle

\begin{abstract}
This is an abstract text.
\end{abstract}

\begin{acknowledgements}
  I acknowledge my acknowledgements.
\end{acknowledgements}

\tableofcontents

\chapter{Introduction}

Start your chapter by writing something smart. Then go get coffee.

\part{Theory}
\chapter{The Quantum Many-Body Problem}

\section{Many-Body Quantum Mechanics}
When describing systems at the size of atoms and electrons, at speeds where relativistic effects are regarded as negligible, quantum mechanics is the theory used to describe the system. 
While in classical mechanics the state of a system of $N$ particles is described by each particle's position and momentum, in quantum mechanics it is described by the complex valued wavefunction 
In quantum mechanics, the state of a system is defined by the complex-valued wavefunction $\Psi$, which is an element of an infinite dimensional Hilbert space. That is, a complete vector space with an inner product.\cite{Kvaal2017}. 

The expectation value of an operator $\hat{\bm{O}}$ for a system of $N$ particles is given as \cite{HjortJensen2015}
\begin{align}
	%\langle \hat{\bm{O}} \rangle
	=& \frac{\int \Psi^{\ast}(\bm{x}_1,...,\bm{x}_N) \hat{\bm{O}}(\bm{x}_1,...,\bm{x}_N)
	\Psi (\bm{x}_1,...,\bm{x}_N)
	\dif \bm{x}_1 ... \dif \bm{x}_N  }
	{\int \Psi^{\ast}(\bm{x}_1,...,\bm{x}_N)
	\Psi (\bm{x}_1,...,\bm{x}_N)
	\dif \bm{x}_1 ... \dif \bm{x}_N}
\end{align}

Finding $\Psi$ requires us to solve the time-independent Schrödinger equation
\begin{align}
	\hat{\bm{H}} \Psi =& E\Psi 
\end{align}
for a complex nuclear many body problem this can quickly become a problem consisting of millions of coupled second-order differential equations in $3N$ dimensions.

Computing the expectation value of the observable requires solving the multidimensional integral. This can be approximated using Monte Carlo methods. But what do we do with the fact that we don't know $Psi$? We solve this by employing the variational principle.

\section{Approximating the Wavefunction}
Some selected conditions $\Psi$ must satisfy \cite{HjortJensen2015} are normalization 
\begin{align}
	\int_{-\infty}^{\infty} P(x,t) \dif x = \int_{-\infty}^{\infty} \Psi^{\ast}(x,t) \Psi (x,t) \dif x = 1
\end{align}
and that $\Psi (x,t)$ and $\partial \Psi(x,t) / \partial x$ must be finite, continuous and single valued.


requirements from Ledum og Helgaker.

\section{Methods}
\begin{itemize}
	\item Summary from
	\begin{itemize}
			\item Ledum master \cite{Ledum2017}
	\end{itemize}
	\item The advent of computer simulations during the last several decades have in par- ticular made it possible to study moderately sized quantum mechanical systems from  rst principles. Our ability to solve—in closed form—the governing equations of quantum mechanics (QM) vanishes extremely quickly as the number of constituent particles exceed just a few.
	\item Any approximative scheme which aims to solve the many-body schrodinger equa- tion from scratch subject to some (more or less) well-de ned simpli cations is called an ab initio method. Working from  rst principles the aim of such algorithms is to extract information from a theoretical QM system in a reasonable amount of time. In order to accomplish this, a number of complicating intricacies need to be disre- garded. The magnitude of the simpli cations—essentially the number and impor- tance of complicating factors dropped—determine both the e cacy and the e ciency of the method: More simpli cations made allow solutions to be found for larger sys- tems (albeit less precise solutions), whereas extremely precise solutions can be found for small systems if very few simpli cations are employed.
	\item Despite tremendous increases in available numerical computational power in the latter half of the previous-, and the early parts of the current century, any such approximate scheme used is still heavily limited w.r.t. the system size. In practice, most methods are limited to systems of containing on the order of between $10^2$ (for high- precision methods such as con guration interaction, coupled cluster, di usion Monte Carlo, etc.) and $10^5$ electrons (for faster Hartree-Fock and density functional methods).
	\cite{Hu2015} \cite{VandeVondele2012} \cite{Bowler2010}
	\item As previously noted, solving the Schrodinger equation (SE) exactly by hand is impos- sible in the overwhelming majority of interesting cases. However, methods which can get close to the exact solution exists. Full Con guration Interaction (FCI) or direct di- agonalization of the Hamiltonian is exact in the limit of an in nite orbital basis set but su ers from an exponential complexity scaling (in system and basis size). 
	\begin{itemize}
		\item Molecular Electronic-Structure Theory - Helgaker. 2000 isbn: 0-471-96755-6.
	\end{itemize}
	\item The related Confguration Interaction (CI) and Coupled Cluster (CC) approaches both truncate the FCI expansion of Slater determinants, thus gaining speed but loosing some accuracy.
	\begin{itemize}
		\item S. Kvaal. Lecture notes for FYS-KJM4480. Lecture notes. Sept. 2017
		\item I. Shavitt and R.J. Bartlett. Many-Body Methods in Chemistry and Physics. MBPT and Coupled-Cluster Theory. Cambridge University Press, 2009. isbn: 0-521- 81832-X
	\end{itemize}
	\item Diffusion Monte Carlo (DMC) techniques can in principle provide the exact solution to the SE by imaginary-time evolution of an initial wave function guess
	\begin{itemize}
		\item M. Hjorth-Jensen. Computational Physics. Lecture notes. Aug. 2015. url: https: //github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Lectures/ lectures2015.pdf .
		\item B.L. Hammond, W.A. Lester Jr., and P.J. Reynolds. Monte Carlo Methods in Ab Initio Quantum Chemistry. Wspc, 1994. isbn: 981-02-0322-5.
	\end{itemize}
	\item In practice, DMC methods are highly dependent on this ansatz and thus require as input the results of less accurate method but faster methods. One example may be the Variational Monte Carlo (VMC) method: conceptually simpler and faster than DMC, but not as accurate
	\begin{itemize}
		\item B.L. Hammond, W.A. Lester Jr., and P.J. Reynolds. Monte Carlo Methods in Ab Initio Quantum Chemistry. Wspc, 1994. isbn: 981-02-0322-5.
		\item Harold Conroy. “Molecular Schrodinger Equation. II. Monte Carlo Evaluation of Integrals”. In: The Journal of Chemical Physics 41.5 (1964), pp. 1331–1335. doi: 10.1063/1.1726069. eprint: https://doi.org/10.1063/1.1726069. url: https://doi.org/10.1063/1.1726069.
		\item J.B. Anderson. Quantum Monte Carlo. Origins, Development, Applications. Ox- ford University Press, 2007. isbn: 0-19-531010-1.
	\end{itemize}
	\item The Hartree-Fock (HF) framework—which provides an e cient but not enormously accurate result—has seen extensive use since its inception in 1930
	\begin{itemize}
		\item D. R. Hartree. “The Wave Mechanics of an Atom with a Non-Coulomb Central Field. Part I. Theory and Methods”. In: Mathematical Proceedings of the Cam- bridge Philosophical Society 24.1 (1928), pp. 89–110. doi: 10.1017/S0305004100011919.
		\item V. Fock. %“Näherungsmethode zur Lösung des quantenmechanischen Mehrkör- perproblems”. In: Zeitschrift für Physik 61.1 (Jan. 1930), pp. 126–148. issn: 0044-3328. doi: 10.1007/BF01340294. url: https://doi.org/10.1007/BF01340294.
		\item A. Szabo and N.S. Ostlund. Modern Quantum Chemistry. Dover Publications, 1996. isbn: 0-486-69186-1.
	\end{itemize}
	\item However, by far the most popular approximation is Density Functional Theory (DFT), devel- oped by W. Kohn and L. J. Sham in 1965
	\begin{itemize}
		\item W. Kohn and L. J. Sham. “Self-Consistent Equations Including Exchange and Correlation E ects”. In: Phys. Rev. 140 (4A Nov. 1965), A1133–A1138. doi: 10. 1103/PhysRev.140.A1133. url: https://link.aps.org/doi/10.1103/PhysRev.140. A1133.
		\item R.M. Martin. Electronic Structure. Cambridge University Press, 2004. isbn: 0- 521-53440-2.
	\end{itemize}
	Between 1980 and 2010, DFT was the most active  eld in physics with eight out of the top ten most cited papers being on the subject 
	\begin{itemize}
		\item Axel D. Becke. “Perspective: Fifty years of density-functional theory in chem- ical physics”. In: The Journal of Chemical Physics 140.18 (2014), 18A301. doi: 10.1063/1.4869598. eprint: https://doi.org/10.1063/1.4869598. url: https: //doi.org/10.1063/1.4869598.
	\end{itemize}
	\item Computational scaling of \textit{ab initio} QM models range from $\mathcal{O}(N!)$ in the extreme (FCI) via $\mathcal{O}(N^6)$ (CC with singles, doubles, and estimated connected triples) and $\mathcal{O}(N^4)$ (formal HF), to $\mathcal{O}(N^3)$ for Hatree-Fock with integral pre-screening and density fitting.
	\begin{itemize}
		\item Laura E. Ratcli  et al. “Challenges in large scale quantum mechanical calcu- lations”. In: Wiley Interdisciplinary Reviews: Computational Molecular Science 7.1 (2017). e1290, e1290–n/a. issn: 1759-0884. doi: 10.1002/wcms.1290. url: http://dx.doi.org/10.1002/wcms.1290.
	\end{itemize}
\end{itemize}

\section{VMC}
\subsection{The Variational Principle}
The variatoional princple states that given any function (possibly fulfilling some conditions?) acting as the wavefunction, it will give a higher energy than the ground state energy. So we are sure to get an upper bound. This leads to a method where we construct a trial wavefunction with some number of variational parameters. Minimizing the variational parameters are then sure to give us the best upper bound of the ground state energy. In the process we have also obtained an approximated wave function. 

In order to do these we will employ minimization techniques discussed later.

Kvaale lecture notes, Helgaker. Two theorems

\subsection{Monte Carlo Integration}
The Monte Carlo method is an excellent approach to approximating high dimensional integrals. We do so by evaluating the function of interest at random values drawn according to some probability distribution function of interest. 

Expected value of random variable $x$ is
\begin{align}
	E(x) = \langle x \rangle = \int p(x) x \dif x = \mu
\end{align}
if we have a real valued function $g(x)$ and $x$ is a random variable, $g(x)$ is a random variable as well, with the mean value defined
\begin{align}
	E(g(x)) = \langle g(x) \langle = \int p(x) g(x) \dif x
\end{align}
The $n$th moment of $x$ is defined as the expectation of the $n$th power of $x$,
\begin{align}
	\langle x^n \rangle = \int p(x) x^n \dif x
\end{align}
where we notice that the first moment recovers the definition of the expectation value.
The central moments of $x$ are defined
\begin{align}
	\langle (x-\mu)^n \rangle = \int p(x) (x - \mu)^n \dif x
\end{align}
Where the second central moment is what we know as the variance of $X$
\begin{align}
	\sigma^2 = var(x) = \langle (X-\mu)^2 \rangle =& \int p(x) (x - \mu)^2 \dif x  \\
	=& \int p(x) (x^2 - 2x \langle x \rangle + \langle x \rangle^2) \dif x \\
	=& \langle x^2 \rangle - \langle x \rangle \langle x \rangle + \langle x \rangle^2 \dif x \\
	=& \langle x^2 \rangle - \langle x \rangle^2 \dif x 
\end{align}
The variance of the function $g(x)$ is similarly defined
\begin{align}
	var(g(x)) = \langle (g(x) - \langle g(x) \rangle)^2 \rangle 
	= \langle g(x)^2 \rangle - \langle g(x) \rangle^2
\end{align}
If two random variables $x$ and $y$ are independent, we have that
\begin{align}
	\langle xy \rangle = \langle x \rangle \langle y \rangle
\end{align}
If they are not independent, the degree of independence is measured by the covariance, which is defined
\begin{align}
	cov(x,y) = \langle xy \rangle - \langle x \rangle \langle y \rangle
\end{align}
However, zero covariance by itself does not guarantee independence.

From the covariance we may derive the correlation coefficient
\begin{align}
	\rho(x,y) =& \frac{cov(x,y)}{sqrt{ \langle x \rangle \langle y \rangle }}, \quad
	-1 \geq \rho(x,y) \leq 1
\end{align}

A bivariate probability distribution function $p(x,y)$ may be defined for two random variables. The expected value of a function $g(x,y)$ depending on them is then
\begin{align}
	E(g(x,y)) = \langle g(x,y) \rangle = \int \int p(x,y) g(x,y) \dif x \dif y
\end{align}
The marginal probability distribution function can be found for either of the two variables by 
\begin{align}
	p(x) = \int p(x,y) \dif y
\end{align}
The conditional probability is then given
\begin{align}
	p(y|x) = \frac{p(x,y)}{p(x)}
\end{align}
This means that if we know the marginal and conditional probabilities, the bivariate distribution can be sampled by sampling two univariate distributions. This can be generalized to multivariate distributions of more than two correlated random variables.

\subsubsection{Estimators}
Suppose random variables $x_1, x_2,...$ are drawn at random, but not necessarily independently, from the probability distribution function $p(x)$. Let $g_i$ be functions of $x_i$ and let $\lambda_i$ be a real number. Define the function $G$ by
\begin{align}
	G = \sum_{i=1}^N \lambda_i g_i (x_i)
\end{align}
The expected value of $G$ is
\begin{align}
	E(G) = E (\sum_i^N \lambda_i g_i(x_i)) = \sum_i^N \lambda_i E(g_i(x))
\end{align}
(having used the linearity of the expectation value). If all the $x_i$ are independent, the variance of $G$ becomes
\begin{align}
	var(G) =& \langle G^2 \rangle - \langle G \rangle^2 \\
	=& \sum_i^N \lambda_i^2 var(g_i(x))
\end{align}
If $\lambda_i=\frac{1}{N}$ and all the $g_i(x)$ are identical, $g(x)$. The expected value of $G$ is then
\begin{align}
	E(G) = E(\frac{1}{N} \sum_i^N g(x_i) ) = \frac{1}{N} \sum_i^N E(g(x)) = E(g(x))
\end{align} 
$G$ is the arithmetic average of $g(x)$ and has the same mean. $G$ is said to be an \textit{estimator} of $E(g(x))$. The variance of $G$ becomes
\begin{align}
	var(G) = var( \frac{1}{N} \sum_i^N g(x_i)  ) = \sum \frac{1}{N^2} var(g(x)) 
	= \frac{1}{N} var(g(x))
\end{align}

The implication of this is that as the number of samples of $x$, $N$, increases, the variance of the mean value of $G$ decreases as $\frac{1}{N}$. This is a core idea of Monte Carlo integration. That is, an integral may be estimated by a sum
\begin{align}
	E(g(x)) = \int p(x) g(x) \dif x = E(\frac{1}{N} \sum_{i=1}^N g(x_i))
\end{align}
In summary, to apply this:
\begin{itemize}
	\item Choose a series of random variables $x_i$ from $p(x)$
	\item Evaluate $g(x)$ at each $x_i$
	\item The mean of all the values of $g(x_i)$ is an estimate of the integral, and the variance of this estimate decreases as the number of samples increases.
\end{itemize}

\subsubsection{Convergence}
\textbf{The law of large numbers}: Suppose the random variables $x_1, x_2, ..., x_N$ are independent and all drawn from the same distribution, that is i.i.d. random variables. The expecation of each $x$ is then $\mu$. As $N\rightarrow \infty$, the mean value of the $\{ x \}$ 
\begin{align}
	\bar{x}_N = \frac{1}{N} \sum_{i=1}^N X_i
\end{align}
is almost guaranteed to converge to $\mu$:

To estimate the speed of convergence on the other hand, stronger assumptions are needed. We assume an estimator $G$, its mean $\langle G \rangle$ and variance $var(G)$ all exist. \textbf{The Chebychev inequality} then is
\begin{align}
	p(|G - \langle G \rangle | \geq \sqrt{\frac{var(G)}{\delta}}) &\leq \delta \\
	p(|G - \langle G \rangle | \geq \sqrt{\frac{var(g)}{\delta N}}) &\leq \delta \\
\end{align}
with $\delta$ any postive number. This estimates the chances of generating a large deviation in a Monte Carlo calculation. By making $N$ big, we can make the variance of $G$ as small as we want. The probability of getting a large deviation relative to $\delta$ between the estimate of the integral and the true value becomes small. This is at the core of the Monte Carlo method for evaluating integrals.

A much stronger statement than the Chebychev inequality about the range of values of $G$ that can be observed is given by the \textbf{central limit theorem} of probability. For any fixed value of $N$, there is a pdf that describes the values of $G$ that occur in the course of a Monte Carlo calculation (for a fixed $N$, if $G$ was calculated $M$ times, each times with a different sequence of i.d.d random variables, the set $\{ G_j\}$, $j=1,2,...,M$ has a specific distribution function). As $N\rightarrow \infty$, however, the central limit theorem shows that there is a specific limit distribution for the observed values of $G$, namely, the normal distribution. Set
\begin{align}
	G_N = \frac{1}{N} \sum_i^N g(x_i)
\end{align}
and
\begin{align}
	t_N = \frac{(G_N - \langle g(x) \rangle)}{\sqrt{var(G_N)}}
	= \frac{\sqrt{N} (G_N - \langle g(x) \rangle)}{\sqrt{var(g(x))}}
\end{align}
then
\begin{align}
	lim_{N\rightarrow \infty} p(a < t_N < b)
	= \int_a^b \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \dif t
\end{align}
Let $\sigma^2 = var(g)$. We can then rewrite the above equation to specify a pdf for values of $G_N$
\begin{align}
	p(G_N) = \frac{1}{\sqrt{2\pi (\sigma^2/N)}} e^{\frac{N(G_N - \langle g\rangle)^2}{2\sigma^2}}
\end{align}
As $N\rightarrow \infty$ the observed $G_N$ occurs in ever narrower intervals near $\langle g \rangle$ and one can predict the probability of deviations measured in units of $\sigma$. That is, the observed $G_N$ is within the \textit{standard error} (i.e. $\sigma/\sqrt{N}$) of $\langle g \rangle$ 68.3\% of the time, within two standard errors of $\langle g \rangle$ 95.4\% of the time, and within three standard errors 99.7\% of the time.

The central limit theorem is very powerful in that it gives a specific distribution for the values of $G_N$, but it applies only asymptotically. How large $N$ must be before the central limit theorem applies depends on the problem. If for a problem we have that the third central moment $\mu_3$ of $g$ exists, then the central limit theorem will be substantially satisfied when
\begin{align}
	|\mu_3| << \sigma^3 \sqrt{N}
\end{align}
Then confidence limits derived from the normal distribution can be applied to the results of a Monte Carlo calculation. 

Without the central limit theorem, there is in general only the much weaker upper bound of the Chebychev inequality to suggest how much the observed $G_N$ deviates from the actual mean. Of course, in specific cases, studies can be made of the distribution of the estimator. Much Monte Carlo is done assuming that the theorem has been satisfied no matter what the sample size; reported errors must be considered optimistic in such cases.
When the variance is infinite, it is sometimes possible to find a limit distribution for $G$ that will lead to a central limit theorem for that particular problem. The limit distribution will in general not be normal.

The variance used in the discussion above may itself be estimated using independent values of $g(x_n)$ the following way:
\begin{align}
	\langle \frac{1}{N} \sum_{i=1} g^2 (x_i) - [\frac{1}{N}\sum_{i=1}^N g(x_i)]^2 \rangle
	= \langle g^2 \rangle - \frac{1}{N^2} \langle
	\sum_{i=1}^N g(x_i)^2 + \sum_{i,i\neq j}^N \sum_{j=1}^N g(x_i) g(x_j) \rangle
\end{align}
Using the independence of $g(x_i)$ and $g(x_j)$ in evaluating $\langle g(x_i) g(x_j)\rangle $, we find the right-hand side equal to 
\begin{align}
	(1- \frac{1}{N}) \langle g^2 \rangle - \frac{N(N-1)}{N^2} \langle g \rangle^2
	= \frac{N-1}{N} var(g)
\end{align}

thus an estimator for $\sigma^2$ is
\begin{align}
	\sigma^2 \approx \frac{N}{N-1} ( \frac{1}{N} \sum_{i=1}^N g^2(x_i) -
	( \frac{1}{N} \sum_{i=1}^N g^2(x_i))^2 )
\end{align}
An estimator of the variance of the estimated mean is given by
\begin{align}
	var(G_N) \approx \frac{1}{N-1} ( \frac{1}{N} \sum_{i=1}^N g^2(x_i) -
	( \frac{1}{N} \sum_{i=1}^N g^2(x_i))^2 )
\end{align}

\subsubsection{Summary}
If $x_1, x_2,...,x_n$ arr i.i.d. random variables with pdf $p(X)$, then for a function $g(x)$ an estimator is
\begin{align}
	G_N =& \frac{1}{N} \sum_{i=1}^N g(x_i) \\
	\langle G_N \rangle =& \int p(x) g(x) \dif x
\end{align}
and
\begin{align}
	var(G_N) =& \frac{1}{N} var(g)
\end{align}
as $N\rightarrow \infty$ and if the variance exists, the distribution of possible values of $G_N$ narrows about the mean as $\sqrt{N}$; or the probability of finding a $G_N$ some fixed distance away from $\langle G_N \rangle$ becomes smaller. 

The basic random variable used in Monte Carlo has been set by historical convention to be distributed uniformly between 0 and 1.

Note that we can do integrations in multiple dimensions with suprisingly little loss of computational efficiency. That one can work in many dimensions – indeed one can add extra dimensions – is a characteristic of Monte Carlo quadrature in contrast to discrete numerical quadrature, and is a property that can be exploited to great advantage in many applications.

To evaluate the $L$-dimensional integral over the unit hypercube 
\begin{align}
	\int ... \int g(x_1, x_2, ..., x_L) \dif x_1 \dif x_2 ... \dif x_L
\end{align}
$L$ uniform random variables could be samples, the function $g(x_1, x_2, ..., x_L)$ calculated, and the whole process repeated $N$ times. The arithmetic mean of the function values gives an unbiased estimate of the integral.

Of course, given the ability to generate random variables from any distribution over any space (as discussed in the next chapter) the domain of integration need not be limited to hypercubes.

Pseudorandom numbers might be wanted over truly random numbers for several reasons where one is dependent on being able to recreate program runs.

\subsubsection{Monte Carlo Estimators}
We have defined an estimator as a useful approximation to a quantity of interest $Q$ (for example an integral) which may be derived from a Monte Carlo calculation. The estimator is then the approximation (as given by earlier definitions), which is a function $\theta(\xi_1, \xi_2, ..., \xi_N)$ of the $N$ random or pseudorandom variables used in the calculation.

The function $\theta$ is of course itself random, and the statement that it gives a
satisfactory approximation to $Q$ means that it is not expected to fluctuate far from $Q$. Put a little more formally,
\begin{align}
	\frac{\langle (\theta - Q)^2 \rangle}{Q^2} \ll 1
\end{align}

Acceptable values of the ratio depend on the application. A Monte Carlo calculation may be intended to give a rough estimate of some numerical quantity, or it may be aimed at high precision, or at a target in between. The appropriate balance between small or zero bias and small variance will depend on these choices.
We write
\begin{align}
	\langle (\theta - Q)^2 \rangle = \langle (\theta -\langle \theta \rangle)^2 \rangle
	+ (\langle \theta \rangle - Q)^2
\end{align}
and observe that the quality of $\theta$ as a measure of $Q$ comes separately from the variance of $\theta$ and from the departure of its mean from $Q$. The quantity $\langle \theta \rangle - Q$ is called the \textit{bias of the estimator}. 

The quadratures we have discussed are unbiased since the result is linear in the functions calculated. For some problems, however, it is very difficult to formulate unbiased estimators. As we shall see, there are many problems for which the answer required is a ratio of integrals,

\begin{align}
	Q = \frac{\int_0^1 g_1 (x) \dif x}{\int_0^1 g_2 (x) \dif x} 
\end{align}
for which a suitable estimator is
\begin{align}
	\theta (\xi_1, ..., \xi_N) = \frac{\sum_i g_1 (\xi_i)}{\sum_i g_2 (\xi_i)}
\end{align}
Since this is not a linear function of $g_2$, it is biased. (...example...) This $\frac{1}{N}$ behavior is typical of the bias of such ratios. The results that may be derived from a Monte Carlo calculation are more general than this, and may have different variation of the bias. It is of course best if the bias becomes 0 as $N$ grows large.

An estimator $\theta$ is termed consistent for the quantity $Q$ if $\theta$ converges to $Q$ with probability 1 as $N$ approaches infinity. That is, $\theta$ is a consistent estimator of $Q$ if 
\begin{align}
	p(lim_{N\rightarrow \infty} \theta (\xi_1, \xi_2, ..., \xi_N)=Q) =1 
\end{align}
The law of large numbers states that the sample mean $\bar{x}_N$ is a consistent (and unbiased) estimator of the mean $\mu$. It further implies that estimators of quotients that are quotients of means are also consistent (although, in general, biased).

While unbiased estimators are desirable, they should not be introduced at the expense of a large variance, since the overall quality is a combination of both bias and consistency. In general, one seeks the minimum of $\langle (\theta -Q)^2 \rangle$. 

(example)

Just as a good Monte Carlo calculation must be supplemented with an estimate of the statistical error, sources of bias should be identified. The bias should be estimated numerically or an upper bound should be determined. A useful way of estimating bias when the behavior with $N$ is known is to group the data in samples smaller than $N$, say $n=N/m$. One can average this more biased estimator over the m groups obtained and study the dependence on m:
\begin{align}
	\text{Bias of } \frac{\sum_{i=1}^N g_1 (\xi_i)}{\sum_{i=1}^N g_2 (\xi_i)}
	&\approx \frac{c}{N} \\
	\text{Bias of} \frac{1}{m} \sum_{l=1}^m (\frac{\sum_{i=n(l-1)+1}^{nl} g_1 (\xi_i)}
	{\sum_{i=n(l-1)+1}^{nl} g_2 (\xi_i)})_{group_l}
	&= \frac{c}{n} = \frac{cm}{N}
\end{align}
where $c$ is a positive constant.

We note in passing that this method of grouping is also a practical way of estimating the variance of the quotient. This consists in selecting groups of numerators and denominators that are nearly independent, forming partial quotients for the groups, and then applying Equation 2.44. \cite{Kalos2008}

\subsection{Markov Chains}
Why? Because Monte Carlo requires us to sample from the probability distribution. We are not able to do this directly because the normalization constant is intractable. 

What is a Markov Chain?
A Markov Chain is a type of stochastic process. 

\subsubsection{Stochastic processes}
A stochastic process is a stochastic quantity $Y$ that can be mapped from a stochastic variable $X$ with a function $f$, and that also depends on another "normal" variable $t$, which usually represents time. That is,
\begin{align}
	Y_X (t) = f(X, t)
\end{align}

We may compute averages based on the given probability density $P_X(X)$ of $X$, for example
\begin{align}
	\langle Y(t) \rangle = \int Y_x (t) P_x(x) \dif x
\end{align}
More generally we may compute the $n$th moment, given time values $t_1,...,t_n$ as
\begin{align}
	\langle Y(t_1) ... Y(t_n) \rangle = \int Y_x(t_1) ... Y_x(t_n) P_x (x) \dif x 
\end{align}
Similarly we may also compute the \textbf{autocorrelation function}.

The probability density for $Y_X(t)$ to take any value $y$ at time $t$ is then
\begin{align}
	P_1 (y, t) = \int \delta (y- Y_X (t)) P_X (x) \dif x
\end{align}
Similarly the joint probability density that $Y$ has the value $y_1$ at $t_1$, and also $y_2$ at $t_2$ and so on till $y_n$ at $t_n$ is
\begin{align}
	P_n (y_1, y_1; y_2, t_2; ...; y_n, t_n)
	= \int \delta (y_1 - Y_x (t_1)) ... \delta (y_n - Y_x (t_n)) P_x (x) \dif x
\end{align}
In this way a hierarchy of probability densities $P_n (n=1,2,...)$ is defined.
Considering $P_n$ to be defined only when all times are different, the hierarchy of functions $P_n$ then obeys the following four \textbf{consistency conditions}.
\begin{enumerate}
	\item $P_n \geq 0$
	\item $P_n$ does not change on interchanging two pairs $(y_k, t_k)$ and $(y_l, t_l)$
	\item $\int P_n (y_1, t_1; ...; y_n, t_n) \dif y_n = P_{n-1} (y_1, t_1; ...; y_{n-1}, t_{n-1}) $
	\item $\int P_1 (y_1, t_1) \dif y_1 = 1$ 
\end{enumerate}
Any set of functions that obey the four consistency conditions determines a stochastic process.



\subsubsection{Markov processes}
A Markov Process is a stochastic process that has the Markov property. The Markov property is that the next state is dependent only on the current state, and no more history than that. It is that for any set of $n$ successive times (i.e. $t_1<t_2<t_3$) one has
\begin{align}
	P_{1|n-1} (y_n, t_n | y_1, t_1; ...; y_{n-1}, t_{n-1}) = P_{1|1} (y_n, t_n | y_{n-1}, t_{n-1})
\end{align}
Meaning, the conditional probability density at $t_n$, given the value $y_{n-1}$ at $t_{n-1}$, is uniquely determined and not affected by any knowledge of the values at earlier times. $P_{1|1}$ is then known as the \textbf{transition probability}.

A Markov Process then is fully determined by the two functions:
\begin{enumerate}
	\item The initial probability distribution: $P_1 (y_1, t_1)$
	\item The transition probability: $P_{1|1} (y_2, t_2| y_1, t_1)$ 
\end{enumerate}
From this one can use Kolmogorov's definition of conditional probability to successively construct all $P_n$. For example, for 
$t_1 < t_2 < t_3$
\begin{align}
	P_3 (y_1, t_1; y_2, t_2; y_3, t_3) =& P_2 (y_1, t_1; y_2, t_2) P_{1|2} (y_3, t_3 | y_1, t_1; y_2, t_2) \\
	=& P_1 (y_1, t_1) P_{1|1} (y_2, t_2| y_1, t_1) P_{1|1} (y_2, t_2| y_1, t_1) P_{1|1} (y_3, t_3 | y_2, t_2)
\end{align}

\subsubsection{The Chapman-Kolmogorov equation}
If we integrate the equation above over $y_2$, and then divide by $P_1(y_1, t_1)$, we obtain for $t_1 < t_2 < t_3$ 

\begin{align}
	P_2 (y_1, t_1; y_3, t_3) =& P_1(y_1, t_1) P_{1|1} (y_2, t_2 | y_1, t_1) P_{1|1} (y_3, t_3| y_2, t_2) \dif y_2 \\
	P_{1|1} (y_3, t_3| y_1, t_1) =& \int P_{1|1} (y_3, t_3|y_2, t_2) P_{1|1} (y_2, t_2 | y_1, t_1) \dif y_2
\end{align}
This is an identity which must be obeyed by any Markov Process. The two functions $P_1$ and $P_{1|1}$ which fully determine a Markov Process then cannot be chosen arbitrarily then, but have to obey
\begin{enumerate}
	\item The Chapman-Kolmogorov equation above
	\item $P_1(y_2, t_2) = \int P_{1|1} (y_2, t_2| y_1, t_1) P_1 (y_1, t_1) \dif y_1$ ("obviously")
\end{enumerate}

Any two non-negative functions $P_1$ and $P_{1|1}$ that obey these consistency conditions uniquely determines a Markov Process.

\subsubsection{Stationary processes}
A \textit{stochastic process} in general is defined to be stationary when the moments are not affected by a shift in time, that is
\begin{align}
	\langle Y(t_1 + \tau)...Y(t_n + \tau) \rangle = \langle Y(t_1 )...Y(t_n ) \rangle
\end{align}
for all $n$, all $\tau$, and all $t_1, ..., t_n$. In particular $\langle Y \rangle$.

For a stationary \textit{Markov process} we have that the transition probability $P_{1|1}$ does not depend on two times but only on the time interval; we introduce the notation
\begin{align}
	P_{1|1} (y_2, t_2| y_1, t_1) = T_\tau (y_2|y_1), \quad \tau = t_2 - t_1
\end{align}

The Chapman-Kolmogorov equation then becomes $(\tau, \tau' > 0)$
\begin{align}
	T_{\tau+\tau'} (y_3 | y_2) = \int T_{\tau'} (y_3|y_2) T_\tau (y_2|y_1) \dif y_2
\end{align}
If we read the integral as a product of two matrices, or integral kernels, this may be written
\begin{align}
	T_{\tau + \tau'} = T_{\tau'} T_{\tau}, \quad (\tau, \tau' > 0)
\end{align}

\subsubsection{Markov Chains}
Markov chains are an especially simple class of Markov processes, which we define by the properties
\begin{enumerate}
	\item The range of $Y$ is a discrete set of states
	\item The time variable is discrete and take only integer values $t=...,-2,-1,0,1,2,...$
	\item The process is stationary (or at least \textbf{homogeneous}), so the transition probability depends on the time difference alone.
\end{enumerate}
A \textbf{finite} Markov chain then has a range consisting of a finite number $N$ states. We then have
\begin{itemize}
	\item The initial probability distribution $P_1 (y, t)$ is an $N$-component vector $p_n(t)$ ($n=1,2,...,N$).
	\item The transition probability $T_\tau (y_2|y_1)$ is an $N\times N$ matrix.
\end{itemize}

The Markov property then leads to the equation
\begin{align}
	T_\tau = (T_1)^{\tau} (\tau = 0,1,2,...)
\end{align}
The probability distribution $p(t)$ originating from the initial distribution $p(0)$ is, in matrix notation,
\begin{align}
	p(t) = T^t p(0)
\end{align}
The study of finite Markov chains amounts to investigating the powers of an $N\times N$ matrix $T$ of which we only know that it is a \textbf{stochastic matrix}. That is,
\begin{enumerate}
	\item Its elements are non-negative.
	\item Each column adds up to unity.
\end{enumerate}
It is clear that $T$ has a left eigenvector $(1,1,...,1)$ with eigenvalue 1, and therefore a right eigenvector $p^s$ such that $T p^s = p^s$, which is the $P_1(y)$ of the stochastic process.
The prinicpal task of the theory then is to show that for any initial $p(0)$
\begin{align}
	\text{lim}_{t\rightarrow \infty} p(t) = \text{lim}_{t\rightarrow \infty} T^t p(0) = p^s
\end{align}
(and this has been shown by so and so except for a few exceptional cases?)
\cite{VanKampen2007}


\subsection{Markov Chain Monte Carlo}
Markov Chain Monte Carlo methods are constructed to have the desired probability distribution as their equilibrium distribution, so that having reached the Markov Chain equilibrium distribution, we may obtain samples from the desired distribution. This is done using random walkers. While samples in conventional Monte Carlo integration are independent, the samples in this method are correlated.
When solving an integral the desired equilibrium of the Markov chain is the integrand.

What is a random walk?
Assume a set of random variables $x_1, x_2,...$ represent the state of a system at some "time" $n$ for $n=1,...,N$. The probability of going from state $i$ at time $n$ to state $j$ at time $n+1$ is
\begin{align}
	p_{ij} = p(x_{n+1} = j| x_n = i)
\end{align}
and is independent of past states, prior to state $i$. The set $\{ x_n \}$, $n\geq 0$ is called a \textit{Markov chain process} with stationary (a stationary transition probability is independent of when the transition from state $i$ to state $j$ occurs in the Markov chain) transition probabilities $p_{ij}$, $i,j=1,...,N$. A Markov chain is another term for a random walk. 

Random walks are frequently used as discrete approximations to continuous physical processes. Consider the motion of diffusing particles, which can be described by a differential equation. The motion of the particles is continuous but the positions, subject to collisions and perhaps random forces, fluctuate randomly. If a particle’s future position depends only on the current position, then the set $\{ x_n \}$ where $x_n$ is the position at a time "$n$" represents a Markov process.

Every time the system represented by a Markov Chain leaves a state $i$, it must be in another of the possible states; thus
\begin{align}
	\sum_{j=1}^N p_{ij} = 1, \quad i=1,...,N
\end{align}
If a state $j$ can be reached from state $i$ in a finite number of steps or transitions, then state $j$ is called \textit{accessible} from state $i$. If state $i$ is accessible from state $j$, then the states are said to \textit{communicate} with each other. If the two states do not communicate, then either $p_{ij}=0$ or $p_{ji}=0$. A Markov chain is termed \textit{irreducible} if all states communicate with each other. 

The $p_{ij}$ form a matrix $\bm{P}$ called the \textit{one-step transition probability matrix}. Given $\bm{P}$ and the probability distribution of the state of the process at time 0, the Markov chain is completely specified. Let $p_{ij}^n$ be the probability that the system goes from state $i$ to state $j$ in $n$ transitions. Then it can be expressed as 
\begin{align}
	p_{ij}^n = \sum_{k=0}^N p_{ik}^r p_{kj}^s
\end{align}
where $r+s = n$ and we define
\[ p_{ij}^0 = \begin{cases}
				 1, & i = j \\
				 0, & i \neq j  
\end{cases} \]
The period of state $i$, $d(i)$, is the greatest common divisor of all integers $n\geq 1$ for which $p_{ii}^n > 0$. If $p_{ii}^n = 0$ for all $n$, then $d(i)$ is defined to be 0. If the states $i$ and $j$ communicate, then $d(i) = d(j)$. If all the states in a Markov chain have a period of 1, the system is called \textit{aperiodic}. Let $f_{ii}^n$ be the probability that starting from state $i$, the first return to state $i$ occurs at the $n$th transition,
\begin{align}
	f_{ii}^n = p(x_n =i, x_m\neq i, m=1,2,...., n-1| x_0 =i)
\end{align}
State $i$ is said to be \textit{recurrent} iff
\begin{align}
	\sum_{n=1}^N f_{ii}^n = 1
\end{align}
That is, starting from state $i$, the probability of returning to state $i$ in finite time is 1. If this is not true, the state is said to be a \textit{transient} state. If states $i$ and $j$ communicate and $i$ is recurrent, then $j$ is recurrent. Thus, all states in an equivalence class (i.e. states that communicate with each other) are either recurrent or transient. 

Suppose we have a Markov chain with a recurrent aperiodic class of states. We can define the stationary probability for states $j=1,2,...,N$ by
\begin{align}
	\pi_j &= sum_{i=0}^N \pi_i p_{ij}, \quad pi_i \geq 0 \\
	\sum_{i=0}^N \pi_i &= 1
\end{align}
$\pi_j$ is the proportion of the time that the system spends in state $j$. The quantity $\pi_i p_{ij}$ is the proportion of the time that the random walk has just entered state $j$ from state $i$. If the initial state of the random walk is chosen to be distributed as $\pi_j$, then $p(x_n=j)$ is equal to $\pi_j$, for all $n$ and $j$. 

If $\pi_i > 0$ for one state $i$ in an aperiodic recurrent class, then $\pi_j > 0$ for all states $j$ in the same class. Such a class is called \textit{strongly ergodic}. When
\begin{align}
	\pi_i p_{ij} = \pi_j p_{ji}
\end{align}
for all $i\neq j$, then the Markov chain is said to be \textit{time reversible}. If the initial state $x_0$ is chosen from the set $\{ \pi_j \}$, the sequence of steps going backwards in time from any point $n$ will also be a Markov chain with transition probabilities given by $p_{ij}$. 

\subsubsection{Estimators and Markov Chains}
Markov Chains are used in simulations to model the behavior of systems whose probability distribution functions are not easily sampled. Suppose a discrete ranom variable has a probability mass function given by $p_j = p(x=j), j=1,2,..., N$. It can be sampled by creating an irreducible aperiodic Markov chain such that $p_j = \pi_j$ for $j=1,2,...,N$ and obtaining the values of $x_n$ when $n$ is large. An estimator for a property, $h(x)$, of the system, $E(h(x)) = \sum_{j=1}^N h(j) p_j$ can be formed from the states reached in the random walk,
\begin{align}
	E(h(x)) \approx \frac{1}{m} \sum_{i=1}^m h(x_i)
\end{align}
However, at step $n$ in a Markov chain, the state of the next step, $n+1$, is dependent on the properties of the previous step. Thus, it takes some number of steps, not always easily quantified, to lose the influence of the distribution of the initial state $x_0$. As a result, in forming estimators of quantities of interest, the first $k$ steps of the Markov chain are usually discarded. The estimator of the property $h(x)$ then becomes
\begin{align}
	E(h(x)) \approx \frac{1}{m-k} \sum_{i=k+1}^m h(x_i)
\end{align}
The choice of $k$ is guided by the properties of the application. 

The strong serial correlation that exists between the steps of a Markov chain also effect the estimation of the standard error of the estimator in the above equation. To correct for the correlation, one approach is to divide the Markov chain into blocks large enough so that the correlation is minimized between the blocks. This approach is called \textit{batch means} or \textit{blocking}. The $m-k$ steps in the above equation are broken up into $s$ blocks or batches of lenght $r$ steps, where $s = \frac{m-k}{r}$ and both $s$ and $r$ are integers. The average value of $h(x)$ is evaluated for each of the $s$ blocks.
\begin{align}
	H_{\mathit{\ell}} = \frac{1}{r} \sum_{i=k+(\ell - 1)r + 1}^{k + \ell r} h(x_i), \quad \ell = 1,2,...,s
\end{align}
Since each of the $H_\ell$ is considered to be an i.i.d. random variable, the mean value is given by 
\begin{align}
	\bar{H} = \frac{1}{s} \sum_{\ell = 1}^s H_\ell
\end{align}
and $\bar{H}$ is used to obtain the sample estimate of the variance,
\begin{align}
	var(H_\ell) = \frac{1}{s-1} \sum_{\ell = 1}^s (H_\ell - \bar{H})^2
\end{align}
The estimate of the standard error associated with $E(h(x))$ can then be obtained as $\sqrt{var(H_\ell)/s}$. 
 \cite{Kalos2008}
\subsubsection{Application Example: Simulated Annealing}
 \cite{Kalos2008}
\subsubsection{Variance reduction - importance sampling of integral equations}
 \cite{Kalos2008}


\subsubsection{Different source on Markov Chains}
Variational Monte Carlo is usually used to calculate expectation values of operators $\hat{X}$ that are diagonal or near-diagonal in the $\bm{R}$ representation. (The case of an arbitrary operator $\hat{X}$ is treated in the chapter by Nightingale.) The expectation value can be written in the form
\begin{align}
	\langle \hat{X} \rangle_\rho =& \frac{\sum_{\bm{R}} X(\bm{R}) \rho(\bm{R})}{\sum_{\bm{R}} \rho (\bm{R})} \\
	\approx & \frac{1}{T} \sum_{i=1}^T X(\bm{R}_i)
\end{align}
where $\rho(\bm{R}) = |\Psi (\bm{R})|^2$ and $\Psi(\bm{R})$ is a trial state that approximates the egienstate of interest.
The \textit{configurational eigenvalue} $X(\bm{R})$ is defined by
\begin{align}
	X(\bm{R}) \Psi (\bm{R}) = \langle \bm{R} | \hat{X} | \Psi (\bm{R}) \rangle = \sum_{\bm{R}'} \langle \bm{R} | \hat{X} | \bm{R}' \rangle \langle \bm{R}' | \Psi \rangle
\end{align}
and $X(\bm{R}) = \langle \bm{R}|\Psi \rangle = \langle \Psi | \bm{R} \rangle$. 
To be of practical use it is necessary that the sum on the right hand side can be performed quickly. If the states $\bm{R}$ form a continuum, then this is the case if $\hat{X}$ is diagonal or near-diagonal (involves only low-order derivatives in the $\bm{R}$ representation).
The operator of greatest interest is the Hamiltonian $\mathcal{H}$, in which case the configurational eigenvalue $X(\bm{R})$ is the \textit{local energy}.

The second line in the first equation expresses the fact that a Monte Carlo simulation of expectation values is obtained as a time average over a sequence of $T$ configurations that are sampled with relative probability $\rho(\bm{R})$. The time average is an approximation to the quantum mechanical expectation value, which becomes an equality in the $T\rightarrow \infty$ limit. To evaluate the right hand side, we need to find a way to sample  $\rho(\bm{r})$. The \textit{generalized Metropolis} method is almost universally used to sample complicated prob dists. Although it is a very simple method, we will see that a little bit of thought invested in the implemenetation of the method can have a large effect on its efficiency by reducing sequential correlations of the sampled configs. 

As the trial state $\Psi(\bm{R})$ approaches an exact eigenstate of $\hat{X}$, the configurational eigenvalue becomes a constant, equal to the eigenvalue of $\hat{X}$ for that state, i.e., both systematic and the statistical errors of the Monte Carlo estimate of the first equation vanish. Although this ideal situation is never achieved in practice for problems of interest, it does indicate that significant gains can be achieved by optimizing $\Psi(\bm{R})$ . We present later the \textit{variance minimization method} for optimizing trial states.
\subsubsection{(cont. ) Generalized Metropolis method}
For distributions that can't be sampled directly, at the cost of the generated samples being serially correlated.

The Metropolis method is an application of a \textbf{Markov chain}. A Markov chain is specified by two ingredients: an initial prob dist and a transition matrix whose elements $M(\bm{R}_f | \bm{R}_i )$ represent the prob of making a transition from an initial state $\bm{R}_i$ to a final state $\bm{R}_f$. Clearly, since the prob must be non-negative and since the system must evolve to some state (including the initial state), we require
\begin{align}
	M(\bm{R}_f | \bm{R}_i) \geq 0 \quad \text{and} \quad
	\sum_{\bm{R}_f} M(\bm{R}_f | \bm{R}_i) = 1
\end{align}
Such a matrix = a \textbf{stochastic matrix}.

To sample the desired distribution $\rho (\bm{R})$ we start from an arbitrary initial state $\bm{R}_i$ and evolve the system by repeated application of a Markov matrix $M$ that satisifies the \textbf{stationary condition}:
\begin{align}
	\sum_i M(\bm{R}_f | \bm{R}_i) \rho (\bm{R}_i) = \sum_i M (\bm{R}_i | \bm{R}_f) \rho(\bm{R}_f) = \rho(\bm{R}_f)
\end{align}
for all states $\bm{R}_f$ where the second equality follows from previous eq. 

Hence, $\rho$ is a right eigenvector of $M$ with eigenvalue 1. The stationary property implies if we start with the desired dist $\rho$ we will continue to sample the same dist. 

However, we want that any intial dist should evolve to the desired dist $\rho$ under the repeated application of $M$, i.e., $lim_{n\rightarrow \infty} M^n (\bm{R}' | \bm{R}) = \rho(\bm{R}')$, independent of $\bm{R}$. 

Aka, not only should $\rho$ be a right eigenvector of $M$ but it should be the \textit{dominant} right eigenvector. Necessary conditions are that transitions can be made with non-zero prob between (almost, ref) any pair of states in a finite number of steps and that $M$ not be cyclic. Such a matrix is \textbf{ergodic}. 

\subsubsection{(cont.) Detailed balance condition}
We have shown so far that we can sample $\rho$ by repeated application of a Markov matrix that satisfies the stationary condition, but we have not shown how such matrices are constructed in practice. In order to do this, the more stringent \textbf{detailed balance} condition
\begin{align}
	M(\bm{R}_f | \bm{R}_i ) \rho(\bm{R}_i) = M(\bm{R}_i | \bm{R}_f ) \rho(\bm{R}_f)
\end{align}
is usually imposed. 
It expresses the condition that for any pair of states $\bm{R}_i$ and $\bm{R}_f$ the rate of flow is the same in both directions. It is a sufficient, but not necessary, condition. Detailed balance implies the stationary condition, but the reverse is not true.

\cite{Umrigar1999}

\subsection{Metropolis-Hastings Sampling}
There are several approaches to how one should best generate new samples. One is Metropolis-Hastings method. It is based on using both a proposal probability and an acceptance probability. 
Introduced by \cite{Metropolis1953}


\begin{itemize}
	\item Simple and powerful. Can be used to sample essentially any distribution function regardless of analytic complexity in any number of dimensions. Complementary disadvantages: sampling is correct only asymptotically, and successive variables produced are correlated, often very strongly. $\rightarrow$ evaluation of integrals normally produces positive correlations in the values of the integrand, with consequent increase in variance for a fixed number of steps as compared with independent samples. Also the method is not well suited to sampling distributions with parameters that change frequently.
	\item Motivation: an analogy with the behavior of systems in stat mech that approach an equilibrium whose statistical properties are independent of the kinetics of the system.
	\item Here
	\begin{itemize}
		\item \textit{system} = a point $x$ in a space $\Omega$ (typically in $\bm{R}^M$) that may be thought of as a possible description of a physical problem.
		\item \textit{kinetics} = a stochastic transition that governs the evolution of the system: a pdf $K(X|Y)$ that ensures that the evolution of a system known to be at $Y$ will bring it near $X$ next.
		\item $K$ = a model of the physical process by which a system changes. Or a mathematical abstraction. In a Monte Carlo calculation, it plays the role of a sampling distribution.
	\end{itemize}
	\item One condition for a system to evolve toward equilibrium and stay there is, simply, that the system be on the average as likely to move into a specific neighborhood of $X$ from a neighborhood of $Y$ as to move exactly in the reverse direction. If the prob dist for observing the system near $X$ in equilibrium is $f(X)$, then the kinetics must satisfy
	\begin{align}
		K(X|Y) f(Y) = K(Y|X)f(X)
	\end{align}
	This relation is called \textbf{detailed balance}. $K(X|Y)f(Y)$=the prob of moving from $Y$ to $X$ expressed as the a priori chance of finding the system near $Y$ (i.e. $f(Y)$) times the conditional probability $K(X|Y)$ that it will move from $X$ to $Y$.
	\item For a physical system, one usually assumes $K(X|Y)$ known, and one has the task of finding $f(X)$. The Metropolis algorithm  (as in much of Monte Carlo) reverses this: one has the task of finding a convenient and correct kinetics that will equilibrate the system so that the given $f(X)$ turns out to be the chance of observing the system near $X$.
	\item This turns out to be extremely easy given the elegant device suggested by the Metropolis algorithm. Transitions are \textit{proposed} from, say, $Y$, to $X'$ using essentially \textit{any} distribution $T(X'|Y)$. Then on comparing $f(X')$ with $f(Y)$ and taking into accoutn $T$ as well, the system is either moved to $X'$  (move 'accepted') or returned to $Y$ (move 'rejected'). Acceptance of the move occurs with probability $A(X'|Y)$, which must be calculated so as to satisfy detailed balance. 
	\item We then have
	\begin{align}
		K(X|Y) = A(X|Y) T(X|Y)
	\end{align}
	Detailed balance requires
	\begin{align}
		A(X|Y) T(X|Y) f(Y) = A(Y|X) T(Y|X) f(X)
	\end{align}
	We expect that the ratio 
	\begin{align}
		\frac{T(Y|X) f(X)}{T(X|Y) f(Y) }
	\end{align}
	will play a significant role in determining $A$.
	\item Given a pdf $f(X)$, where $X$=a many-dimensional vector, the Metropolis technique establishes a random walk whose steps are designed so that when repeated again and again, the asymptotic distribution of $X'$s is $f(X)$. Suppose that $X_1, X_2, ..., X_n$ are the steps in a random walk. Each of the $X'$s is a random variable and has an associated probability $\phi_1(X), \phi_2(X), ..., \phi_n(X)$, where $\phi_1(X)$ can be any distribution for $X$. The $\phi_n(X)$ have the property that asymptotically
	\begin{align}
		lim_{n\rightarrow \infty} \phi_n(X) = f(X)
	\end{align}
	At each step, in the random walk, there is a transition distribution $T(X|Y)$, that is, the pdf for a trial move to $X$ from $Y$. The $T(X|Y)$ is normalized such that
	\begin{align}
		\int T(X|Y) \dif X = 1
	\end{align}
	for all values of $Y$. A quantity $q(X|Y)$ is defined as 
	\begin{align}
		q(X|Y) = \frac{T(Y|X)f(X)}{T(X|Y)f(Y)} \geq 0
	\end{align}
	where we explicitly assume that it's possible to move from $X$ to $Y$ if one can move from $Y$ to $X$ and vice versa. From $q(X|Y)$, the probability of accepting a move can be calculated; one frequently used probability is
	\begin{align}
		A(X|Y) = min(1, q(X|Y))
	\end{align}
	\item The algorithm can now be described concretely.
	\begin{itemize}
		\item At step $n$ of the random walk, the value of $X$ is $X_n$; a possible next value for $X$, $X_{n+1}'$, is sampled from $T(X_{n+1}'|X_n)$, and the prob of accepting $X_{n+1}'$ is computed. If
		\begin{itemize}
			\item If $q(X_{n+1}'|X_n) >1$ then $A(X_{n+1}'|X_n) =1$
			\item If $q(X_{n+1}'|X_n) <1$ then $A(X_{n+1}'|X_n) = q(X_{n+1}'|X_n)$
		\end{itemize}
		where
		\begin{align}
			q(X_{n+1}'|X_n) = \frac{T(X_n|X_{n+1}') f(X_{n+1}')}{T(X_{n+1}'|X_n) f(X_n)}
		\end{align}
		Then
		\begin{itemize} 
			\item With probability $A(X_{n+1}'|X_n)$ (that is, if $A(X_{n+1}'|X_n) > \xi$), we set $X_{n+1} = X_{n+1}'$
			\item Otherwise, $X_{n+1} = X_n$
		\end{itemize}
		For $q(X_{n+1}'|X_n) > 1$, $X_{n+1}$ will always equal $X_{n+1}'$.
	\end{itemize}
	\item As the random walk proceeds, a recursive relationship develops between succeeding $\phi_n (X)$'s. Let $\phi_n (X)$ be the distribution of values of $X_n$; what is the distribution $\phi_{n+1}$ for the values of $X_{n+1}$? There are two contributions to the distribution of the $X_{n+1}$:
	\begin{itemize}
		\item The prob of entering into the vicinity $\dif X$ of $X$ when we successfully move from $X_n$ and
		\item The prob that once we are at $X$, we will stay at $X$. 
	\end{itemize}
	If we start out at some value $Y$ contained in $\dif Y$, the prob of moving from the neighborhood $Y$ to the neighborhood of $X$ is $T(X|Y)\phi_n(Y) \dif Y$. The prob of successfully moving from $Y$ to $X$ is $A(X|Y)T(X|Y)\phi_n(Y) \dif Y$, so the net prob of successfully moving from any point $Y$ to the neighborhood of $X$ becomes
	\begin{align}
		\int A(X|Y) T(X|Y) \phi_n (Y) \dif Y
	\end{align}
	Similarly, the net prob that a move away from $X$ is not accepted is
	\begin{align}
		\int (1-A(Y|X)) T(Y|X) \dif Y
	\end{align}
	where $T(Y|X)$= the prob of moving from $X$ to $Y$ and $(1-A(Y|X))$ =the prob that the move was not accpeted. Upon multiplying the above equation by $\phi_n (X)$, the prob that we were at $X$, the relationship for $\phi_{n+1} (X)$ becomes
	\begin{align}
		\phi_{n+1} (X) = \int A(X|Y) T(X|Y) \phi_n (Y) \dif Y
		+ \phi_n(X) \int (1-A(Y|X)) T(Y|X) \dif Y
	\end{align}
	The random walk generates a recursion relationship for the distribution functions.
	\item Earlier, we asserted the asymptotic distribution sampled in the random walk would be $f(X)$. According to a theorem in Feller, if a random walk defines a system that is ergodic, then an asymptotic pdf exists and is unique if
	\begin{align}
		\phi_n (X) = f(X) \Rightarrow \phi_{n+1} (X) = f(X)
	\end{align}
	that is, if $f(X)$ is a stationary point of the recursion. Systems defined by random walks can be partitioned into several categories.
	\begin{itemize}
		\item If, in a random walk, the prob of returning to a neighborhood about $X$ is 0, then the system is called a \textit{null system} and the expected recurrence time is infinite. An example would be a one-dimensional system where $X_{n+1}$ 
	\end{itemize}
\end{itemize}
 \cite{Kalos2008}

\subsubsection{Brute Force}
We do it quite simple.

\subsubsection{Fokker-Planc and Langevin equations}
We make the proposal distribution and acceptance distribution smarter, using two complicated equations that should be derived.

\subsection{Gibbs Sampling}
If certain conditional probabilities are available to us we may simplify the Metropolis-Hastings algorithm by using them. We see then that the acceptance probability is one, that is, all proposed samples are accepted.

\subsection{Minimization}
Minimization is a huge field in itself. The algorithms proposed here will all have their basis in the gradient descent method. 

\subsubsection{Gradient Descent}
Given the gradient $G_i$ at iteration $i$ the updating scheme for the parameters $\alpha$ is then
\begin{align}
	\alpha_i = \alpha_i - \eta G_i
\end{align}
where $\eta$ is referred to as the step size or the learning rate, as is common in machine learning.

\subsubsection{Other}
We could write about Adaptive SGD here but probably some other like AdaGrad or Adam might be more interesting. Also there is some unclearity on whether we are doing stochastic gradient descent simply by doing stochastic samples or if we should also do mini-batches.

\subsection{The algorithm}
Putting it all together, we have the variational monte carlo method. 

\section{Quantum Dot}
The quantum dot may be modeled by an Hamiltonian with a harmonic oscillator potential and a Couloumb interaction potential in order to represent the interaction between particles.


\chapter{Statistical toolbox - inc methods shared by traditional and ML}
\section{Building blocks}
-the i.i.d. assumption, central limit theorem, how did this enter Mortens regression lectures, the thing i was wondering about
-variance
-covariance
-correlation
-autocorrelation - a type of cross-correlation
-autocorrelation time, effective number of measurements (mortens vmc pdf)
-time-displacement autocorrelation

-Bayesian inference
-Maximum Likelihood Estimation
-Its relation to regression and least squares
-Regression as the simplest form of supervised learning - laying the ground for DNNs in the ML chapter

there is a Bayesian formulation of linear regression, the method is called MLE, and from it can be derived the least squares method (does this mean MLE more general than LS? or are they equal?)
So the LS/regression cost function = the negative of the Bayesian log likelihood func

\section{Sampling}
\subsection{Markov Chain Monte Carlo}
\subsection{Metropolis}
ergodicity/detailed balance
with and without "hastings"
\subsubsection{Brute Force}
\subsubsection{Importance Sampling}

\subsection{Gibbs Sampling}
\subsection{Blocking}

\section{Optimization}
\subsection{Gradient Descent}
\subsubsection{Variations}
\subsection{Second order}



\chapter{Machine learning}
\section{Overview}
\section{Regression etc etc}
\section{DNNs - getting to neural networks}
This function can be represented in the form of a network diagram as shown in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward propagation of information through the network. It should be emphasized that these diagrams do not represent probabilistic graphical models of the kind to be consid- ered in Chapter 8 because the internal nodes represent deterministic variables rather than stochastic ones. For this reason, we have adopted a slightly different graphical notation for the two kinds of model. We shall see later how to give a probabilistic interpretation to a neural network.
http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} (used in the german master's thesis)


\section{Discriminative vs Generative}
Basically: want generative because we want to be able to sample.

Generative or Discriminative? Getting the Best of Both Worlds. Bruk denne !!!
%https://pdfs.semanticscholar.org/fcd8/9e702a15b67abbec1b4d81584d812f79dfda.pdf?_ga=2.136588272.1961892878.1525605616-1442489588.1525605616

DEEP DISCRIMINATIVE AND GENERATIVE MODELS FOR PATTERN RECOGNITION
https://pdfs.semanticscholar.org/cea9/c5f7117b3db7e62f35b4d290cfb84ddd7ba3.pdf

Generative classifiers learn a model of the joint prob, $p(x,y)$, of the inputs $x$ and the label $y$, and make their predictions by using Bayes rules to calc $p(y|x)$, and then picking the most likely label $y$. Discriminative classifiers model the posterior $p(y|x)$ directly, or learn a direct map from inputs $x$ to the class labels. 
https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf



\section{Graphical models - undirected=Bayesian networks(=belief networks, causal networks), directed=Markov networks (=Markov random fields)}
Literature:
Murphy and
http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} (used in the german master's thesis)

and this article

Exponential Family Harmoniums
with an Application to Information Retrieval
https://www.ics.uci.edu/~welling/publications/papers/GenHarm3.pdf

\subsection{Energy based (=a type of MRF?)}

http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf

\subsection{Random Markov Fields}
\begin{itemize}
	\item Connection to \textbf{hopfield networks} (introduced around 80ies, binary nodes) (a type of MRF). Unlike BM, units not stochastic.
	\item In probability theory and related fields, a \textbf{stochastic or random process} is a mathematical object usually defined as a collection of random variables. Historically, the random variables were associated with or indexed by a set of numbers, usually viewed as points in time, giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time
	\item A \textbf{random field} is a generalization of a stochastic process such that the underlying parameter need no longer be a simple real or integer valued "time", but can instead take values that are multidimensional vectors, or points on some manifold.
	At its most basic, discrete case, a random field is a list of random numbers whose indices are identified with a discrete set of points in a space (for example, n-dimensional Euclidean space). When used in the natural sciences, values in a random field are often spatially correlated in one way or another. In its most basic form this might mean that adjacent values (i.e. values with adjacent indices) do not differ as much as values that are further apart. This is an example of a covariance structure, many different types of which may be modeled in a random field. More generally, the values might be defined over a continuous domain, and the random field might be thought of as a "function valued" random variable.
	\item \textbf{Markov random fields} 
	In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.

	A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite.

	When the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley–Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model.

	The foundations of the theory of Markov random fields may be dound in Preston (1974) and Spitzer (1971). The concept of an MRF came from attempts to put into a general probabilistic setting a very specified model named after the German phycisist Ernst Ising, a student of Lenz. Ising published his results in 1925 while a paper by Lenz in 1920 gave a sketchy idea of the model.
	\item An RMF is a type of Product of Experts
	\item A BM is a type of RMF
\end{itemize}

Boltzmann Machines are networks just like neural nets and have units that are very similar to Perceptrons, but instead of computing an output based on inputs and weights, each unit in the network can compute a probability of it having a value of 1 or 0 given the values of connected units and weights. The units are therefore stochastic - they behave according to a probability distribution, rather than in a known deterministic way. 


In order to establish the framework in which to introduce the Boltzmann Machine in the next section, we need to leave this one off with having introduced the general form of a Random Markov Field. In order to then show the BM in next chapter on this form.

Long section from http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} follows below.

We turn now to the second ma- jor class of graphical models that are described by undirected graphs and that again specify both a factorization and a set of conditional independence relations.
A Markov random field, also known as a Markov network or an undirected graphical model (Kindermann and Snell, 1980), has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes. The links are undirected, that is they do not carry arrows. In the case of undirected graphs, it is convenient to begin with a discussion of conditional independence properties.

Let us denote a clique by $C$ and the set of variables in that clique by $\bm{x}_C$. Then the joint distribution is written as a product of \textit{potential functions} $\phi$ over the maximal cliques of the graph
\begin{align}
	p(\bm{x}) = \frac{1}{Z} \prod_C \phi_C (\bm{x}_C)
\end{align} 
where $Z$, sometimes called the \textit{partition function}, is a normalization constant given by
\begin{align}
	Z = \sum_{\bm{x}} \prod_C \phi_C (\bm{x}_C)
\end{align}
By considering only potential functions which satisfy $\phi_C (\bm{x}_C) \geq 0$ we ensure that $p(\bm{x}) \geq 0$. Here $\bm{x}$ assumed to comprise discrete variables, but it can also be continuous or a combination of the two, given that the summation in $Z$ is replaced by the appropriate combination of summation and integration.

The presence of this normalization constant is one of the major limitations of undirected graphs. If we have a model with $M$ discrete nodes each having $K$ states, then the evaluation of the normalization term involves summing over $K^M$ states and so (in the worst case) is exponential in the size of the model. 

Have so far
\begin{enumerate}
	\item Discussed the notion of conditional independence based on simple graph separation
	\item Proposed a factorization of the joint distribution that is intended to correspond to this conditional independence structure
\end{enumerate}
In order to make a formal connection/ precise relationship between factorization for undirected graphs and conditional independence we need to restrict attention to potential functions $\phi_C (\bm{x}_C)$ that are \textit{strictly} positive.

Define
\begin{itemize}
	\item $\mathcal{U I}$ = the set of \textit{all possible distributions defined over a fixed set of variables corresponding to the nodes of a particular undirected graph} that are consistent with the set of conditional independence statements that can be read from the graph using graph separation.
	\item $\mathcal{U F}$ = the set of \textit{all possible distributions defined over a fixed set of variables corresponding to the nodes of a particular undirected graph} that can be expressed as a factorization of the form presented above with respect to the maximal cliques of the graph. 
\end{itemize}
The Hammersley-Clifford theorem (Clifford, 1990) states that the sets $\mathcal{U I}$ and $\mathcal{U F}$ are identical.

Because we are restricted to potential functions which are strictly positive it is convenient to express them as exponentials, so that
\begin{align}
	\phi_C (\bm{x}_C) = e^{-E_C(\bm{x}_C)}
\end{align}

where $E(\bm{x}_C)$ is called an \textit{energy function}, and the exponential representation is called the \textit{Boltzmann distribution}. The joint distribution is defined as the product of potentials, and so the total energy is obtained by adding the energies of each of the maximal cliques.

The joint distribution is then
\begin{align}
	p(\bm{x}) =& \frac{1}{Z} \prod_C \phi_C (\bm{x}_C) \nonumber \\
	=& \frac{1}{Z} \prod_C e^{-E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-\sum_C E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-E(\bm{x})}
\end{align} 

The set of random variables $\bm{x}$ then forms a Markov random field with respect to $G$ (the undirected graph to which the set of cliques $C$ belong). 






\chapter{The Restricted Boltzmann Machine}

\section{The Boltzmann Machine}

Necessity of hidden units, free energy, relation to logistic regression. \cite{Osogami2017}

A general energy expression
\begin{align}
	E_{BM}(\bm{x}, \bm{h}) =& - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j) \nonumber \\
	&- \sum_{i, m=i+1, k}^{M, M, K} \alpha_i^k (x_i) v_{im}^k \alpha_m^k (x_m)
	- \sum_{j,n=j+1,l}^{N,N,L} \beta_j^l (h_j) u_{jn}^l \beta_n^l (h_n)
\end{align}
with
\begin{itemize}
	\item $\alpha_i^k (x_i), \beta_j^l (h_j) $= One dimensional transfer functions, mapping a given input value to a desired feature value. They are sufficient statistics of the model and can be arbitrary non-parametrized functions of the input variable $x_i$ or $h_j$ respectively, but they need to be independent of the parametrization.
	\item $k, l $= These indices denote there can be multiple transfer funcs pr variable.
	\item $a_i^k,  b_j^l$= appear in the first and second term which only depends ont he visible and hidden units respectively. Thus they could be interpreted as the corresponding visible and hidden bias respectively.
	\item $w_{ij}^{kl}$ = inter layer connection term, connects the visible and hidden bias, respectively.
	\item $ v_{im}^k, u_{jn}^l$ = intra layer connection terms, connecting the visible units to each other, and the hidden units to each other, respectively.
\end{itemize}

\subsection{Restricted Boltzmann Machines}
\subsubsection{Energy Function}
OBS: from the high bias - low variance review: the free-energy of the marginalized probability shows the modeling of higher order interactions.

The Boltzmann machine is restricted by removing intra layer connections, meaning nodes within the same layer are conditionally independent. This is done setting $v_{im}$ and $u_{jn}$ to zero. The structure is then a bipartite graph. The expression for the energy of the RBM is then
\begin{align}
	E_{RBM}(\bm{x}, \bm{h}) = - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)
\end{align}

%\subsubsection{Joint Probability Density Function}

\subsubsection{Marginal Probability Density Functions}
\begin{align}
	P_{RBM} =& \int P_{RBM} (\bm{x}, \tilde{\bm{h}}) \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\bm{x}, \tilde{\bm{h}}) } \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} 
	\dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\int \prod_j^N e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\biggl( \int e^{\sum_l b_1^l \beta_1^l (\tilde{h}_1) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i1}^{kl} \beta_1^l (\tilde{h}_1)} \dif  \tilde{h}_1 \nonumber \\
	& \times \int e^{\sum_l b_2^l \beta_2^l (\tilde{h}_2) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i2}^{kl} \beta_2^l (\tilde{h}_2)} \dif  \tilde{h}_2 \nonumber \\
	& \times ... \nonumber \\
	& \times \int e^{\sum_l b_N^l \beta_N^l (\tilde{h}_N) + \sum_{i,k,l} \alpha_i^k (x_i) w_{iN}^{kl} \beta_N^l (\tilde{h}_N)} \dif  \tilde{h}_N \biggr) \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j
\end{align}

Similarly
\begin{align}
	P_{RBM} (\bm{h}) =& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\tilde{\bm{x}}, \bm{h})} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{j, l} b_j^l \beta_j^l (h_j)}
	\prod_i^M \int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i
\end{align}

\subsubsection{Conditional Probability Density Functions}
Using Bayes theorem:
\begin{align}
	P_{RBM} (\bm{h}|\bm{x}) =& \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (h_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j} \nonumber \\
	=& \prod_j^N \frac{e^{\sum_l b_j^l \beta_j^l (h_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)} }
	{\int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j}
\end{align}
Similarly
\begin{align}
	P_{RBM} (\bm{x}|\bm{h}) =&  \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{h})} \nonumber \\
	=& \prod_i^M \frac{e^{\sum_k a_i^k \alpha_i^k (x_i)
	+ \sum_{j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i}
\end{align}


\subsection{Binary-Binary Restricted Boltzmann Machines}
\subsubsection{Energy Function}
\subsubsection{Joint Probability Density Function}
\subsubsection{Marginal Probability Density Functions}
\subsubsection{Conditional Probability Density Functions}

\subsection{Gaussian-Binary Restricted Boltzmann Machines}
\subsubsection{Energy Function}
We derive this from the general RBM by using $K=3$ and $L=1$ and setting the transfer functions 
\begin{align}
	\alpha_i^1 (x_i) &= -x_i^2 , \\
	\alpha_i^2 (x_i) &= x_i , \\
	\alpha_i^3 (x_i) &= 1 , \\
	\beta_j^1 (h_j) &= h_j , \\
\end{align}
and the parameters to
\begin{align}
	a_i^1 &= \frac{1}{2\sigma_i^2} , \\
	a_i^2 &= \frac{a_i}{\sigma_i^2} , \\ 
	a_i^3 &= -\frac{a_i^2}{2\sigma_i^2} , \\
	b_j^1 &= b_j , \\
	w_{ij}^{11} &= 0 , \\
	w_{ij}^{21} &= \frac{w_{ij}}{\sigma_i^2} , \\
	w_{ij}^{31} &= 0
\end{align}


Inserting this into $E_{RBM}(\bm{x},\bm{h})$ results in the energy
\begin{align}
	E_{GB}(\bm{x}, \bm{h}) =& \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	- \sum_j^N b_j h_j 
	-\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2} \nonumber \\
	=& \norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 - \bm{b}^T \bm{h} 
	- (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}
\end{align}

\subsubsection{Joint Probability Density Function}
The energy results in the joint probability
\begin{align}
	P_{GB} (\bm{x}, \bm{h}) =& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{- \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ \sum_j^N b_j h_j 
	+\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} e^{\frac{(x_i - a_i)^2}{2\sigma_i^2}
	- b_j h_j 
	-\frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} \phi_{GB_{ij}} (x_i, h_j)
\end{align}
with the partition function
\begin{align}
	Z_{GB} =& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \dif \tilde{\bm{x}} \nonumber \\
	=& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} \prod_{ij}^{M, N} \phi_{GB_{ij}} (\tilde{x}_i, \tilde{h}_j) \dif \tilde{\bm{x}}
\end{align}


\subsubsection{Marginal Probability Density Functions}
\begin{align}
	P_{GB} (\bm{x}) =& \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} P_{GB} (\bm{x}, \tilde{\bm{h}}) \nonumber \\
	=& \frac{1}{Z_{GB}} \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} 
	e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) \nonumber \\
\end{align}

Furthermore we have, using the factorization property
\begin{align}
	P_{GB} (\bm{h}) =& \int P_{GB} (\tilde{\bm{x}}, \bm{h}) \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} \int e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} } \int \prod_i^M
	e^{- \frac{(\tilde{x}_i - a_i)^2}{2\sigma_i^2} + \frac{\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{\sigma_i^2} } \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} }
	\biggl( \int e^{- \frac{(\tilde{x}_1 - a_1)^2}{2\sigma_1^2} + \frac{\tilde{x}_1 \bm{w}_{1\ast}^T \bm{h}}{\sigma_1^2} } \dif \tilde{x}_1 \nonumber \\
	& \times \int e^{- \frac{(\tilde{x}_2 - a_2)^2}{2\sigma_2^2} + \frac{\tilde{x}_2 \bm{w}_{2\ast}^T \bm{h}}{\sigma_2^2} } \dif \tilde{x}_2 \nonumber \\
	& \times ... \nonumber \\
	&\times \int e^{- \frac{(\tilde{x}_M - a_M)^2}{2\sigma_M^2} + \frac{\tilde{x}_M \bm{w}_{M\ast}^T \bm{h}}{\sigma_M^2} } \dif \tilde{x}_M \biggr) \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - a_i)^2 - 2\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \tilde{x}_i \bm{w}_{i\ast}^T \bm{h}) + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \bm{w}_{i\ast}^T \bm{h}) + (a_i + \bm{w}_{i\ast}^T \bm{h})^2 - (a_i + \bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - (a_i + \bm{w}_{i\ast}^T \bm{h}))^2 - a_i^2 -2a_i \bm{w}_{i\ast}^T \bm{h} - (\bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}
	\int e^{- \frac{(\tilde{x}_i - a_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}}
	\dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}
	\nonumber \\
\end{align}

Thus we can calculate the partition function, using factorization..?

\subsubsection{Conditional Probability Density Functions}
\begin{align}
	P_{GB} (\bm{h}| \bm{x}) =& \frac{P_{GB} (\bm{x}, \bm{h})}{P_{GB} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) }
	\nonumber \\
	=& \prod_j^N \frac{e^{(b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j})h_j } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N P_{GB} (h_j|\bm{x})
\end{align}

Resulting in the probability of a particular hidden unit being activated or not being
\begin{align}
	P_{GB} (h_j =1 | \bm{x}) =& \frac{e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j} } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	&= \frac{1}{1 + e^{-b_j - (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}}
\end{align}
and
\begin{align}
	P_{GB} (h_j =0 | \bm{x}) =&
	\frac{1}{1 + e^{b_j +(\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}}
\end{align}

We further have
\begin{align}
	P_{GB} (\bm{x}|\bm{h})
	=& \frac{P_{GB} (\bm{x}, \bm{h})}{P_{GB} (\bm{h})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{- \frac{(x_i - a_i)^2}{2\sigma_i^2} + \frac{x_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} }}
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{-\frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h} }{2\sigma_i^2} } }
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{- \frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h}
	+ 2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2}
	{2\sigma_i^2} }
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{ - \frac{(x_i - b_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}} \nonumber \\
	=& \prod_i^M \mathcal{N}
	(x_i | b_i + \bm{w}_{i\ast}^T \bm{h}, \sigma_i^2)
\end{align}

\subsection{Gaussian-something continuous RBM?}

\section{Training the RBM}
\subsection{Sampling}
\subsection{Cost function}
\subsection{Optimization}
\subsection{Kullback-Leibler Divergence}

\section{The Restricted Boltzmann Machine - References}

\subsection{The history}

\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2004 Welling, Rosen-Zvi, Hinton: Exponential Family Harmoniums with an Application to Information Retrieval} \cite{Welling2005}
\end{itemize}
	\item First introcued in
	\begin{itemize}
		\item 1986 Smolensky: Harmoniums (Information processing in dynamical systems: foundations of harmony theory) \cite{Smolensky1986}
	\end{itemize}
	\item Later papers have studied it under various names:
	\begin{itemize}
		\item 1992 Y Freund, D Haussler: The combination machine (Unsupervised learning of distributions of binary vectors using 2-layer networks) \cite{Freund1992}
		\item 2002 Hinton: The RBM (Training product of experts by minimizing contrastive divergence) \cite{Hinton2002}
	\end{itemize}
\end{itemize}

\subsection{Gaussian-Binary RBM history}

\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2017: Wang, Melchior, Wiskott: Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics} \cite{Melchior2017}
\end{itemize}
	\item First proposed by
	\begin{itemize}
		\item 2004 Welling, Rosen-Zvi, Hinton: Exponential Family Harmoniums with an Application to Information Retrieval \cite{Welling2005}
	\end{itemize}
	\item A common choice når man trenger cont visibles, ref
	\begin{itemize}
		\item 2009 A. Krizhevsky: Learning multiple layers of features from tiny images (master's thesis) \cite{Krizhevsky2009}
		\item 2011 Cho, Ilin, Raiko: Improved learning of gaussian-bernoulli restricted boltzmann machines \cite{Cho2011}
	\end{itemize}
	\item Training known to be hard, modifiactions to improve it proposed by:
	\begin{itemize}
		\item 2007 Lee, Ekanadham, Ng: Used a sparse penalty during training, allowing them to learn meaningful features from natural image patches (Sparse deep belief net model for visual area v2) \cite{Lee2008}
		\item 2009 A. Krizhevsky: Trained GRBMs on natural images and concluded the difficulties are mainly due to the existence of high-frequency noise in the images, which further prevents the model from learning the important structures. (referenced above) \cite{Krizhevsky2009}
		\item 2011 Theis, Gerwinn, Sinz, Bethge: Illustrates that in terms of likelihood estimation GRBMs are already outperformed by simple mixture models. (In all likelihood, deep belief is not enough) \cite{Theis2011}
		\item Focus on improving the model in the view of generative models
		\begin{itemize}
			\item 2010 Ranzato, Krizhevsky, Hinton: Factored 3-way restricted boltzmann machines for modeling natural images \cite{Ranzato2010}
			\item 2010 Ranzato, Hinton: Modeling pixel means and covariances using factorized third-order boltzmann machines \cite{Ranzato2010a}
			\item 2011 Courville, Bergstra, Bengio: A spike and slab restricted boltzmann machine \cite{Courville2011}
			\item 2011 Le Roux, Heess, Shotton, Winn: Learning a generative model of images by factoring appearance and shape \cite{LeRoux2011}
		\end{itemize}
		\item 2011 Cho, Ilin, Raiko: Suggested the failure of GRBMs is due to the training algo and proposed some modifications to overcome the difficulties encountered in training GRBMs (referenced above) \cite{Cho2011}
	\end{itemize}
	\item All these studies have shown the failures of GRBMs empirically, but to our knowledge there is no analysis of GRBMs apart from out preliminary work:
	\begin{itemize}
		\item 2012 Wang, Melchior, Wiskott: (An analysis of gaussian-binary boltzmann machines for natural image) \cite{Wang2012}
	\end{itemize}
	which accounts the reasons behind these failiures. In this paper, we extend our work in which we consider GRBMs from the perspective of density models, i.e. how well the model learns the dist of the data.
	\item We show an GB-RBM can be regarded as a mixture of Gaussians, which has already been mentioned briefly in previous studies:
	\begin{itemize}
		\item 2009 Bengio: Learning deep architectures for AI \cite{Bengio2009}
		\item 2011 Theis, Gerwinn, Sinz, Bethge: referenced above \cite{Theis2011}
		\item 2011 Courville, Bergstra, Bengio: referenced above \cite{Courville2011}
	\end{itemize}
	but has gone unheeded. This formulation makes clear that GRBMs are quite limited in the way they can represent data. However we argue this fact does not necessarily prevent the model from learning the statistical structure in the data. 
	\item We present successful training of GRBMs both on a two-dimensional blind source separation problem and natural image patches, and that the results are comparable to that of independent component analysis (ICA). 
	\item Based on our analysis we propose several training recipes, which allowed successful and fast training in our experiments. 
	\item Finally, we discuss the relationship between GRBMs and above mentioned modifications of the model.
\end{itemize}


\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2012 Melchoir: Learning natural image statistics with gaussian-binary restricted boltzmann machines (Master's thesis)} \cite{Melchior2012}
\end{itemize}
	\item A popular variant of RBM is GB-RBM
	\begin{itemize}
		\item 2004 Welling, Rosen-Zvi, Hinton: \cite{Welling2005} referenced above
	\end{itemize}
	\item Training difficulties
	\begin{itemize}
		\item 2010 Fischer, Igel: RBMs difficult to train (Markov-random-fields und boltzmann maschinen)
		\item 2012 Wang, Melchior, Wiskott: This even more critical when using GB-RBMs (referenced above) \cite{Wang2012}
	\end{itemize}
	Several modifications proposed to overcome training difficulties
	\begin{itemize}
		\item 2007 Lee, Ekanadham, Ng: Added a sparseness penalty on the gradient that forced the model to prefer sparse representations and seems to help learning meaningful features (referenced above) \cite{Lee2008}
		\item 2011 Cho, Ilin, Raiko: Suggested the training failure is due to the training algo and proposed several improvements to overcome the problem (referenced above) \cite{Cho2011}
		\item 2009 A. Krizhevsky: Successfully trained a deep hierarchical network and concluded that a failure is mainly because of the existence of high-frequency noise in natural images, which prevents the model from learning the important structures. (referenced above) \cite{Krizhevsky2009}
		\item Other approaches modified the model such that it is capable of modelling higher order statistics directly:
		\begin{itemize}
			\item 2011 Courville, Bergstra, Bengio: referenced above \cite{Courville2011}
			\item 2010 Ranzato, Hinton: referenced above \cite{Ranzato2010a}
			\item 2010 Ranzato, Krizhevsky, Hinton: referenced above \cite{Ranzato2010}
		\end{itemize}
		All modifications showed that BG-RBMs are in principle capable of learning features comparable to the receptive fields in the early primary visual cortex V1, but in practice this is difficult to achieve.
		\item To derive a better understanding of the limitations of the model, the authors in
		\begin{itemize}
			\item 2011 Le Roux, Heess, Shotton, Winn: ref above \cite{LeRoux2011}
		\end{itemize} 
		evaluated its capabilities from the perspective of image reconstruction. In
		\begin{itemize}
			\item 2011 Theis, Gerwinn, Sinz, Bethge: ref above \cite{Theis2011}
		\end{itemize} 
		the likelihood of the model is compared to classical machine learning methods. Although the model has been analysed to show the failures empirically, there are few works accounting for the failure analytically.
	\end{itemize}
	\item Other interesting points made:
	\begin{itemize}
		\item 1986 Hinton, Sejnowski: The Boltzmann machine (Learning and relearning in boltzmann machines) \cite{Hinton1986}
		\item 2006 Bishop: The BM is an undirected probabilistic graphical model (Pattern recognition  and Machine Learning, chapter 8) \cite{Bishop2006}
		\item with stochastic continu- ous or discrete units. It is often interpreted as a stochastic recurrent neural network where the state of each unit depends on the units it is connected to. The original BM has a fully connected graph with binary units, which turns into a Hopfield net if we choose deterministic rather than stochastic units. But in contrast to Hopfield nets, a BM is a generative model that allows to generate new samples from the learned distribution.
		\item 2009 Bengio: The BM's stackability allows for constructing deep networks \cite{Bengio2009} (ref above)
		\item 2009 Krizhevsky: BMs popular in the field of feature extraction (ref above) \cite{Krizhevsky2009}
		\item 2006 Hinton, Salakhutdinov: BMs popular in the field of dimensionality reduction (Reducing the dimensionality of data with neural networks. SCIENCE) \cite{Hinton2006}
		\item 2006 Bishop: A BM is a special case of a Markov Random Field (MRF) \cite{Bishop2006} (ref above)
		\item 2002 Hinton: An MRF is itself a special case of a Product of Experts (PoE) (ref above) \cite{Hinton2002}
		\item 2010 Fischer, Igel: A PoE model with exponential experts = an MRF with input variables $\bm{x}$ and latent variables $\bm{h}$ - This is shown by the Hammersley-Clifford Theorem (=\textbf{The fundamental theorem of random fields}, a result that gives necessary and sufficient conditions under which a positive prob dist can be represented as a Markov network/Markov random field (Wikipedia)) (ref above)
		\item While an MRF is a particular case of a PoE, a BM is an MRF with a particular energy function that leads to a complete undirected graph. This implies a fully connected network where the pairwise communication between two units is symmetrical.
		\item 2010 Ranzato, Hinton: Can make even complexer BMs where more than two units interact, named higher order BMs (ref above) \cite{Ranzato2010a}
		\item An important subclass of BMs having a restricted communication structure allows an efficient calculation of the conditional probabilities. So that fast inference is possible, which made restricted BMs become very popular over the last decade.
		\item 1985 Ackley, Hinton, Sejnowski: The original definition of BMs. Here, visible and hidden units had binary values. (A learning algorithm for boltzmann machines) \cite{Ackley1985}
		\item Discussion on options for making the visibles continuous, pros/cons of the options
		\begin{itemize}
			\item 2009 Larochelle, Bengio, Lourdaour, Lamblin: Truncated Exponential RBMs (Exploring strategies for training deep neural networks.)
		\end{itemize}
		\item GB-RBM: \textbf{We assume the visibles to be Gaussian distributed, and therefore, a distribution over} $\mathbb{R}$. Appereantly it's natural to assume continuous variables to be Gaussian distributed. But OK in my case????
		\item 2006 Hinton, Salakhutdinov: The GB-RBM. (ref above - Reducing the dimensionality of data with neural networks) \cite{Hinton2006}
		\item Presents GB-RBM energy function from the general RBM one, ending up with
		\begin{align}
			E^{GB} (\bm{x}, \bm{h}) 
			=& \sum_i^N \frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^M b_j h_j - \sum_{i,j}^{N, M} \frac{x_i w_{ij} h_j}{\sigma_i^2} \\
			=& \sum_i^N ||\frac{\bm{x} - \bm{b}}{2 \bm{\sigma}}||^2 - \bm{b}^T \bm{h} - (\frac{\bm{x}}{\sigma^2})^T \bm{W} \bm{h}
		\end{align}
		where the second equation is given in clearer matrix vector notation and the fraction bar denotes the component wise division.
		\item Notice that there exists a slightly different formulation of the GB-RBM energy
		\begin{itemize}
			\item 2009 Krizhevsky: ref above \cite{Krizhevsky2009}
		\end{itemize}
		where the quadratic term uses $\sigma_i$ instead of $\sigma_i^2$. But as stated in
		\begin{itemize}
			\item 2011 Cho, Ilin, Raiko: ref above \ref{Cho2011}
		\end{itemize}
		this leads to a counter intuitive scaling of the conditional mean by $\sigma_i^2$, so that in this work a GB-RBM is always considered to be defined as above.
		\item Parallel tempering: an algorithm that provides a fast mixing rate and surprisingly, we already know all concepts this algorithm is working with. First of all let us reconsider the PDF of MRFs (19) where we defined the temperature parameter $T \in [1, \infty)$,  which we discarded up to now. It scales the energy down, which leads to a regularization of the PDF’s manifold. This becomes clear if we think of that the energy is applied to an exponential function to calculate the probability. If we choose a big temperature the energy is scaled down, which leads to more equally distributed probabilities, due to nature of the exponential function.
		Therefore, we can use the temperature to generate samples, which are distributed more homogeneously.
		The idea of PT is to run several Markov chains on different temperatures. We start Gibbs sampling from the highest temperature where all samples have the same probability. While continuing the sampling procedure, the temperature is lowered, which has the effect that regions of higher density are coming up. If the decreasing of the temperatures is smooth enough, the samples will move to all regions of higher density. This generates samples that are likely from all modes of the distribution which is illustrated in Figure 12.
	\end{itemize}
	\item From the section Analysis of GB-RBMs
	\begin{itemize}
		\item In general, a profound understanding of a model, its capabilities and limitations, requires a clear understanding of how it models data. For probabilistic models like BMs, accordingly, we need to understand how the marginal probability distribution of the input data is structured.
		\item \textbf{BB-RBM}: Figure 15 shows the marginal probability density $P^{BB} (\bm{x})$ of a BB-RBM with two visible units $x_1$, $x_2$ and two hidden units $h_1$, $h_2$. The two visible units can take the four possible states $\bm{x} \in \{ 0,1 \}^2$, which correspond to the four positions on the plain. The probability for each state, illustrated as cylinders depend on the product of the visible experts, $e_{x1}$, $e_{x2}$. The experts themselves, referring to (39) are sigmoid functions, which depend on the hidden units and the corresponding weights. The steepness of the experts’ sigmoid, controlled by the weights, defines how likely it is to switch from an active to an inactive state and vice versa. Figure 15 also implies that RBMs can be universal approximators
		\begin{itemize}
			\item 2008 Le Roux, Bengio: Representational power of restricted boltzmann machines and deep belief networks. \cite{LeRoux2008} 
		\end{itemize}
		Let $N$ be the number of visible units and $K \leq \{ 0,1 \}^N$ be the total number of states of the PDF we want to learn. We are able to model the distribution exactly if we have one hidden unit per visible state plus a bias unit, hence $M=2^N + 1$ hidden units.
		\item Similar to the illustration for a BB-RBM we are able to illustrate the marginal PDF for a GB-RBM. Referring to (145), the experts marginal PDF has a rather unintuitive form where one expert is an unnormalized Gaussian with mean $\bm{b}$ and the other $M$ experts are the sum of the value of one and an exponential function. But we are able to derive a more intuitive formulation of the marginal PDF using the Bayes’theorem and the polynomial expansion as proposed in
		\begin{itemize}
			\item 2012 Wang, Melchior, Wiskott: ref above \cite{Wang2012}
		\end{itemize}
		so we get
		\begin{align}
			P(\bm{x}) &= \sum_{\bm{h}} P(\bm{x}|\bm{h}) P(\bm{h}) \\
			&= \mathcal{N} (\bm{x}; \bm{b} + \bm{w}_{})
		\end{align}
	\end{itemize}
\end{itemize}
Melchior's rewriting of $P(x)$ as a Mixture of Gaussians written out for $M=2$ and $N=2$:
\begin{align}
	P(\bm{x}) =& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \sum_{j=1}^{N=2} \eta_j \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_j, \bm{\sigma}^2) \nonumber \\
	&+ \sum_{j=1}^{N-1=1} \sum_{k>j}^{N=2} \eta_{jk} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_j + \bm{w}_k, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_2, \bm{\sigma}^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_1 + \bm{w}_2, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(x_1|b_1, \sigma_1^2)\mathcal{N}(x_2|b_2, \sigma_2^2) \nonumber \\
	&+ \eta_1 \mathcal{N} (x_1| b_1 + w_{11}, \sigma_1^2)\mathcal{N} (x_2| b_2 + w_{21}, \sigma_2^2) \nonumber \\
	&+ \eta_2 \mathcal{N} (x_1| b_1 + w_{12}, \sigma_1^2) \mathcal{N} (x_2| b_2 + w_{22}, \sigma_2^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (x_1|b_1+ w_{11} + w_{12}, \sigma_1^2) \mathcal{N} (x_2|b_2+ w_{21} + w_{22}, \sigma_2^2) \\
\end{align}
where
\begin{align}
	\eta_0 =& \frac{(\sqrt{2\pi \sigma_i^2})^M}{Z} =  \frac{2\pi \sigma_i^2}{Z} \\
	\eta_j =& \eta_0 e^{\frac{||\bm{b} + \bm{w}_j||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j} \\
	\eta_{jk} =& \eta_0 e^{\frac{||\bm{b} + \bm{w}_j + \bm{w}_k||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j + c_k}
\end{align}
giving us
\begin{align}
	P(\bm{x}) =& \eta_0 \mathcal{N}(x_1|b_1, \sigma_1^2)\mathcal{N}(x_2|b_2, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_1||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1}
	\mathcal{N} (x_1| b_1 + w_{11}, \sigma_1^2)\mathcal{N} (x_2| b_2 + w_{21}, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_2}
	 \mathcal{N} (x_1| b_1 + w_{12}, \sigma_1^2) \mathcal{N} (x_2| b_2 + w_{22}, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_1 + \bm{w}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1 + c_2}
	 \mathcal{N} (x_1|b_1+ w_{11} + w_{12}, \sigma_1^2) \mathcal{N} (x_2|b_2+ w_{21} + w_{22}, \sigma_2^2)  \\
\end{align}
Or write
\begin{align}
	\bm{\mu}_0 =& \bm{b} \\
	\bm{\mu}_j =& \bm{b} + \bm{w}_j \\
	\bm{\mu}_{jk} =& \bm{b} + \bm{w}_j + \bm{w}_k \\
	\Rightarrow 
	\eta_0 =& \frac{(\sqrt{2\pi \sigma_i^2})^M}{Z} =  \frac{2\pi \sigma_i^2}{Z} \\
	\eta_j =& \eta_0 e^{\frac{||\bm{\mu}_j||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j} \\
	\eta_{jk} =& \eta_0 e^{\frac{||\bm{\mu}_{jk}||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j + c_k}
\end{align}
then
\begin{align}
	P(\bm{x})=& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_2, \bm{\sigma}^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_1 + \bm{w}_2, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{\mu}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{\mu}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{\mu}_2, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{\mu}_{12}, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{\mu}_0, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_1||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1} 
	\mathcal{N} (\bm{x}| \bm{\mu}_1, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_2} 
	\mathcal{N} (\bm{x}| \bm{\mu}_2, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_{12}||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1 + c_2}
	\mathcal{N} (\bm{x}|\bm{\mu}_{12}, \bm{\sigma}^2) \\
	\text{using that } 
	\mathcal{N}(\bm{x}| \bm{\mu}, \bm{\Sigma}) 
	=& \frac{1}{\sqrt{(2\pi)^M |\Sigma|}} e^{-\frac{1}{2} (\bm{x}-\bm{\mu})^T\Sigma^{-1} (\bm{x}-\bm{\mu})} %\nonumber \\
	= \frac{1}{\sqrt{(2\pi\sigma_i^2)^M}} e^{-\frac{||\bm{x}-\bm{\mu}||^2}{2\bm{\sigma}^2}} \nonumber \\
	=& \frac{1}{Z} e^{-\frac{||\bm{x}-\bm{\mu}_0||^2}{2\bm{\sigma}^2}} \nonumber \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_1||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_1||^2}{2\bm{\sigma}^2} + c_1} \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_2||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_2||^2}{2\bm{\sigma}^2} + c_2}  \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_{12}||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_{12}||^2}{2\bm{\sigma}^2} + c_1 + c_2} \\
\end{align}
... and if we continue as above I suspect we'll end up with the "origininal non-MoG" form.
\subsection{More on RBMs and GB-RBMs}

\begin{itemize}
	\item \textbf{From}
	\begin{itemize}
		\item \textbf{2010 Nair, Hinton: Rectified linear units improve restricted boltzmann machines} \cite{Nair2010}
	\end{itemize}
	\item RBMs have been used as generative models of many different types of data including
	\begin{itemize}
		\item 2010 Mohamed, Hinton: Sequences of mel-cepstral coefficients that represent speech (Phone recognition using restricted boltzmann machines)
		\item 2009 Hinton, Salakhutdinov: Bags of words that represent documents (Replicated softmax: an undirected topic model)
		\item 2007 Salakhutdinov, Mnih, Hinton: User ratings of movies (Re- stricted Boltzmann machines for collaborative filtering)
		\item 2006 Taylor, Hinton, Roweis: In their conditional form they can be used to model high-dimensional temporal sequences such as video or motion capture data. (Modeling hu- man motion using binary latent variables)
		\item 2006 Hinton, Osindero, Teh: Their most important use is as learning modules that are composed to form deep belief nets (A fast learning algorithm for deep belief nets)
	\end{itemize}
	\item More
	\begin{itemize}
		\item 2002 Hinton: RBMs originally developed using binary stochastic units for both visible and hidden layers (ref above - Training product of experts by minimizing contrastive divergence) \cite{Hinton2002}
		\item 2006 Hinton, Salakhutdinov: \textbf{To deal with real-valued data such as the pixel intensities in natural images, they replaced the binary visible units by linear units with dependent Gaussian noise}. (ref above - Reducing the dimensionality of data with neural networks) \cite{Hinton2006}
		\item 1994 Freund, Haussler: \textbf{This was first suggested here} (Unsupervised learning of distributions on binary vectors using two layer networks) \cite{Freund1992}
	\end{itemize}
	\item \textbf{OBS, std only squared in one term in this GB-RBM? (the visible bias term and not the weight one. Also only visible bias one divided by 2)}
	\item It is possible to learn the variance of the noise for each visible unit but this is difficult using binary hidden units. In many applications, it is much easier to first normalise each component of the data to have zero mean and unit variance and then to use noise-free re- constructions, with the variance in equation 6 set to 1. The reconstructed value of a Gaussian visible unit is then equal to its top-down input from the binary hid- den units plus its bias. We use this type of noise-free visible unit for the models of object and face images described later.
	\item Thorough discussion of ReLUs and modification: It is possible, however, to use a fast approximation in which the sampled value of the rectified linear unit is not constrained to be an integer. Instead it is given by $max(0, x+N(0, \sigma(x)))$ where $N(0, V)$ =Gaussian noise w/zero mean and variance V. We call a unit that uses this approximation a Noisy Rectified Linear Unit (NReLU) and \textbf{this paper shows that NReLUs work better than binary hidden units for several different tasks}. We also give an approximate probabilistic interpretation for the $max(0,x)$ nonlinearity, further justifying their use.
	\item We have shown that NReLUs work well for discrimina- tion, \textbf{but they are also an interesting way of modeling the density of real-valued, high-dimensional data}.
	\begin{itemize}
		\item A standard way to do this is to use a mixture of diag- onal Gaussians. Alternatively we can use a mixture of factor analysers. Both of these models are expo- nentially inefficient if the data contains componential structure. Consider, for example, images of pairs of independent digits. If a mixture model for single digit images needs $N$ components, a single mixture model of pairs of digits needs $N^2$ components. Fortunately, this exponential growth in the number of components in the mixture can be achieved with only linear growth in the number of latent variables and quadratic growth in the number of parameters if we use rectified linear hidden units.
		\item Consider using rectified linear units with zero bias to model data that lies on the surface of a unit hyper- sphere. Each rectified linear unit corresponds to a plane through the centre of the hypersphere. It has an activity of 0 for one half of the hypersphere and for the other half its activity increases linearly with distance from that plane. $N$ units can create $2^N$ re- gions on the surface of the hypersphere (assuming the hypersphere is at least $N$-dimensional). As we move around within each of these regions the subset of units that are non-zero does not change so we have a lin- ear model, but it is a different linear model in every region. The mixing proportions of the exponentially many linear models are defined implicitly by the same parameters as are used to define $p(\bm{v}|\bm{h})$ and, unlike a directed model, the mixing proportions are hard to compute explicitly 
		\begin{itemize}
			\item 2008 Nair, Hinton: Implicit mixtures of restricted boltzmann machine
		\end{itemize}
		\item This is a much better way of implementing an exponentially large mixture of linear models with shared latent variables than the method described in
		\begin{itemize}
			\item 1999 Hinton, Sallanes, Ghahramani: A hierarchical community of experts
		\end{itemize}
		 which uses directed linear models as the components of the mixture and a separate sig- moid belief net to decide which hidden units should be part of the current linear model. In that model, it is hard to infer the values of the binary latent variables and there can be jumps in density at the boundary be- tween two linear regions. A big advantage of switch- ing between linear models at the point where a hidden unit receives an input of exactly zero is that it avoids discontinuities in the modeled probability density.
	\end{itemize}
\end{itemize}

\chapter{The RBM method for the quantum problem}
\subsection{How is $\Psi$ modeled}
\subsection{Further analysis of the GB-RBM - the marginal prob as a MoG}
\subsection{Training - now using the variational principle}
\subsection{Hyperparameters}
\subsubsection{Initialization}
\subsubsection{Sampling parameters (Metropolis)}
\subsubsection{Number of hidden nodes}
\subsubsection{Number of samples}
\subsubsection{Learning rate}
\subsection{Computational efficiency}
Choosing a constant density $\alpha$ of hidden variables pr physical positions, i.e. $M=\alpha N$, the number of variational parameters scales as $\alpha N^2$.

\part{Implementation}
\section{Workflow}
\section{How to use..?}
\subsection{Monetoring the learning progress?}

\section{Structure/any particular reason for choices..?}
Conditionals vs. polymorhisms.

\part{Results and analysis}
\chapter{Making the basic method stripped for extra stuff work - grid search and validation}
Jeg tenker vi tar utgangspkt i Metropolis importance sampling, den virker best, i den mest omfattende utredelsen av hyperparameter search. Så kan Gibbs og metropolis brute force være "artig å sammenligne med". Hele veien i testingen har vi mulighet til å teste med eller uten interaksjon. Men siden uten har veldig god presisjon, og med er den store utfordringen, så kan kanskje hyperparameter søket gjøres MED. Så er det bare å holde track på at ikke uten har blitt dårligere liksom.
Starter også med sigma=1
Enkleste gradient descent
\section{Hyperparameter grid search}
\begin{enumerate}
	\item The first search:
	\begin{itemize}
		\item What we keep constant:
		\begin{itemize}
			\item Sampling method: Metropolis importance sampling
			\item Hamiltonian: With interaction (still just observing that without is very precise and our choices don't "ruin" that)
			\item The $\sigma_{RBM}=1$
			\item The $\omega_{HO}=1$
			\item Optimization method: Simplest gradient descent
		\end{itemize}
		\item What we will change (either grid search or with the random method thing Treider talked about. If too many we can set one or more before/after the others):
		\begin{itemize}
			\item The Metropolis importance sampling step size $\Delta t$ (default pr now 0.45)
			\item The learning rate $\eta$ (default pr now 0.01)
			\item The number of samples $nSamples$ (default pr now $10^4$ or $10^5$)
			\item Number of hidden units (starting defualt is $N$=2)
			\item Optimization stopping criteria
		\end{itemize}
		\item When all working, test a variation of gradient descent?
	\end{itemize}
\end{enumerate}

\section{Validation and limitations given the previously found optimal parameters}
\subsection{Compared to analytical solution}
-With/without interaction
-With different numbers of particles and dimensions
-Use the blocking values to take into account the error and/or standard deviation
-plot the density thing?
-show the learning evolution in video or similar?
\subsection{Compared to the traditional methods benchmarks}
\subsubsection{Precision}
\subsubsection{Performance/computational speed/convergence rate}
\subsubsection{"Work" required by user - setting hyperparameters. And to what extent do they, and potential wrong values for them, give instability in the method}

\section{Analysis}
\subsection{What can we extract from the weighting of the hidden features. Are we actually achieving dimensionality reduction or feature extraction here? Is the model telling us anything interesting?}
\subsection{Important question: do we succeed at sampling the whole config space and consequently train on the whole config space? Relevant to both VMC and RBM I guess.}
\subsection{How does it scale with 1) particles/dimensions 2) hidden nodes}
polynomial cost, polynomial time, etc

\section{More stuff}
\subsection{Training the RBM sigma}
\subsection{Optimize performance/parallelization}
\subsection{More in depth gradient descent variations investigation?}
\subsection{Try other types of hidden nodes (ReLU)}
\subsection{Add one more hidden layer - Deep BM}
\subsection{Collaboration with Alocias}
\subsection{Implementing symmetries of the Hamiltoninan}
\subsection{Try training on sampled data from analytical, for comparison? Then also test open source package?}
\subsection{Other Hamiltonians?}

\part{Conclusions and outlook}

\begin{appendices}
\chapter{Random generators and distributions in C++}
\end{appendices}

\newpage
\bibliography{master}

\end{document}