\documentclass[twoside,english]{uiofysmaster}
\usepackage[T1]{fontenc} %for å bruke æøå
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage[font=scriptsize]{caption}
\usepackage{subcaption}
\usepackage{mdwlist}
\usepackage[toc,page]{appendix}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{mathtools}

\newcommand*\dif{\mathop{}\!\mathrm{d}}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\bibliographystyle{ieeetr}
%\bibliography{references}

\author{Vilde Moe Flugsrud}
\title{Solving Quantum Mechanical Problems with Machine Learning}
\date{June 2018}

\begin{document}

\maketitle

\begin{abstract}
This is an abstract text.
\end{abstract}

\begin{acknowledgements}
  I acknowledge my acknowledgements.
\end{acknowledgements}

\tableofcontents

\chapter{Introduction}

Start your chapter by writing something smart. Then go get coffee.

\part{Theory}
\chapter{The Quantum Many-Body Problem}

\section{Many-Body Quantum Mechanics}
When describing systems at the size of atoms and electrons, at speeds where relativistic effects are regarded as negligible, quantum mechanics is the theory used to describe the system. 
While in classical mechanics the state of a system of $N$ particles is described by each particle's position and momentum, in quantum mechanics it is described by the complex valued wavefunction 
In quantum mechanics, the state of a system is defined by the complex-valued wavefunction $\Psi$, which is an element of an infinite dimensional Hilbert space. That is, a complete vector space with an inner product.\cite{Kvaal2017}. 

The expectation value of an operator $\hat{\bm{O}}$ for a system of $N$ particles is given as \cite{HjortJensen2015}
\begin{align}
	%\langle \hat{\bm{O}} \rangle
	=& \frac{\int \Psi^{\ast}(\bm{x}_1,...,\bm{x}_N) \hat{\bm{O}}(\bm{x}_1,...,\bm{x}_N)
	\Psi (\bm{x}_1,...,\bm{x}_N)
	\dif \bm{x}_1 ... \dif \bm{x}_N  }
	{\int \Psi^{\ast}(\bm{x}_1,...,\bm{x}_N)
	\Psi (\bm{x}_1,...,\bm{x}_N)
	\dif \bm{x}_1 ... \dif \bm{x}_N}
\end{align}

Finding $\Psi$ requires us to solve the time-independent Schrödinger equation
\begin{align}
	\hat{\bm{H}} \Psi =& E\Psi 
\end{align}
for a complex nuclear many body problem this can quickly become a problem consisting of millions of coupled second-order differential equations in $3N$ dimensions.

Computing the expectation value of the observable requires solving the multidimensional integral. This can be approximated using Monte Carlo methods. But what do we do with the fact that we don't know $Psi$? We solve this by employing the variational principle.

\section{Approximating the Wavefunction}
Some selected conditions $\Psi$ must satisfy \cite{HjortJensen2015} are normalization 
\begin{align}
	\int_{-\infty}^{\infty} P(x,t) \dif x = \int_{-\infty}^{\infty} \Psi^{\ast}(x,t) \Psi (x,t) \dif x = 1
\end{align}
and that $\Psi (x,t)$ and $\partial \Psi(x,t) / \partial x$ must be finite, continuous and single valued.


requirements from Ledum og Helgaker.

\section{Methods}
\begin{itemize}
	\item Summary from
	\begin{itemize}
			\item Ledum master \cite{Ledum2017}
	\end{itemize}
	\item The advent of computer simulations during the last several decades have in par- ticular made it possible to study moderately sized quantum mechanical systems from  rst principles. Our ability to solve—in closed form—the governing equations of quantum mechanics (QM) vanishes extremely quickly as the number of constituent particles exceed just a few.
	\item Any approximative scheme which aims to solve the many-body schrodinger equa- tion from scratch subject to some (more or less) well-de ned simpli cations is called an ab initio method. Working from  rst principles the aim of such algorithms is to extract information from a theoretical QM system in a reasonable amount of time. In order to accomplish this, a number of complicating intricacies need to be disre- garded. The magnitude of the simpli cations—essentially the number and impor- tance of complicating factors dropped—determine both the e cacy and the e ciency of the method: More simpli cations made allow solutions to be found for larger sys- tems (albeit less precise solutions), whereas extremely precise solutions can be found for small systems if very few simpli cations are employed.
	\item Despite tremendous increases in available numerical computational power in the latter half of the previous-, and the early parts of the current century, any such approximate scheme used is still heavily limited w.r.t. the system size. In practice, most methods are limited to systems of containing on the order of between $10^2$ (for high- precision methods such as con guration interaction, coupled cluster, di usion Monte Carlo, etc.) and $10^5$ electrons (for faster Hartree-Fock and density functional methods).
	\cite{Hu2015} \cite{VandeVondele2012} \cite{Bowler2010}
	\item As previously noted, solving the Schrodinger equation (SE) exactly by hand is impos- sible in the overwhelming majority of interesting cases. However, methods which can get close to the exact solution exists. Full Con guration Interaction (FCI) or direct di- agonalization of the Hamiltonian is exact in the limit of an in nite orbital basis set but su ers from an exponential complexity scaling (in system and basis size). 
	\begin{itemize}
		\item Molecular Electronic-Structure Theory - Helgaker. 2000 isbn: 0-471-96755-6.
	\end{itemize}
	\item The related Confguration Interaction (CI) and Coupled Cluster (CC) approaches both truncate the FCI expansion of Slater determinants, thus gaining speed but loosing some accuracy.
	\begin{itemize}
		\item S. Kvaal. Lecture notes for FYS-KJM4480. Lecture notes. Sept. 2017
		\item I. Shavitt and R.J. Bartlett. Many-Body Methods in Chemistry and Physics. MBPT and Coupled-Cluster Theory. Cambridge University Press, 2009. isbn: 0-521- 81832-X
	\end{itemize}
	\item Diffusion Monte Carlo (DMC) techniques can in principle provide the exact solution to the SE by imaginary-time evolution of an initial wave function guess
	\begin{itemize}
		\item M. Hjorth-Jensen. Computational Physics. Lecture notes. Aug. 2015. url: https: //github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Lectures/ lectures2015.pdf .
		\item B.L. Hammond, W.A. Lester Jr., and P.J. Reynolds. Monte Carlo Methods in Ab Initio Quantum Chemistry. Wspc, 1994. isbn: 981-02-0322-5.
	\end{itemize}
	\item In practice, DMC methods are highly dependent on this ansatz and thus require as input the results of less accurate method but faster methods. One example may be the Variational Monte Carlo (VMC) method: conceptually simpler and faster than DMC, but not as accurate
	\begin{itemize}
		\item B.L. Hammond, W.A. Lester Jr., and P.J. Reynolds. Monte Carlo Methods in Ab Initio Quantum Chemistry. Wspc, 1994. isbn: 981-02-0322-5.
		\item Harold Conroy. “Molecular Schrodinger Equation. II. Monte Carlo Evaluation of Integrals”. In: The Journal of Chemical Physics 41.5 (1964), pp. 1331–1335. doi: 10.1063/1.1726069. eprint: https://doi.org/10.1063/1.1726069. url: https://doi.org/10.1063/1.1726069.
		\item J.B. Anderson. Quantum Monte Carlo. Origins, Development, Applications. Ox- ford University Press, 2007. isbn: 0-19-531010-1.
	\end{itemize}
	\item The Hartree-Fock (HF) framework—which provides an e cient but not enormously accurate result—has seen extensive use since its inception in 1930
	\begin{itemize}
		\item D. R. Hartree. “The Wave Mechanics of an Atom with a Non-Coulomb Central Field. Part I. Theory and Methods”. In: Mathematical Proceedings of the Cam- bridge Philosophical Society 24.1 (1928), pp. 89–110. doi: 10.1017/S0305004100011919.
		\item V. Fock. %“Näherungsmethode zur Lösung des quantenmechanischen Mehrkör- perproblems”. In: Zeitschrift für Physik 61.1 (Jan. 1930), pp. 126–148. issn: 0044-3328. doi: 10.1007/BF01340294. url: https://doi.org/10.1007/BF01340294.
		\item A. Szabo and N.S. Ostlund. Modern Quantum Chemistry. Dover Publications, 1996. isbn: 0-486-69186-1.
	\end{itemize}
	\item However, by far the most popular approximation is Density Functional Theory (DFT), devel- oped by W. Kohn and L. J. Sham in 1965
	\begin{itemize}
		\item W. Kohn and L. J. Sham. “Self-Consistent Equations Including Exchange and Correlation E ects”. In: Phys. Rev. 140 (4A Nov. 1965), A1133–A1138. doi: 10. 1103/PhysRev.140.A1133. url: https://link.aps.org/doi/10.1103/PhysRev.140. A1133.
		\item R.M. Martin. Electronic Structure. Cambridge University Press, 2004. isbn: 0- 521-53440-2.
	\end{itemize}
	Between 1980 and 2010, DFT was the most active  eld in physics with eight out of the top ten most cited papers being on the subject 
	\begin{itemize}
		\item Axel D. Becke. “Perspective: Fifty years of density-functional theory in chem- ical physics”. In: The Journal of Chemical Physics 140.18 (2014), 18A301. doi: 10.1063/1.4869598. eprint: https://doi.org/10.1063/1.4869598. url: https: //doi.org/10.1063/1.4869598.
	\end{itemize}
	\item Computational scaling of \textit{ab initio} QM models range from $\mathcal{O}(N!)$ in the extreme (FCI) via $\mathcal{O}(N^6)$ (CC with singles, doubles, and estimated connected triples) and $\mathcal{O}(N^4)$ (formal HF), to $\mathcal{O}(N^3)$ for Hatree-Fock with integral pre-screening and density fitting.
	\begin{itemize}
		\item Laura E. Ratcli  et al. “Challenges in large scale quantum mechanical calcu- lations”. In: Wiley Interdisciplinary Reviews: Computational Molecular Science 7.1 (2017). e1290, e1290–n/a. issn: 1759-0884. doi: 10.1002/wcms.1290. url: http://dx.doi.org/10.1002/wcms.1290.
	\end{itemize}
\end{itemize}

\section{VMC}
\subsection{The Variational Principle}
The variatoional princple states that given any function (possibly fulfilling some conditions?) acting as the wavefunction, it will give a higher energy than the ground state energy. So we are sure to get an upper bound. This leads to a method where we construct a trial wavefunction with some number of variational parameters. Minimizing the variational parameters are then sure to give us the best upper bound of the ground state energy. In the process we have also obtained an approximated wave function. 

In order to do these we will employ minimization techniques discussed later.

Kvaale lecture notes, Helgaker. Two theorems

\subsection{Monte Carlo Integration}
The Monte Carlo method is an excellent approach to approximating high dimensional integrals. We do so by evaluating the function of interest at random values drawn according to some probability distribution function of interest. 

Expected value of random variable $x$ is
\begin{align}
	E(x) = \langle x \rangle = \int p(x) x \dif x = \mu
\end{align}
if we have a real valued function $g(x)$ and $x$ is a random variable, $g(x)$ is a random variable as well, with the mean value defined
\begin{align}
	E(g(x)) = \langle g(x) \langle = \int p(x) g(x) \dif x
\end{align}
The $n$th moment of $x$ is defined as the expectation of the $n$th power of $x$,
\begin{align}
	\langle x^n \rangle = \int p(x) x^n \dif x
\end{align}
where we notice that the first moment recovers the definition of the expectation value.
The central moments of $x$ are defined
\begin{align}
	\langle (x-\mu)^n \rangle = \int p(x) (x - \mu)^n \dif x
\end{align}
Where the second central moment is what we know as the variance of $X$
\begin{align}
	\sigma^2 = var(x) = \langle (X-\mu)^2 \rangle =& \int p(x) (x - \mu)^2 \dif x  \\
	=& \int p(x) (x^2 - 2x \langle x \rangle + \langle x \rangle^2) \dif x \\
	=& \langle x^2 \rangle - \langle x \rangle \langle x \rangle + \langle x \rangle^2 \dif x \\
	=& \langle x^2 \rangle - \langle x \rangle^2 \dif x 
\end{align}
The variance of the function $g(x)$ is similarly defined
\begin{align}
	var(g(x)) = \langle (g(x) - \langle g(x) \rangle)^2 \rangle 
	= \langle g(x)^2 \rangle - \langle g(x) \rangle^2
\end{align}
If two random variables $x$ and $y$ are independent, we have that
\begin{align}
	\langle xy \rangle = \langle x \rangle \langle y \rangle
\end{align}
If they are not independent, the degree of independence is measured by the covariance, which is defined
\begin{align}
	cov(x,y) = \langle xy \rangle - \langle x \rangle \langle y \rangle
\end{align}
However, zero covariance by itself does not guarantee independence.

From the covariance we may derive the correlation coefficient
\begin{align}
	\rho(x,y) =& \frac{cov(x,y)}{sqrt{ \langle x \rangle \langle y \rangle }}, \quad
	-1 \geq \rho(x,y) \leq 1
\end{align}

A bivariate probability distribution function $p(x,y)$ may be defined for two random variables. The expected value of a function $g(x,y)$ depending on them is then
\begin{align}
	E(g(x,y)) = \langle g(x,y) \rangle = \int \int p(x,y) g(x,y) \dif x \dif y
\end{align}
The marginal probability distribution function can be found for either of the two variables by 
\begin{align}
	p(x) = \int p(x,y) \dif y
\end{align}
The conditional probability is then given
\begin{align}
	p(y|x) = \frac{p(x,y)}{p(x)}
\end{align}
This means that if we know the marginal and conditional probabilities, the bivariate distribution can be sampled by sampling two univariate distributions. This can be generalized to multivariate distributions of more than two correlated random variables.

\subsubsection{Estimators}
Suppose random variables $x_1, x_2,...$ are drawn at random, but not necessarily independently, from the probability distribution function $p(x)$. Let $g_i$ be functions of $x_i$ and let $\lambda_i$ be a real number. Define the function $G$ by
\begin{align}
	G = \sum_{i=1}^N \lambda_i g_i (x_i)
\end{align}
The expected value of $G$ is
\begin{align}
	E(G) = E (\sum_i^N \lambda_i g_i(x_i)) = \sum_i^N \lambda_i E(g_i(x))
\end{align}
(having used the linearity of the expectation value). If all the $x_i$ are independent, the variance of $G$ becomes
\begin{align}
	var(G) =& \langle G^2 \rangle - \langle G \rangle^2 \\
	=& \sum_i^N \lambda_i^2 var(g_i(x))
\end{align}
If $\lambda_i=\frac{1}{N}$ and all the $g_i(x)$ are identical, $g(x)$. The expected value of $G$ is then
\begin{align}
	E(G) = E(\frac{1}{N} \sum_i^N g(x_i) ) = \frac{1}{N} \sum_i^N E(g(x)) = E(g(x))
\end{align} 
$G$ is the arithmetic average of $g(x)$ and has the same mean. $G$ is said to be an \textit{estimator} of $E(g(x))$. The variance of $G$ becomes
\begin{align}
	var(G) = var( \frac{1}{N} \sum_i^N g(x_i)  ) = \sum \frac{1}{N^2} var(g(x)) 
	= \frac{1}{N} var(g(x))
\end{align}

The implication of this is that as the number of samples of $x$, $N$, increases, the variance of the mean value of $G$ decreases as $\frac{1}{N}$. This is a core idea of Monte Carlo integration. That is, an integral may be estimated by a sum
\begin{align}
	E(g(x)) = \int p(x) g(x) \dif x = E(\frac{1}{N} \sum_{i=1}^N g(x_i))
\end{align}
In summary, to apply this:
\begin{itemize}
	\item Choose a series of random variables $x_i$ from $p(x)$
	\item Evaluate $g(x)$ at each $x_i$
	\item The mean of all the values of $g(x_i)$ is an estimate of the integral, and the variance of this estimate decreases as the number of samples increases.
\end{itemize}

\subsubsection{Convergence}
\textbf{The law of large numbers}: Suppose the random variables $x_1, x_2, ..., x_N$ are independent and all drawn from the same distribution, that is i.i.d. random variables. The expecation of each $x$ is then $\mu$. As $N\rightarrow \infty$, the mean value of the $\{ x \}$ 
\begin{align}
	\bar{x}_N = \frac{1}{N} \sum_{i=1}^N X_i
\end{align}
is almost guaranteed to converge to $\mu$:

To estimate the speed of convergence on the other hand, stronger assumptions are needed. We assume an estimator $G$, its mean $\langle G \rangle$ and variance $var(G)$ all exist. \textbf{The Chebychev inequality} then is
\begin{align}
	p(|G - \langle G \rangle | \geq \sqrt{\frac{var(G)}{\delta}}) &\leq \delta \\
	p(|G - \langle G \rangle | \geq \sqrt{\frac{var(g)}{\delta N}}) &\leq \delta \\
\end{align}
with $\delta$ any postive number. This estimates the chances of generating a large deviation in a Monte Carlo calculation. By making $N$ big, we can make the variance of $G$ as small as we want. The probability of getting a large deviation relative to $\delta$ between the estimate of the integral and the true value becomes small. This is at the core of the Monte Carlo method for evaluating integrals.

A much stronger statement than the Chebychev inequality about the range of values of $G$ that can be observed is given by the \textbf{central limit theorem} of probability. For any fixed value of $N$, there is a pdf that describes the values of $G$ that occur in the course of a Monte Carlo calculation (for a fixed $N$, if $G$ was calculated $M$ times, each times with a different sequence of i.d.d random variables, the set $\{ G_j\}$, $j=1,2,...,M$ has a specific distribution function). As $N\rightarrow \infty$, however, the central limit theorem shows that there is a specific limit distribution for the observed values of $G$, namely, the normal distribution. Set
\begin{align}
	G_N = \frac{1}{N} \sum_i^N g(x_i)
\end{align}
and
\begin{align}
	t_N = \frac{(G_N - \langle g(x) \rangle)}{\sqrt{var(G_N)}}
	= \frac{\sqrt{N} (G_N - \langle g(x) \rangle)}{\sqrt{var(g(x))}}
\end{align}
then
\begin{align}
	lim_{N\rightarrow \infty} p(a < t_N < b)
	= \int_a^b \frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \dif t
\end{align}
Let $\sigma^2 = var(g)$. We can then rewrite the above equation to specify a pdf for values of $G_N$
\begin{align}
	p(G_N) = \frac{1}{\sqrt{2\pi (\sigma^2/N)}} e^{\frac{N(G_N - \langle g\rangle)^2}{2\sigma^2}}
\end{align}
As $N\rightarrow \infty$ the observed $G_N$ occurs in ever narrower intervals near $\langle g \rangle$ and one can predict the probability of deviations measured in units of $\sigma$. That is, the observed $G_N$ is within the \textit{standard error} (i.e. $\sigma/\sqrt{N}$) of $\langle g \rangle$ 68.3\% of the time, within two standard errors of $\langle g \rangle$ 95.4\% of the time, and within three standard errors 99.7\% of the time.

The central limit theorem is very powerful in that it gives a specific distribution for the values of $G_N$, but it applies only asymptotically. How large $N$ must be before the central limit theorem applies depends on the problem. If for a problem we have that the third central moment $\mu_3$ of $g$ exists, then the central limit theorem will be substantially satisfied when
\begin{align}
	|\mu_3| << \sigma^3 \sqrt{N}
\end{align}
Then confidence limits derived from the normal distribution can be applied to the results of a Monte Carlo calculation. 

Without the central limit theorem, there is in general only the much weaker upper bound of the Chebychev inequality to suggest how much the observed $G_N$ deviates from the actual mean. Of course, in specific cases, studies can be made of the distribution of the estimator. Much Monte Carlo is done assuming that the theorem has been satisfied no matter what the sample size; reported errors must be considered optimistic in such cases.
When the variance is infinite, it is sometimes possible to find a limit distribution for $G$ that will lead to a central limit theorem for that particular problem. The limit distribution will in general not be normal.

The variance used in the discussion above may itself be estimated using independent values of $g(x_n)$ the following way:
\begin{align}
	\langle \frac{1}{N} \sum_{i=1} g^2 (x_i) - [\frac{1}{N}\sum_{i=1}^N g(x_i)]^2 \rangle
	= \langle g^2 \rangle - \frac{1}{N^2} \langle
	\sum_{i=1}^N g(x_i)^2 + \sum_{i,i\neq j}^N \sum_{j=1}^N g(x_i) g(x_j) \rangle
\end{align}
Using the independence of $g(x_i)$ and $g(x_j)$ in evaluating $\langle g(x_i) g(x_j)\rangle $, we find the right-hand side equal to 
\begin{align}
	(1- \frac{1}{N}) \langle g^2 \rangle - \frac{N(N-1)}{N^2} \langle g \rangle^2
	= \frac{N-1}{N} var(g)
\end{align}

thus an estimator for $\sigma^2$ is
\begin{align}
	\sigma^2 \approx \frac{N}{N-1} ( \frac{1}{N} \sum_{i=1}^N g^2(x_i) -
	( \frac{1}{N} \sum_{i=1}^N g^2(x_i))^2 )
\end{align}
An estimator of the variance of the estimated mean is given by
\begin{align}
	var(G_N) \approx \frac{1}{N-1} ( \frac{1}{N} \sum_{i=1}^N g^2(x_i) -
	( \frac{1}{N} \sum_{i=1}^N g^2(x_i))^2 )
\end{align}

\subsubsection{Summary}
If $x_1, x_2,...,x_n$ arr i.i.d. random variables with pdf $p(X)$, then for a function $g(x)$ an estimator is
\begin{align}
	G_N =& \frac{1}{N} \sum_{i=1}^N g(x_i) \\
	\langle G_N \rangle =& \int p(x) g(x) \dif x
\end{align}
and
\begin{align}
	var(G_N) =& \frac{1}{N} var(g)
\end{align}
as $N\rightarrow \infty$ and if the variance exists, the distribution of possible values of $G_N$ narrows about the mean as $\sqrt{N}$; or the probability of finding a $G_N$ some fixed distance away from $\langle G_N \rangle$ becomes smaller. 

The basic random variable used in Monte Carlo has been set by historical convention to be distributed uniformly between 0 and 1.

Note that we can do integrations in multiple dimensions with suprisingly little loss of computational efficiency. That one can work in many dimensions – indeed one can add extra dimensions – is a characteristic of Monte Carlo quadrature in contrast to discrete numerical quadrature, and is a property that can be exploited to great advantage in many applications.

To evaluate the $L$-dimensional integral over the unit hypercube 
\begin{align}
	\int ... \int g(x_1, x_2, ..., x_L) \dif x_1 \dif x_2 ... \dif x_L
\end{align}
$L$ uniform random variables could be samples, the function $g(x_1, x_2, ..., x_L)$ calculated, and the whole process repeated $N$ times. The arithmetic mean of the function values gives an unbiased estimate of the integral.

Of course, given the ability to generate random variables from any distribution over any space (as discussed in the next chapter) the domain of integration need not be limited to hypercubes.

Pseudorandom numbers might be wanted over truly random numbers for several reasons where one is dependent on being able to recreate program runs.

\subsubsection{Monte Carlo Estimators}
We have defined an estimator as a useful approximation to a quantity of interest $Q$ (for example an integral) which may be derived from a Monte Carlo calculation. The estimator is then the approximation (as given by earlier definitions), which is a function $\theta(\xi_1, \xi_2, ..., \xi_N)$ of the $N$ random or pseudorandom variables used in the calculation.

The function $\theta$ is of course itself random, and the statement that it gives a
satisfactory approximation to $Q$ means that it is not expected to fluctuate far from $Q$. Put a little more formally,
\begin{align}
	\frac{\langle (\theta - Q)^2 \rangle}{Q^2} \ll 1
\end{align}

Acceptable values of the ratio depend on the application. A Monte Carlo calculation may be intended to give a rough estimate of some numerical quantity, or it may be aimed at high precision, or at a target in between. The appropriate balance between small or zero bias and small variance will depend on these choices.
We write
\begin{align}
	\langle (\theta - Q)^2 \rangle = \langle (\theta -\langle \theta \rangle)^2 \rangle
	+ (\langle \theta \rangle - Q)^2
\end{align}
and observe that the quality of $\theta$ as a measure of $Q$ comes separately from the variance of $\theta$ and from the departure of its mean from $Q$. The quantity $\langle \theta \rangle - Q$ is called the \textit{bias of the estimator}. 

The quadratures we have discussed are unbiased since the result is linear in the functions calculated. For some problems, however, it is very difficult to formulate unbiased estimators. As we shall see, there are many problems for which the answer required is a ratio of integrals,

\begin{align}
	Q = \frac{\int_0^1 g_1 (x) \dif x}{\int_0^1 g_2 (x) \dif x} 
\end{align}
for which a suitable estimator is
\begin{align}
	\theta (\xi_1, ..., \xi_N) = \frac{\sum_i g_1 (\xi_i)}{\sum_i g_2 (\xi_i)}
\end{align}
Since this is not a linear function of $g_2$, it is biased. (...example...) This $\frac{1}{N}$ behavior is typical of the bias of such ratios. The results that may be derived from a Monte Carlo calculation are more general than this, and may have different variation of the bias. It is of course best if the bias becomes 0 as $N$ grows large.

An estimator $\theta$ is termed consistent for the quantity $Q$ if $\theta$ converges to $Q$ with probability 1 as $N$ approaches infinity. That is, $\theta$ is a consistent estimator of $Q$ if 
\begin{align}
	p(lim_{N\rightarrow \infty} \theta (\xi_1, \xi_2, ..., \xi_N)=Q) =1 
\end{align}
The law of large numbers states that the sample mean $\bar{x}_N$ is a consistent (and unbiased) estimator of the mean $\mu$. It further implies that estimators of quotients that are quotients of means are also consistent (although, in general, biased).

While unbiased estimators are desirable, they should not be introduced at the expense of a large variance, since the overall quality is a combination of both bias and consistency. In general, one seeks the minimum of $\langle (\theta -Q)^2 \rangle$. 

(example)

Just as a good Monte Carlo calculation must be supplemented with an estimate of the statistical error, sources of bias should be identified. The bias should be estimated numerically or an upper bound should be determined. A useful way of estimating bias when the behavior with $N$ is known is to group the data in samples smaller than $N$, say $n=N/m$. One can average this more biased estimator over the m groups obtained and study the dependence on m:
\begin{align}
	\text{Bias of } \frac{\sum_{i=1}^N g_1 (\xi_i)}{\sum_{i=1}^N g_2 (\xi_i)}
	&\approx \frac{c}{N} \\
	\text{Bias of} \frac{1}{m} \sum_{l=1}^m (\frac{\sum_{i=n(l-1)+1}^{nl} g_1 (\xi_i)}
	{\sum_{i=n(l-1)+1}^{nl} g_2 (\xi_i)})_{group_l}
	&= \frac{c}{n} = \frac{cm}{N}
\end{align}
where $c$ is a positive constant.

We note in passing that this method of grouping is also a practical way of estimating the variance of the quotient. This consists in selecting groups of numerators and denominators that are nearly independent, forming partial quotients for the groups, and then applying Equation 2.44. \cite{Kalos2008}

\subsection{Local Energy}
How do we relate the quantum mechanical energy to a Monte Carlo estimator? We have that \cite{Toulouse2016}
\begin{align}
	E_v =& \frac{\langle \Psi | \hat{H} | \Psi \rangle}{\langle \Psi | \Psi \rangle}  \\
	=& \frac{ \int \Psi^\ast (\bm{R}) \hat{H} \Psi(\bm{R}) \dif \bm{R} }
	{\int \Psi^\ast \Psi(\bm{R}) \dif \bm{R}} \\
	=& \frac{ \int \Psi^\ast (\bm{R}) (\Psi(\bm{R}) \frac{1}{\Psi(\bm{R})}) \hat{H} \Psi(\bm{R}) \dif \bm{R} }
	{\int | \Psi(\bm{R})|^2 \dif \bm{R}} \\
	=&  \frac{ \int \Psi^\ast (\bm{R}) \Psi(\bm{R}) \frac{\hat{H} \Psi(\bm{R}) }{\Psi(\bm{R})}  \dif \bm{R} }
	{\int | \Psi(\bm{R})|^2 \dif \bm{R}} \\
	=&  \frac{ \int | \Psi(\bm{R})|^2  E_L(\bm{R})  \dif \bm{R} }
	{\int | \Psi(\bm{R})|^2 \dif \bm{R}} \\
	=& \int P(\bm{R}) E_L (\bm{R}) \dif \bm{R}
\end{align}

where the local energy is defined $E_L = \frac{\hat{H} \Psi(\bm{R}) }{\Psi(\bm{R})} $ and $P(x) = \frac{| \Psi(\bm{R})|^2 }{ \int | \Psi(\bm{R})|^2 \dif \bm{R} }$ is the normalized probability given by the squared absolute wave function. 

We then recognize that we can use as our Monte Carlo \textit{estimator} the quantity
\begin{align}
	\frac{1}{N} \sum_{i=1}^N E_L(\bm{R}_i)
\end{align}
with $\bm{R_i}$ sampled from $P(\bm{R})$ to approximate, as $N \rightarrow \infty$, 
\begin{align}
	\langle \frac{1}{N} \sum_{i=1}^N E_L(\bm{R}_i) \rangle = \int P(\bm{R}) E_L(\bm{R}) \dif \bm{R} = E_v
\end{align}

Toulouse et al. \cite{Toulouse2016} gives the definition of local energy as: The expectation value of an operator $\hat{O}$ can be computed by averaging the corresponding local value
\begin{align}
	O(\bm{R}_f) = \frac{ \langle \bm{R}_f | \hat{O} |\Psi \rangle }{ \langle \bm{R}_f | \Psi \rangle }
\end{align}
at the Monte Carlo points $\bm{R}_f$ after the accept/reject step.

Umrigar \cite{Umrigar1999} defines $E_L$, or generally a \textbf{configurational eigenvalue} $X (\bm{R})$ as
\begin{align}
	X(\bm{R}) \Psi(\bm{R}) = \langle \bm{R} | \hat{X} | \Psi(\bm{R}) \rangle
	 = \sum_{\bm{R}'} \langle \bm{R} | \hat{X} | \bm{R}' \rangle \langle \bm{R}' | \Psi \rangle
\end{align}

More on how we need the local energy because we need operators that are diagnoal/near diagonal in the used basis in 
\cite{Umrigar1999} and Carleo seminar notes.

\subsection{Markov Chains}
Why? Because Monte Carlo requires us to sample from the probability distribution. We are not able to do this directly because the normalization constant is intractable. 

What is a Markov Chain?
A Markov Chain is a type of stochastic process. 

\subsubsection{Stochastic processes}
A stochastic process is a stochastic quantity $Y$ that can be mapped from a stochastic variable $X$ with a function $f$, and that also depends on another "normal" variable $t$, which usually represents time. That is,
\begin{align}
	Y_X (t) = f(X, t)
\end{align}

We may compute averages based on the given probability density $P_X(X)$ of $X$, for example
\begin{align}
	\langle Y(t) \rangle = \int Y_x (t) P_x(x) \dif x
\end{align}
More generally we may compute the $n$th moment, given time values $t_1,...,t_n$ as
\begin{align}
	\langle Y(t_1) ... Y(t_n) \rangle = \int Y_x(t_1) ... Y_x(t_n) P_x (x) \dif x 
\end{align}
Similarly we may also compute the \textbf{autocorrelation function}.

The probability density for $Y_X(t)$ to take any value $y$ at time $t$ is then
\begin{align}
	P_1 (y, t) = \int \delta (y- Y_X (t)) P_X (x) \dif x
\end{align}
Similarly the joint probability density that $Y$ has the value $y_1$ at $t_1$, and also $y_2$ at $t_2$ and so on till $y_n$ at $t_n$ is
\begin{align}
	P_n (y_1, y_1; y_2, t_2; ...; y_n, t_n)
	= \int \delta (y_1 - Y_x (t_1)) ... \delta (y_n - Y_x (t_n)) P_x (x) \dif x
\end{align}
In this way a hierarchy of probability densities $P_n (n=1,2,...)$ is defined.
Considering $P_n$ to be defined only when all times are different, the hierarchy of functions $P_n$ then obeys the following four \textbf{consistency conditions}.
\begin{enumerate}
	\item $P_n \geq 0$
	\item $P_n$ does not change on interchanging two pairs $(y_k, t_k)$ and $(y_l, t_l)$
	\item $\int P_n (y_1, t_1; ...; y_n, t_n) \dif y_n = P_{n-1} (y_1, t_1; ...; y_{n-1}, t_{n-1}) $
	\item $\int P_1 (y_1, t_1) \dif y_1 = 1$ 
\end{enumerate}
Any set of functions that obey the four consistency conditions determines a stochastic process.
\cite{VanKampen2007}


\subsubsection{Markov processes}
A Markov Process is a stochastic process that has the Markov property. The Markov property is that the next state is dependent only on the current state, and no more history than that. It is that for any set of $n$ successive times (i.e. $t_1<t_2<t_3$) one has
\begin{align}
	P_{1|n-1} (y_n, t_n | y_1, t_1; ...; y_{n-1}, t_{n-1}) = P_{1|1} (y_n, t_n | y_{n-1}, t_{n-1})
\end{align}
Meaning, the conditional probability density at $t_n$, given the value $y_{n-1}$ at $t_{n-1}$, is uniquely determined and not affected by any knowledge of the values at earlier times. $P_{1|1}$ is then known as the \textbf{transition probability}.

A Markov Process then is fully determined by the two functions:
\begin{enumerate}
	\item The initial probability distribution: $P_1 (y_1, t_1)$
	\item The transition probability: $P_{1|1} (y_2, t_2| y_1, t_1)$ 
\end{enumerate}
From this one can use Kolmogorov's definition of conditional probability to successively construct all $P_n$. For example, for 
$t_1 < t_2 < t_3$
\begin{align}
	P_3 (y_1, t_1; y_2, t_2; y_3, t_3) =& P_2 (y_1, t_1; y_2, t_2) P_{1|2} (y_3, t_3 | y_1, t_1; y_2, t_2) \\
	=& P_1 (y_1, t_1) P_{1|1} (y_2, t_2| y_1, t_1) P_{1|1} (y_2, t_2| y_1, t_1) P_{1|1} (y_3, t_3 | y_2, t_2)
\end{align}
\cite{VanKampen2007}

\subsubsection{The Chapman-Kolmogorov equation}
If we integrate the equation above over $y_2$, and then divide by $P_1(y_1, t_1)$, we obtain for $t_1 < t_2 < t_3$ 

\begin{align}
	P_2 (y_1, t_1; y_3, t_3) =& P_1(y_1, t_1) P_{1|1} (y_2, t_2 | y_1, t_1) P_{1|1} (y_3, t_3| y_2, t_2) \dif y_2 \\
	P_{1|1} (y_3, t_3| y_1, t_1) =& \int P_{1|1} (y_3, t_3|y_2, t_2) P_{1|1} (y_2, t_2 | y_1, t_1) \dif y_2
\end{align}
This is an identity which must be obeyed by any Markov Process. The two functions $P_1$ and $P_{1|1}$ which fully determine a Markov Process then cannot be chosen arbitrarily then, but have to obey
\begin{enumerate}
	\item The Chapman-Kolmogorov equation above
	\item $P_1(y_2, t_2) = \int P_{1|1} (y_2, t_2| y_1, t_1) P_1 (y_1, t_1) \dif y_1$ ("obviously")
\end{enumerate}

Any two non-negative functions $P_1$ and $P_{1|1}$ that obey these consistency conditions uniquely determines a Markov Process.
\cite{VanKampen2007}

\subsubsection{Stationary processes}
A \textit{stochastic process} in general is defined to be stationary when the moments are not affected by a shift in time, that is
\begin{align}
	\langle Y(t_1 + \tau)...Y(t_n + \tau) \rangle = \langle Y(t_1 )...Y(t_n ) \rangle
\end{align}
for all $n$, all $\tau$, and all $t_1, ..., t_n$. In particular $\langle Y \rangle$.

For a stationary \textit{Markov process} we have that the transition probability $P_{1|1}$ does not depend on two times but only on the time interval; we introduce the notation
\begin{align}
	P_{1|1} (y_2, t_2| y_1, t_1) = T_\tau (y_2|y_1), \quad \tau = t_2 - t_1
\end{align}

The Chapman-Kolmogorov equation then becomes $(\tau, \tau' > 0)$
\begin{align}
	T_{\tau+\tau'} (y_3 | y_2) = \int T_{\tau'} (y_3|y_2) T_\tau (y_2|y_1) \dif y_2
\end{align}
If we read the integral as a product of two matrices, or integral kernels, this may be written
\begin{align}
	T_{\tau + \tau'} = T_{\tau'} T_{\tau}, \quad (\tau, \tau' > 0)
\end{align}
\cite{VanKampen2007}

\subsubsection{Markov Chains}
Markov chains are an especially simple class of Markov processes, which we define by the properties
\begin{enumerate}
	\item The range of $Y$ is a discrete set of states
	\item The time variable is discrete and take only integer values $t=...,-2,-1,0,1,2,...$
	\item The process is stationary (or at least \textbf{homogeneous}), so the transition probability depends on the time difference alone.
\end{enumerate}
A \textbf{finite} Markov chain then has a range consisting of a finite number $N$ states. We then have
\begin{itemize}
	\item The initial probability distribution $P_1 (y, t)$ is an $N$-component vector $p_n(t)$ ($n=1,2,...,N$).
	\item The transition probability $T_\tau (y_2|y_1)$ is an $N\times N$ matrix.
\end{itemize}

The Markov property then leads to the equation
\begin{align}
	T_\tau = (T_1)^{\tau} (\tau = 0,1,2,...)
\end{align}
The probability distribution $p(t)$ originating from the initial distribution $p(0)$ is, in matrix notation,
\begin{align}
	p(t) = T^t p(0)
\end{align}
The study of finite Markov chains amounts to investigating the powers of an $N\times N$ matrix $T$ of which we only know that it is a \textbf{stochastic matrix}. That is,
\begin{enumerate}
	\item Its elements are non-negative.
	\item Each column adds up to unity.
\end{enumerate}
It is clear that $T$ has a left eigenvector $(1,1,...,1)$ with eigenvalue 1, and therefore a right eigenvector $p^s$ such that $T p^s = p^s$, which is the $P_1(y)$ of the stochastic process.
The prinicpal task of the theory then is to show that for any initial $p(0)$
\begin{align}
	\text{lim}_{t\rightarrow \infty} p(t) = \text{lim}_{t\rightarrow \infty} T^t p(0) = p^s
\end{align}
(and this has been shown by so and so except for a few exceptional cases?)
\cite{VanKampen2007}



\subsection{Metropolis-Hastings algorithm}
How do we use Markov Chains to sample from $P(\bm{R})$ then?

\subsubsection{Ergodicity}
The point of using Markov Chains is to be able to sample our desired distribution $P(\bm{R})$. We start from an arbitrary initial state $\bm{R}_i$ and evolve the system by repeated application of a Markov transition matrix $T$. The stationary property, which implies that
\begin{align}
	\sum_i T(\bm{R}_f | \bm{R}_i) P (\bm{R}_i) = \sum_i T (\bm{R}_i | \bm{R}_f) P (\bm{R}_f) = P(\bm{R}_f)
\end{align}
(where we used that $P$ is an eigenvector of $T$ with eigenvalue 1), and ensures that if we start with the desired distribution $P$, we will continue to sample from that same distribution.

However, we want that \textit{any} initial distribution should evolve to the desired distribution $P$ under the repeated application of $T$, i.e., $lim_{n\rightarrow \infty} M^n (\bm{R}' | \bm{R}) = \rho(\bm{R}')$, independent of $\bm{R}$. 

Aka, not only should $\rho$ be a right eigenvector of $M$ but it should be the \textit{dominant} right eigenvector. Necessary conditions are that transitions can be made with non-zero prob between (almost, ref) any pair of states in a finite number of steps and that $M$ not be cyclic. Such a matrix is \textbf{ergodic}. 
\cite{Umrigar1999}

\subsubsection{Detailed balance condition}
We have shown so far that we can sample $\rho$ by repeated application of a Markov matrix that satisfies the stationary condition, but we have not shown how such matrices are constructed in practice. In order to do this, the more stringent \textbf{detailed balance} condition
\begin{align}
	M(\bm{R}_f | \bm{R}_i ) \rho(\bm{R}_i) = M(\bm{R}_i | \bm{R}_f ) \rho(\bm{R}_f)
\end{align}
is usually imposed. 
It expresses the condition that for any pair of states $\bm{R}_i$ and $\bm{R}_f$ the rate of flow is the same in both directions. It is a sufficient, but not necessary, condition. Detailed balance implies the stationary condition, but the reverse is not true.
\cite{Umrigar1999}
A Markov chain satisfying the detailed balance condition is said to be \textbf{reversible}.\cite{Toulouse2016} 

\subsubsection{obs: When is a Markov chain a random walk?}
In practice, a Markov chain is realized by a \textbf{random walk}. More in \cite{Toulouse2016} p. 293.

Starting from an initial point $\bm{R}_1$ (or walker) - i.e., a delta-function distribution $\delta (\bm{R} - \bm{R}_1)$ - sample the second point $\bm{R}_2$ by drawing from the probability distribution $P(\bm{R}_2|\bm{R}_1)$, then a third point $\bm{R}_3$ by drawing from $P(\bm{R}_3 | \bm{R}_2)$, and so on. After disregarding a certain number of iterations $M_{eq}$ corresponding to a transient phase called \textit{equilibration}, the random walk samples the stationary distribution $\rho(\bm{R})$ in the sense that
\begin{align}
	\rho(\bm{R}) = E[\delta (\bm{R} - \bm{R}_k) ] \approx \frac{1}{M} \sum_{k=1}^M \delta (\bm{R} - \bm{R}_k)
\end{align}
and the averages of the estimators of the observables of interest are calculated.

The rate of convergence to the stationary distribution $\rho(\bm{R})$ and the autocorrelation times of the observables are determined by the second largest eigenvalue of the matrix $P$ (see, e.g., Ref. \cite{Gilks1996}). The random walk must be sufficiently long so as to obtain a representative sample of states, making a nonnegligible contribution to the expected values. 
\cite{Toulouse2016}

\subsubsection{The Metropolis-Hastings Algorithm}
In the Metropolis-Hastings algorithm , one realizes a Markov chain with the following random walk. Starting from a point $\bm{R}_i$, a new point $\bm{R}_f$ is determined in two steps:
\begin{enumerate}
	\item A temporary point $\bm{R}_f'$ is proposed with the probability $P_{prop} (\bm{R}_f' | \bm{R}_i)$,
	\item The point $\bm{R}_f'$ is accepted (i.e., $\bm{R}_f = \bm{R}_f'$) with probability $P_{acc} (\bm{R}_f' | \bm{R}_i)$ or rejected (i.e., $\bm{R}_f = \bm{R}_i$) with probability $P_{rej}(\bm{R}_f' | \bm{R}_i) = 1 - P_{acc} (\bm{R}_f' | \bm{R}_i)$.
\end{enumerate}
The corresponding transition probability can be written as
\[ P(\bm{R}_f |\bm{R}_i ) = \begin{cases}
				P_{acc}(\bm{R}_f |\bm{R}_i ) P_{prop} (\bm{R}_f |\bm{R}_i )  
				& \quad \text{if} \bm{R}_f \neq \bm{R}_i \\
				1 - \int  P_{acc}(\bm{R}_f' |\bm{R}_i ) P_{propr} (\bm{R}_f' |\bm{R}_i )  \dif \bm{R}_f'
				& \quad \text{if} \bm{R}_f = \bm{R}_i  
\end{cases} \]
or, in a single expression,
\begin{align}
	P(\bm{R}_f |\bm{R}_i ) =& P_{acc}(\bm{R}_f |\bm{R}_i ) P_{prop} (\bm{R}_f |\bm{R}_i ) \\
	&+ [1-\int  P_{acc}(\bm{R}_f' |\bm{R}_i ) P_{propr} (\bm{R}_f' |\bm{R}_i )  \dif \bm{R}_f'] \delta(\bm{R}_i - \bm{R}_f)
\end{align}

The proposal probability $P_{prob}(\bm{R}_f |\bm{R}_i )$ is a stochastic matrix, i.e.
\begin{align}
	P_{prob}(\bm{R}_f |\bm{R}_i ) \geq 0 \quad \text{and} \quad
	\int P_{prob}(\bm{R}_f |\bm{R}_i ) \dif \bm{R}_f = 1,
\end{align}
ensuring that $P(\bm{R}_f| \bm{R}_i)$ fullfills the non-negativity condition. The second term with the delta function ensures that $P(\bm{R}_f| \bm{R}_i)$ fullfills the normalization condition. (Nonnegativity and column normalization then ensures $P(\bm{R}_f| \bm{R}_i)$ is a \textbf{stochastic matrix}).

The acceptance probability is chosen so as to fulfill the \textbf{detailed balance} condition,
\begin{align}
	P(\bm{R}_f| \bm{R}_i)  \rho (\bm{R}_i) = P(\bm{R}_i| \bm{R}_f) \rho (\bm{R}_f)
\end{align}

that is, for $\bm{R}_f \neq \bm{R}_i$,
\begin{align}
	\frac{ P_{acc} (\bm{R}_f| \bm{R}_i) }{ P_{acc} (\bm{R}_i| \bm{R}_f) }
	= \frac{ P_{prop} (\bm{R}_i| \bm{R}_f) \rho (\bm{R}_f) }
	{ P_{prop} (\bm{R}_f| \bm{R}_i) \rho (\bm{R}_i) }
\end{align}
Several choices are possible. The choice of Metropolis et al. \cite{Metropolis1953} maximizes the acceptance probability
\begin{align}
	P_{acc} (\bm{R}_f | \bm{R}_i) = \text{min} \{ 1, 
	\frac{ P_{prop} (\bm{R}_i| \bm{R}_f) \rho (\bm{R}_f) }
	{ P_{prop} (\bm{R}_f| \bm{R}_i) \rho (\bm{R}_i) } \}
\end{align}
The acceptance probability is not a stochastic matrix, even though both the proposal and the total Markov matrices are stochastic. Since only the ratio $\rho (\bm{R}_f)/\rho (\bm{R}_i)$ is involved in the above form of the accpetance probability, it is not necessary to calculate the normalization constaint of the probability density $\rho (\bm{R})$. It is clear that this acceptance probability is optimal, but there is a considerable scope for ingenuity in choosing a proposal probability $P_{prop} (\bm{R}_f| \bm{R}_i)$ that leads to a small autocorrelation time.

\cite{Toulouse2016}





\subsection{Markov Chain Monte Carlo}
Markov Chain Monte Carlo methods are constructed to have the desired probability distribution as their equilibrium distribution, so that having reached the Markov Chain equilibrium distribution, we may obtain samples from the desired distribution. This is done using random walkers. While samples in conventional Monte Carlo integration are independent, the samples in this method are correlated.
When solving an integral the desired equilibrium of the Markov chain is the integrand.

What is a random walk?
Assume a set of random variables $x_1, x_2,...$ represent the state of a system at some "time" $n$ for $n=1,...,N$. The probability of going from state $i$ at time $n$ to state $j$ at time $n+1$ is
\begin{align}
	p_{ij} = p(x_{n+1} = j| x_n = i)
\end{align}
and is independent of past states, prior to state $i$. The set $\{ x_n \}$, $n\geq 0$ is called a \textit{Markov chain process} with stationary (a stationary transition probability is independent of when the transition from state $i$ to state $j$ occurs in the Markov chain) transition probabilities $p_{ij}$, $i,j=1,...,N$. A Markov chain is another term for a random walk. 

Random walks are frequently used as discrete approximations to continuous physical processes. Consider the motion of diffusing particles, which can be described by a differential equation. The motion of the particles is continuous but the positions, subject to collisions and perhaps random forces, fluctuate randomly. If a particle’s future position depends only on the current position, then the set $\{ x_n \}$ where $x_n$ is the position at a time "$n$" represents a Markov process.

Every time the system represented by a Markov Chain leaves a state $i$, it must be in another of the possible states; thus
\begin{align}
	\sum_{j=1}^N p_{ij} = 1, \quad i=1,...,N
\end{align}
If a state $j$ can be reached from state $i$ in a finite number of steps or transitions, then state $j$ is called \textit{accessible} from state $i$. If state $i$ is accessible from state $j$, then the states are said to \textit{communicate} with each other. If the two states do not communicate, then either $p_{ij}=0$ or $p_{ji}=0$. A Markov chain is termed \textit{irreducible} if all states communicate with each other. 

The $p_{ij}$ form a matrix $\bm{P}$ called the \textit{one-step transition probability matrix}. Given $\bm{P}$ and the probability distribution of the state of the process at time 0, the Markov chain is completely specified. Let $p_{ij}^n$ be the probability that the system goes from state $i$ to state $j$ in $n$ transitions. Then it can be expressed as 
\begin{align}
	p_{ij}^n = \sum_{k=0}^N p_{ik}^r p_{kj}^s
\end{align}
where $r+s = n$ and we define
\[ p_{ij}^0 = \begin{cases}
				 1, & i = j \\
				 0, & i \neq j  
\end{cases} \]
The period of state $i$, $d(i)$, is the greatest common divisor of all integers $n\geq 1$ for which $p_{ii}^n > 0$. If $p_{ii}^n = 0$ for all $n$, then $d(i)$ is defined to be 0. If the states $i$ and $j$ communicate, then $d(i) = d(j)$. If all the states in a Markov chain have a period of 1, the system is called \textit{aperiodic}. Let $f_{ii}^n$ be the probability that starting from state $i$, the first return to state $i$ occurs at the $n$th transition,
\begin{align}
	f_{ii}^n = p(x_n =i, x_m\neq i, m=1,2,...., n-1| x_0 =i)
\end{align}
State $i$ is said to be \textit{recurrent} iff
\begin{align}
	\sum_{n=1}^N f_{ii}^n = 1
\end{align}
That is, starting from state $i$, the probability of returning to state $i$ in finite time is 1. If this is not true, the state is said to be a \textit{transient} state. If states $i$ and $j$ communicate and $i$ is recurrent, then $j$ is recurrent. Thus, all states in an equivalence class (i.e. states that communicate with each other) are either recurrent or transient. 

Suppose we have a Markov chain with a recurrent aperiodic class of states. We can define the stationary probability for states $j=1,2,...,N$ by
\begin{align}
	\pi_j &= sum_{i=0}^N \pi_i p_{ij}, \quad pi_i \geq 0 \\
	\sum_{i=0}^N \pi_i &= 1
\end{align}
$\pi_j$ is the proportion of the time that the system spends in state $j$. The quantity $\pi_i p_{ij}$ is the proportion of the time that the random walk has just entered state $j$ from state $i$. If the initial state of the random walk is chosen to be distributed as $\pi_j$, then $p(x_n=j)$ is equal to $\pi_j$, for all $n$ and $j$. 

If $\pi_i > 0$ for one state $i$ in an aperiodic recurrent class, then $\pi_j > 0$ for all states $j$ in the same class. Such a class is called \textit{strongly ergodic}. When
\begin{align}
	\pi_i p_{ij} = \pi_j p_{ji}
\end{align}
for all $i\neq j$, then the Markov chain is said to be \textit{time reversible}. If the initial state $x_0$ is chosen from the set $\{ \pi_j \}$, the sequence of steps going backwards in time from any point $n$ will also be a Markov chain with transition probabilities given by $p_{ij}$. 

\subsubsection{Estimators and Markov Chains}
Markov Chains are used in simulations to model the behavior of systems whose probability distribution functions are not easily sampled. Suppose a discrete ranom variable has a probability mass function given by $p_j = p(x=j), j=1,2,..., N$. It can be sampled by creating an irreducible aperiodic Markov chain such that $p_j = \pi_j$ for $j=1,2,...,N$ and obtaining the values of $x_n$ when $n$ is large. An estimator for a property, $h(x)$, of the system, $E(h(x)) = \sum_{j=1}^N h(j) p_j$ can be formed from the states reached in the random walk,
\begin{align}
	E(h(x)) \approx \frac{1}{m} \sum_{i=1}^m h(x_i)
\end{align}
However, at step $n$ in a Markov chain, the state of the next step, $n+1$, is dependent on the properties of the previous step. Thus, it takes some number of steps, not always easily quantified, to lose the influence of the distribution of the initial state $x_0$. As a result, in forming estimators of quantities of interest, the first $k$ steps of the Markov chain are usually discarded. The estimator of the property $h(x)$ then becomes
\begin{align}
	E(h(x)) \approx \frac{1}{m-k} \sum_{i=k+1}^m h(x_i)
\end{align}
The choice of $k$ is guided by the properties of the application. 

The strong serial correlation that exists between the steps of a Markov chain also effect the estimation of the standard error of the estimator in the above equation. To correct for the correlation, one approach is to divide the Markov chain into blocks large enough so that the correlation is minimized between the blocks. This approach is called \textit{batch means} or \textit{blocking}. The $m-k$ steps in the above equation are broken up into $s$ blocks or batches of lenght $r$ steps, where $s = \frac{m-k}{r}$ and both $s$ and $r$ are integers. The average value of $h(x)$ is evaluated for each of the $s$ blocks.
\begin{align}
	H_{\mathit{\ell}} = \frac{1}{r} \sum_{i=k+(\ell - 1)r + 1}^{k + \ell r} h(x_i), \quad \ell = 1,2,...,s
\end{align}
Since each of the $H_\ell$ is considered to be an i.i.d. random variable, the mean value is given by 
\begin{align}
	\bar{H} = \frac{1}{s} \sum_{\ell = 1}^s H_\ell
\end{align}
and $\bar{H}$ is used to obtain the sample estimate of the variance,
\begin{align}
	var(H_\ell) = \frac{1}{s-1} \sum_{\ell = 1}^s (H_\ell - \bar{H})^2
\end{align}
The estimate of the standard error associated with $E(h(x))$ can then be obtained as $\sqrt{var(H_\ell)/s}$. 
 \cite{Kalos2008}
\subsubsection{Application Example: Simulated Annealing}
 \cite{Kalos2008}
\subsubsection{Variance reduction - importance sampling of integral equations}
 \cite{Kalos2008}


\subsubsection{Different source on Markov Chains}
Variational Monte Carlo is usually used to calculate expectation values of operators $\hat{X}$ that are diagonal or near-diagonal in the $\bm{R}$ representation. (The case of an arbitrary operator $\hat{X}$ is treated in the chapter by Nightingale.) The expectation value can be written in the form
\begin{align}
	\langle \hat{X} \rangle_\rho =& \frac{\sum_{\bm{R}} X(\bm{R}) \rho(\bm{R})}{\sum_{\bm{R}} \rho (\bm{R})} \\
	\approx & \frac{1}{T} \sum_{i=1}^T X(\bm{R}_i)
\end{align}
where $\rho(\bm{R}) = |\Psi (\bm{R})|^2$ and $\Psi(\bm{R})$ is a trial state that approximates the egienstate of interest.
The \textit{configurational eigenvalue} $X(\bm{R})$ is defined by
\begin{align}
	X(\bm{R}) \Psi (\bm{R}) = \langle \bm{R} | \hat{X} | \Psi (\bm{R}) \rangle = \sum_{\bm{R}'} \langle \bm{R} | \hat{X} | \bm{R}' \rangle \langle \bm{R}' | \Psi \rangle
\end{align}
and $X(\bm{R}) = \langle \bm{R}|\Psi \rangle = \langle \Psi | \bm{R} \rangle$. 
To be of practical use it is necessary that the sum on the right hand side can be performed quickly. If the states $\bm{R}$ form a continuum, then this is the case if $\hat{X}$ is diagonal or near-diagonal (involves only low-order derivatives in the $\bm{R}$ representation).
The operator of greatest interest is the Hamiltonian $\mathcal{H}$, in which case the configurational eigenvalue $X(\bm{R})$ is the \textit{local energy}.

The second line in the first equation expresses the fact that a Monte Carlo simulation of expectation values is obtained as a time average over a sequence of $T$ configurations that are sampled with relative probability $\rho(\bm{R})$. The time average is an approximation to the quantum mechanical expectation value, which becomes an equality in the $T\rightarrow \infty$ limit. To evaluate the right hand side, we need to find a way to sample  $\rho(\bm{r})$. The \textit{generalized Metropolis} method is almost universally used to sample complicated prob dists. Although it is a very simple method, we will see that a little bit of thought invested in the implemenetation of the method can have a large effect on its efficiency by reducing sequential correlations of the sampled configs. 

As the trial state $\Psi(\bm{R})$ approaches an exact eigenstate of $\hat{X}$, the configurational eigenvalue becomes a constant, equal to the eigenvalue of $\hat{X}$ for that state, i.e., both systematic and the statistical errors of the Monte Carlo estimate of the first equation vanish. Although this ideal situation is never achieved in practice for problems of interest, it does indicate that significant gains can be achieved by optimizing $\Psi(\bm{R})$ . We present later the \textit{variance minimization method} for optimizing trial states.
\subsubsection{(cont. ) Generalized Metropolis method}
For distributions that can't be sampled directly, at the cost of the generated samples being serially correlated.

The Metropolis method is an application of a \textbf{Markov chain}. A Markov chain is specified by two ingredients: an initial prob dist and a transition matrix whose elements $M(\bm{R}_f | \bm{R}_i )$ represent the prob of making a transition from an initial state $\bm{R}_i$ to a final state $\bm{R}_f$. Clearly, since the prob must be non-negative and since the system must evolve to some state (including the initial state), we require
\begin{align}
	M(\bm{R}_f | \bm{R}_i) \geq 0 \quad \text{and} \quad
	\sum_{\bm{R}_f} M(\bm{R}_f | \bm{R}_i) = 1
\end{align}
Such a matrix = a \textbf{stochastic matrix}.

To sample the desired distribution $\rho (\bm{R})$ we start from an arbitrary initial state $\bm{R}_i$ and evolve the system by repeated application of a Markov matrix $M$ that satisifies the \textbf{stationary condition}:
\begin{align}
	\sum_i M(\bm{R}_f | \bm{R}_i) \rho (\bm{R}_i) = \sum_i M (\bm{R}_i | \bm{R}_f) \rho(\bm{R}_f) = \rho(\bm{R}_f)
\end{align}
for all states $\bm{R}_f$ where the second equality follows from previous eq. 

Hence, $\rho$ is a right eigenvector of $M$ with eigenvalue 1. The stationary property implies if we start with the desired dist $\rho$ we will continue to sample the same dist. 

However, we want that any intial dist should evolve to the desired dist $\rho$ under the repeated application of $M$, i.e., $lim_{n\rightarrow \infty} M^n (\bm{R}' | \bm{R}) = \rho(\bm{R}')$, independent of $\bm{R}$. 

Aka, not only should $\rho$ be a right eigenvector of $M$ but it should be the \textit{dominant} right eigenvector. Necessary conditions are that transitions can be made with non-zero prob between (almost, ref) any pair of states in a finite number of steps and that $M$ not be cyclic. Such a matrix is \textbf{ergodic}. 

\subsubsection{(cont.) Detailed balance condition}
We have shown so far that we can sample $\rho$ by repeated application of a Markov matrix that satisfies the stationary condition, but we have not shown how such matrices are constructed in practice. In order to do this, the more stringent \textbf{detailed balance} condition
\begin{align}
	M(\bm{R}_f | \bm{R}_i ) \rho(\bm{R}_i) = M(\bm{R}_i | \bm{R}_f ) \rho(\bm{R}_f)
\end{align}
is usually imposed. 
It expresses the condition that for any pair of states $\bm{R}_i$ and $\bm{R}_f$ the rate of flow is the same in both directions. It is a sufficient, but not necessary, condition. Detailed balance implies the stationary condition, but the reverse is not true.

\cite{Umrigar1999}

\subsection{Metropolis-Hastings Sampling}
There are several approaches to how one should best generate new samples. One is Metropolis-Hastings method. It is based on using both a proposal probability and an acceptance probability. 
Introduced by \cite{Metropolis1953}


\begin{itemize}
	\item Simple and powerful. Can be used to sample essentially any distribution function regardless of analytic complexity in any number of dimensions. Complementary disadvantages: sampling is correct only asymptotically, and successive variables produced are correlated, often very strongly. $\rightarrow$ evaluation of integrals normally produces positive correlations in the values of the integrand, with consequent increase in variance for a fixed number of steps as compared with independent samples. Also the method is not well suited to sampling distributions with parameters that change frequently.
	\item Motivation: an analogy with the behavior of systems in stat mech that approach an equilibrium whose statistical properties are independent of the kinetics of the system.
	\item Here
	\begin{itemize}
		\item \textit{system} = a point $x$ in a space $\Omega$ (typically in $\bm{R}^M$) that may be thought of as a possible description of a physical problem.
		\item \textit{kinetics} = a stochastic transition that governs the evolution of the system: a pdf $K(X|Y)$ that ensures that the evolution of a system known to be at $Y$ will bring it near $X$ next.
		\item $K$ = a model of the physical process by which a system changes. Or a mathematical abstraction. In a Monte Carlo calculation, it plays the role of a sampling distribution.
	\end{itemize}
	\item One condition for a system to evolve toward equilibrium and stay there is, simply, that the system be on the average as likely to move into a specific neighborhood of $X$ from a neighborhood of $Y$ as to move exactly in the reverse direction. If the prob dist for observing the system near $X$ in equilibrium is $f(X)$, then the kinetics must satisfy
	\begin{align}
		K(X|Y) f(Y) = K(Y|X)f(X)
	\end{align}
	This relation is called \textbf{detailed balance}. $K(X|Y)f(Y)$=the prob of moving from $Y$ to $X$ expressed as the a priori chance of finding the system near $Y$ (i.e. $f(Y)$) times the conditional probability $K(X|Y)$ that it will move from $X$ to $Y$.
	\item For a physical system, one usually assumes $K(X|Y)$ known, and one has the task of finding $f(X)$. The Metropolis algorithm  (as in much of Monte Carlo) reverses this: one has the task of finding a convenient and correct kinetics that will equilibrate the system so that the given $f(X)$ turns out to be the chance of observing the system near $X$.
	\item This turns out to be extremely easy given the elegant device suggested by the Metropolis algorithm. Transitions are \textit{proposed} from, say, $Y$, to $X'$ using essentially \textit{any} distribution $T(X'|Y)$. Then on comparing $f(X')$ with $f(Y)$ and taking into accoutn $T$ as well, the system is either moved to $X'$  (move 'accepted') or returned to $Y$ (move 'rejected'). Acceptance of the move occurs with probability $A(X'|Y)$, which must be calculated so as to satisfy detailed balance. 
	\item We then have
	\begin{align}
		K(X|Y) = A(X|Y) T(X|Y)
	\end{align}
	Detailed balance requires
	\begin{align}
		A(X|Y) T(X|Y) f(Y) = A(Y|X) T(Y|X) f(X)
	\end{align}
	We expect that the ratio 
	\begin{align}
		\frac{T(Y|X) f(X)}{T(X|Y) f(Y) }
	\end{align}
	will play a significant role in determining $A$.
	\item Given a pdf $f(X)$, where $X$=a many-dimensional vector, the Metropolis technique establishes a random walk whose steps are designed so that when repeated again and again, the asymptotic distribution of $X'$s is $f(X)$. Suppose that $X_1, X_2, ..., X_n$ are the steps in a random walk. Each of the $X'$s is a random variable and has an associated probability $\phi_1(X), \phi_2(X), ..., \phi_n(X)$, where $\phi_1(X)$ can be any distribution for $X$. The $\phi_n(X)$ have the property that asymptotically
	\begin{align}
		lim_{n\rightarrow \infty} \phi_n(X) = f(X)
	\end{align}
	At each step, in the random walk, there is a transition distribution $T(X|Y)$, that is, the pdf for a trial move to $X$ from $Y$. The $T(X|Y)$ is normalized such that
	\begin{align}
		\int T(X|Y) \dif X = 1
	\end{align}
	for all values of $Y$. A quantity $q(X|Y)$ is defined as 
	\begin{align}
		q(X|Y) = \frac{T(Y|X)f(X)}{T(X|Y)f(Y)} \geq 0
	\end{align}
	where we explicitly assume that it's possible to move from $X$ to $Y$ if one can move from $Y$ to $X$ and vice versa. From $q(X|Y)$, the probability of accepting a move can be calculated; one frequently used probability is
	\begin{align}
		A(X|Y) = min(1, q(X|Y))
	\end{align}
	\item The algorithm can now be described concretely.
	\begin{itemize}
		\item At step $n$ of the random walk, the value of $X$ is $X_n$; a possible next value for $X$, $X_{n+1}'$, is sampled from $T(X_{n+1}'|X_n)$, and the prob of accepting $X_{n+1}'$ is computed. If
		\begin{itemize}
			\item If $q(X_{n+1}'|X_n) >1$ then $A(X_{n+1}'|X_n) =1$
			\item If $q(X_{n+1}'|X_n) <1$ then $A(X_{n+1}'|X_n) = q(X_{n+1}'|X_n)$
		\end{itemize}
		where
		\begin{align}
			q(X_{n+1}'|X_n) = \frac{T(X_n|X_{n+1}') f(X_{n+1}')}{T(X_{n+1}'|X_n) f(X_n)}
		\end{align}
		Then
		\begin{itemize} 
			\item With probability $A(X_{n+1}'|X_n)$ (that is, if $A(X_{n+1}'|X_n) > \xi$), we set $X_{n+1} = X_{n+1}'$
			\item Otherwise, $X_{n+1} = X_n$
		\end{itemize}
		For $q(X_{n+1}'|X_n) > 1$, $X_{n+1}$ will always equal $X_{n+1}'$.
	\end{itemize}
	\item As the random walk proceeds, a recursive relationship develops between succeeding $\phi_n (X)$'s. Let $\phi_n (X)$ be the distribution of values of $X_n$; what is the distribution $\phi_{n+1}$ for the values of $X_{n+1}$? There are two contributions to the distribution of the $X_{n+1}$:
	\begin{itemize}
		\item The prob of entering into the vicinity $\dif X$ of $X$ when we successfully move from $X_n$ and
		\item The prob that once we are at $X$, we will stay at $X$. 
	\end{itemize}
	If we start out at some value $Y$ contained in $\dif Y$, the prob of moving from the neighborhood $Y$ to the neighborhood of $X$ is $T(X|Y)\phi_n(Y) \dif Y$. The prob of successfully moving from $Y$ to $X$ is $A(X|Y)T(X|Y)\phi_n(Y) \dif Y$, so the net prob of successfully moving from any point $Y$ to the neighborhood of $X$ becomes
	\begin{align}
		\int A(X|Y) T(X|Y) \phi_n (Y) \dif Y
	\end{align}
	Similarly, the net prob that a move away from $X$ is not accepted is
	\begin{align}
		\int (1-A(Y|X)) T(Y|X) \dif Y
	\end{align}
	where $T(Y|X)$= the prob of moving from $X$ to $Y$ and $(1-A(Y|X))$ =the prob that the move was not accpeted. Upon multiplying the above equation by $\phi_n (X)$, the prob that we were at $X$, the relationship for $\phi_{n+1} (X)$ becomes
	\begin{align}
		\phi_{n+1} (X) = \int A(X|Y) T(X|Y) \phi_n (Y) \dif Y
		+ \phi_n(X) \int (1-A(Y|X)) T(Y|X) \dif Y
	\end{align}
	The random walk generates a recursion relationship for the distribution functions.
	\item Earlier, we asserted the asymptotic distribution sampled in the random walk would be $f(X)$. According to a theorem in Feller, if a random walk defines a system that is ergodic, then an asymptotic pdf exists and is unique if
	\begin{align}
		\phi_n (X) = f(X) \Rightarrow \phi_{n+1} (X) = f(X)
	\end{align}
	that is, if $f(X)$ is a stationary point of the recursion. Systems defined by random walks can be partitioned into several categories.
	\begin{itemize}
		\item If, in a random walk, the prob of returning to a neighborhood about $X$ is 0, then the system is called a \textit{null system} and the expected recurrence time is infinite. An example would be a one-dimensional system where $X_{n+1}$ 
	\end{itemize}
\end{itemize}
 \cite{Kalos2008}

\subsubsection{Choice of proposal distribution}
The original paper of Metropolis et al. \cite{Metropolis1953} employed a symmetric proposal matrix, in which cae the proposal matrix drops out of the formula for the acceptance. The advante of having a nonsymmetric proposal matrix was pointed out by Hastings \cite{Hastings1970}. One has a lot of freedom in the choice of the proposal probability $P_{prop} (\bm{R}_f | \bm{R}_i)$. The only constraints are that
\begin{enumerate}
	\item $P_{prop} (\bm{R}_f | \bm{R}_i)$ must be a stochastic matrix leading to an ergodic Markov chain (which means which conditions again?)
	\item It must be possible to efficiently sample $P_{prop} (\bm{R}_f | \bm{R}_i)$ with a direct sampling method.
\end{enumerate}
The proposal probability determines 
\begin{enumerate}
	\item The average size of the proposed moves $\bm{R}_i \rightarrow \bm{R}_f$ and 
	\item The average acceptance rate of these moves.
\end{enumerate}
In order to reduce \textbf{sequential correlation}, one has to 
\begin{enumerate}
	\item Make moves as large as possible, but 
	\item With a high acceptance rate.
\end{enumerate}
In practice, for a given form of the proposal matrix, there is a compromise to be found between
\begin{enumerate}
	\item The average size of the proposed moves
	\item The average accpetance rate
\end{enumerate}
\cite{Toulouse2016}

\subsubsection{Proposal distribution - simplest choice}
The simplest choice for $P_{prop} (\bm{R}_f | \bm{R}_i)$ is a distribution that is uniform inside a small cube $\Omega(\bm{R}_i)$ centered in $\bm{R}_i$ and of side length $\Delta$ and zero outside
\[ P_{prop} (\bm{R}_f | \bm{R}_i) = \begin{cases}
				 \frac{1}{\Delta^{3N}} & \quad \text{if} \bm{R}_f \in \Omega(\bm{R}_i) \\
				 0 & \quad \text{elsewhere}  
\end{cases} \]
In practice, a move according to this probability is propsed,
\begin{align}
	\bm{R}_f = \bm{R}_i + \frac{\Delta}{2} \chi
\end{align}
where $\chi$ is a vector of $3N$ random numbers drawn from the uniform distribution between $-1$ and $1$. The size of the cube $\Delta$ can be adjusted so as to \textbf{minimize the autocorrelation time of the local energy}, but the latter remains large and the sampling is inefficient.
\cite{Toulouse2016}

\subsubsection{Proposal distribution - Fokker-Planc and Langevin equations}
Clever choices use information from the distribution $\rho(\bm{R})$, in particular its local gradient, to guide the sampling. A choice for $P_{prop} (\bm{R}_f | \bm{R}_i)$ which would lead to large moves with an acceptance probability equal to 1 would be $P_{prop} (\bm{R}_f | \bm{R}_i) = \rho(\bm{R}_f)$, independently from $\bm{R}_i$, but we would then be back to the initial problem of sampling a complicated distribution $\rho(\bm{R})$. A good choice for $P_{prop} (\bm{R}_f | \bm{R}_i)$ is the Green function of the Fokker-Planck equation in the short-time approximation
\begin{align}
	P_{prop} (\bm{R}_f | \bm{R}_i) = 
	\frac{1}{(2\pi \tau)^{3N/2}}
	e^{- \frac{ ( \bm{R}_f - \bm{R}_i -\bm{v}(\bm{R}_i)\tau )^2 }{ 2\tau } }
\end{align}
where
\begin{align}
	\bm{v}(\bm{R}) = \frac{ \nabla \Psi (\bm{R}) }{ \Psi (\bm{R}) }
\end{align}
is called the \textbf{drift velocity} of the wave function and $\tau$ is the \textbf{time step which can be adjusted so as to minimize the autocorrelation time of the local energy}. In parctice, a move according to this probability is proposed
\begin{align}
	\bm{R}_f = \bm{R}_i + \bm{v} (\bm{R}_i) \tau + \bm{\eta}
\end{align}
where $\bm{\eta}$ is a vector of $3N$ random numbers drawn from the Gaussian distribution of average 0 and standard deviation $\sqrt{\tau}$. The term $\eta$ describes an \textbf{isotropic Gaussian diffusion process (or Wiener process)}. The term $\bm{v}(\bm{R}_i) \tau$ is a \textbf{drift term} which moves the random walk in the direction of increasing $| \Psi (\bm{R})|$. 

The optimal size of the move is smaller in regions where $\bm{v}(\bm{R})$ is changing rapidly. For example, $\bm{v}(\bm{R})$ has a discontinuity at the nuclear positions. Hence, it is more efficient to make smaller moves for electrons in the core than for electrons in the valence regions. In doing this, care must be taken to ensure the \textbf{detailed balance} condition. An elegant solution is provided in the VMC algorithm of (Two Refs) where the electron moves are made in spherical coordinates centered on the nearest nucleus and the size of radial moves is proportional to the distance to the nearest nucleus.
In addition, the size of the angular moves gets larger as one approaches a nucleus. This algorithm allows one to achieve, in many cases, an autocorrelation time of the local energy close to 1.
\cite{Toulouse2016}

\subsubsection{Practical considerations}
Mentioned in relation to nucleus discontinuity issues because we need the derivatives for the sampling to make sense: (There is also a derivative discontinuity when two electrons overlap, but this is less problematic since electrons repel each other)
\cite{Umrigar1999}

\subsubsection{One or all particles}
\subsubsection{If one particle at the time - choose them randomly or sequentially}
\subsection{Convergence and errors}
\subsubsection{Notes from Umrigar1999}
In the Monte Carlo method, direct sampling methods, were feasible, are preferable to the Metropolis method, since the points generated by the latter are \textbf{serially correlated}, sometimes with very long \textbf{correlation times}. 

The rate at which the initial distribution evolves to the desired distribution $\rho$ and the \textbf{autocorrelation time} of estimates of various observables is governed by the subdominant eigenvalues. The ideal situation (realized in direct sampling methods) is that all the other eigenvalues are zero. In that case a distribution of initial configurations evolves to $\rho$ in a single Monte Carlo step and every measurement is independent. In practice, Markov matrices with short autocorrelation times for the values of interest are constructed not by analyzing the eigenvalue spectrum of the matrix but by using heuristic arguments to device matrices that allow the system to evolve rapidly through configuration space.

The points sampled by the Metropolis method are sequentially correlated, resulting in a loss of computational efficiency. The effective number of independent observations in a Monte Carlo run of length $N$ is only $N/T_{corr}$ where $T_{corr}$ is the autocorrelation time of the observable of interest for the particular form of the Metropolis method used. It is clearly advantageous to reduce $T_{corr}$. This can be done by either
\begin{enumerate}
	\item Increasing the average sixe of the proposed moves or by
	\item Increasing the acceptance of the moves
\end{enumerate}

Some observations on the Metropolis method:
\begin{itemize}
	\item The Metropolis method automatically samples $\rho(\bm{R})/\sum \rho(\bm{R})$ and can only be used to calculate expectation values of the form
	\begin{align}
		\frac{\sum \rho(\bm{R}) X(\bm{R})} {\sum \rho(\bm{R})}
	\end{align}
	\item The variance of the estimate for the expectation value $\langle X \rangle$ is given by
	\begin{align}
		\frac{T_{corr}}{N} (\frac{\sum (X(\bm{R}_i))^2}{N} - (\frac{\sum X(\bm{R}_i)}{N})^2  )
	\end{align}
	That is, the effective number of configurations $N_{eff}$ is smaller than $N$ by a factor of $T_{corr}$, which we define to be the autocorrelation time.
	\item As mentioned, for a given functional form of $S(\bm{R}_f | \bm{R_i})$, the acceptance decreases as the volume $\Omega(\bm{R}_i)$ (and therefore the average size of the proposed moves) increases, so there exists an optimal $\Omega(\bm{R}_i)$ for which the system evolves the fastest. There is folklore that the optimal occurs when the average acceptance ratio is close to 1/2. In fact the optimal choice may have an average accpetance that is anywhere between zero and one. A much \textbf{better cirterion is to maximize the rate at which the system diffuses} through configuration space
	\begin{align}
		\langle A(\bm{R}_f | \bm{R}_i) (\bm{R}_f - \bm{R}_i)^2 \rangle
	\end{align}
	The real measure of goodness is of course \textbf{the autocorrelation time for the observable of interest}.
\end{itemize}

The efficiency of the algorithm is inversely proportional to the autocorrelation time of observables of interest, thus the autocorrelation time is a measure of the algorithm's efficiency. It is determined as follows. The enture Monte Carlo run (after discarding the equilibration updates), consists of $N$ Monte Carlo updates that are divided into $N_b$ blocks each consisting of $N_s$ Monte Carlo steps for each of the $N$ electrons. The local energy is measured after each Monte Carlo update. The autocorrelation time is given by 
\begin{align}
	T_{corr} = N_s (\frac{\sigma_b}{\sigma})^2
\end{align}
where $\sigma$ and $\sigma_b$ are the root-mean-square fluctuations of the individual energies and of the block average energies respectively. $N_s$ must be chosen such that $N_s \gg T_{corr}$. The autocorrelation time presented in the article table 1 were obtained using values of $N_s$ that were at least 100 times greater than $T_{corr}$. Using $N_s \approx 10 T_{corr}$ results in estimates of $T_{corr}$ that are too low by as much as 20\%. When Monte Carlo moves consist of moving one electron at a time, it takes twice the computer time to move all the electron as compared to an algorithm where they are all moved at once \cite{Ceperley1977}. Hence, in order to have a fair comparison, table 1 has values of $T_{corr}^\ast = 2T_{corr}$ for algorithms moving only one electron at a time. We note that $\sigma$ and the variational energy $\bar{E}$ \textbf{depend on the trial wavefunction but not on the alrogithm used}. $T_{corr}$ and the accpetance $\bar{A}$ \textbf{depend on both the algorithm and the trial wavefunction}.
\cite{Umrigar1999}

\subsubsection{Results discussion in Umrigar 1999}
For each of the algorithms $\Delta$ etc. were crudely optimized to yield the smallest possible $T_{corr}$. Table 1 shows the values of $T_{corr}^\ast$ for approximately optimal values of the parameters. Moderate variations of the parameters about their optimal values affect the efficiency of the algorithm only slightly. For example, changing the value of $\Delta_r$ from 5 to 4 alters the value of $T_{corr}$ by less than 10\% for each of the wavefunctions. We see that large improvements of $T_{corr}$ are achieved - it gets smaller by factors of ... respectively for the different wavefunctions as we progress from algorithm 1 to 4.

Note that for each of the four algorithms $T_{corr}$ is smaller for the good Ne wavefunction than for the simple Ne wavefunction. The likely reason for this is that the good Ne wavefunction has a more rapidly varying local energy (but with smaller amplitude of course) and consequently it takes fewer Monte Carlo steps to wander from a region where the local energy is too high to one where it is too low and vice versa. Hence the gain in efficiency from improving the wavefunction is greater than would be supposed by merely comparing their respective values of $\sigma$.

For algorithm 1 there is a considerable increase in $T_{corr}$ in going from Ne to Ar, while for algorithm 4, $T_{corr}$ increases very little. This does not mean that calculations for heavy atoms can be performed as rapidly as those of light atoms, unless, of course, pseudopotentials are used to eliminate the core electrons. 
\begin{enumerate}
	\item First, the time for evaluating determinants in the wavefunctions scales as $N^3$. 
	\item Second, the fluctuation in the local energy $\sigma$ increases with the number of electrons $N$. The dependence of $\sigma$ on $N$ is different depending on whether one is interested in 
	\begin{itemize}
		\item progressively heavier neutral atoms, or in 
		\item increasing the number of atoms of the same species. 
	\end{itemize}
\end{enumerate}
It is not known how $\sigma$ scales with the nuclear charge $Z$. We show later empirically that going from the lightest to the heaviest first-row homonuclear molecules the increase is somewhat larger than $Z^2$, but that for the heavier molecules the increase is nealry linear in $Z$. \textbf{The statistical error goes down as the square root of the number of Monte Carlo updates or equivalently the computer time}. Hence, if we assume $\sigma \sim Z^2$, \textbf{then the computer time needed to obtain a given statistical error scales as} $T \sim Z^7$, while if $\sigma \sim Z$ , then $T \sim Z^5$. 

On the other hand, the scaling is not as bad if one is interested in increasingly larger clusters of atomes of fixed $Z$. In that case $\sigma \sim \sqrt{N}$ and so $T \sim N^4$ . 

The scaling for each of these situations is worse of course if one uses a Metropolis method for which $T_{corr}$ \textbf{increases with system size}. The poor scaling wrt $Z$ is the reason why it is necessary to employ pseudopotentials in calculations involving second row and heavier atoms.

One of the values $T_{corr} = T_{corr}^\ast /2 = 0.85$ in the table may at first sight seem impossible since one may suppose that the best possible value for $T_{corr}$ is 1. The reason it is in fact possible is that we are using a single-electron move algorithm and updating the local energy at each Monte Carlo step but $T_{corr}$ is in \textbf{units of all-electron moves} (?).
\cite{Umrigar1999}

\subsubsection{Variance minimization method for optimizing wavefunctions - Umrigar 1999 discussion}
\begin{itemize}
	\item Optimized trial wavefunctions that closely approximate eigenstates of Hamiltonians are essential ingredients of accurate electronic structure calculations employing quantum Monte Carlo methods. The quality of these trial wavefunctions is relevant both for
	\begin{itemize}
		\item expectation values of physical interest and for 
		\item the variance of the Monte Carlo estimators. 
	\end{itemize}
	\item As the trial wave function approaches an exact eigenstate of the Hamiltonian, the energy and expectation values of operators commuting with the Hamiltonian satisfy a \textbf{zero-variance principle}, i.e., 
	\begin{itemize}
		\item the expectation values approach the exact eigenstate values, while 
		\item the Monte Carlo variance goes to zero.
	\end{itemize}
\end{itemize}

A major advantage of the quantum Monte Carlo methods is that \textbf{there is no restriction on the form of the trial wavefunction} $\Psi (\bm{R})$. Hence, any insight one may have, as regards to the nature of the many-body correlations can be built into $\Psi(\bm{R})$ and tested. To exploit this freedom it is necessary to have a method for optimizing arbitrary wavefunctions.

\begin{itemize}
	\item The method consists of the minimization of the variance of the local energy (Refs) over a set of $N_c$ configurations $\{ \bm{R}_i \}$ sampled from the square of the best wavefunction, $\Psi_0$, available before we start the optimization, i.e.,
	\begin{align}
		\sigma_{opt}^2 [\Psi] = \sum_i^{N_c} [ \frac{ \mathcal{H}\Psi(\bm{R}_i) }{ \Psi(\bm{R}_i) } - \bar{E} ]^2
		\frac{ w(\bm{R}_i) }{ \sum_i^{N_c} w(\bm{R}_i) }
	\end{align}
	where 
	\begin{align}
		\bar{E} = \sum_i^{N_c} \frac{ \mathcal{H} \Psi(\bm{R}_i) }{ \Psi(\bm{R_i}) }
		\frac{ w(\bm{R}_i) }{ \sum_i^{N_c} w(\bm{R}_i) } 
	\end{align}
	is the average energy over the sample of configurations. The weights
	\begin{align}
		w(\bm{R}_i) = | \frac{ \Psi(\bm{R}_i) }{ \Psi_0 (\bm{R}_i) } |^2
	\end{align}
	are introduced to allow the \textbf{nodes} (??) of the wavefunction to move freely during the optimization and to provide the correct weighting (??). The ratio of each of the weights to the average weight is not allowed to exceed a max value; otherwise, it is possible for the optimizer to achieve a small value of $\sigma_{opt}$ by having a few configurations gain a very large weight and a local energy that is very close to $\bar{E}$. Typically, 3000 configurations are sufficient to optimize a wavefunction with 50-100 parameters.
	\item Variance minimization is more effective than energy minimization because the smallest possible variance is zero whereas the energy is unbounded from below on the necessarily sparse set of $N_c$ configurations on which the optimization is performed (\textbf{in a many-dimensional space any number of configurations, that can fit in computer memory, is a sparse sample}). A direct attempt to minimize the energy of a many-dimensional, multi-parameter wavefunction frequently leads to a wavefunction that has
	\begin{itemize}
		\item A low estimated energy on the $N_c$ configurations used to perform the optimization but 
		\item A higher energy than that of the initial wavefunction on a new set of Monte Carlo configurations.
	\end{itemize}
	The "optimiziation" results in a worse wavefunction! (This is overfitting???!!!)
	\item  There are other advantages to variance minimization. Since the function to be minimized is a sum of squares, effective optimization techniques such as the Levenberg-Marquardt method can be employed. In practice we employ a modified (REF - by the author, unpublished, the program is available upon request) \textbf{Levenberg-Marquardt} method wherein up-hill moves are permitted under certain conditions. 
	\item Another advantage is that since any eigenstate has zero variance, the method can be used to optimize trial wavefunctions for excited states as well, including those that have the same symmetry as the ground state. 
	\item Finally, the method can be used even for Hamiltonians that have a spectrum that is unbounded from below, e.g., a relativistic Hamiltonian.
	\item In practice one optimizes a combination of the variance and the energy by a trivial modification of the procedure described above. One possibility (Ref) is to replace $\bar{E}$ by $E_{guess}$, where $E_{guess}$ is chosen to be slightly lower than the best current estimate for the energy. A more elegant solution \cite{Meierovich1996} is to optimize $\sigma^2 / \bar{E}^2$.
	\item We note that the variance minimization method has been extended to simultaneously otpimize several trial states (Ref), to find optimized trial states of transfer matrixes (Ref), and to find the subdominant trial state of a Markov matrix (Ref). The latter two are of relevance to static and dynamic equilibrium problems in statistical mechanics.
\end{itemize}
\cite{Umrigar1999}

\subsubsection{OBS!!! minimization of variance vs energy}
Optimizing wavefunctions by minimizing the variance of the energy is an old idea dating back to the 1930s. The first application within Monte Carlo methods may have been by Conroy [77], but the method was popularized within QMC by the work of Umrigar et al [78]. It is now generally believed that it is better to minimize the VMC energy than its variance, but it has proved more difficult to develop robust and efficient algorithms for this purpose. Since the trial wavefunction forms used cannot generally represent energy eigenstates exactly, except in trivial cases, the minima in the energy and variance do not coincide. Energy minimization should therefore produce lower VMC energies, and although it does not necessarily follow that it produces lower DMC energies, experience indicates that, more often than not, it does.
\cite{Needs2010}

\subsubsection{Forms of Many-Body Wavefunctions}
We next discuss the functional form of the wavefunctions used in electronic structure calculations of atoms and molecules.
\begin{itemize}
	\item Quantum chemistry methods such as Configuration Interaction (CI) expand the wavefunction as a linear combination of determinants of single-particle orbitals. In the limit of an infinite sum this is exact but the rate of convergence is very slow, in part because the true wavefunction has a cusp at elecron coincidences but the determinants, used to expand the wavefunction, do not. 
	\item In the quantum Monte Carlo methods the above form of the wavefunction is multiplied by a generalized Jastrow factor (also known as a Feenberg factor), expressed in terms of electron-electron and electron-nucleus coordinates, which allows the trial wavefunction to have the correct cusp at electron coincidences. 
	\item Since the Jastrow factor is everywhere positive, it does not alter the nodes of the wavefunction and so it is necessary to have multiple determinants to accurately describe the nodes of the wavefunction. The intuition one has is that 
	\begin{itemize}
		\item the \textbf{near-degeneracy correlation} (?) is most effectively described by the linear combination of determinants, whereas 
		\item the Jastrow factor describes well the remaining correlation (which requires very large number of determinants in a purely determinantal wavefunction) and allows the imposition of the electron-electron cusps.
	\end{itemize}
	In practice it's possible to construct rather compact quantum Monte Carlo wavefunctions with typically 50 to 100 parameters (!!!) that have an energy that is comparable to a CI wavefunction with orders of magnitude (!!!) more parameters.
	\item Some more discussion, slaters, spin, etc
	\item In order to obtain a wavefunction with a small variance of the local energy, it is important that the trial wavefunction satisfy a set of \textbf{cusp conditions} which prescribe the proper derivative discontinuity at the collision points to \textbf{ensure that the divergence in the local potential is canceled by an opposite divergence in the local kinetic energy}. Kato (REF) first rigorously derived these conditions as a general property of Coulombic systems. He showed that in the limit that two particles of masses $m_i$ and $m_j$ and charges $q_i$ and $q_j$ approach each other and all other interparticle distances remain larger than zero,
	\begin{align}
		\frac{ \partial \hat{\Psi} }{ \partial r_{ij} } |_{r_{ij}=0} = \mu_{ij} q_i q_j \Psi (r_{ij} = 0),
	\end{align}
	where $\mu_{ij} = m_i m_j/(m_i + m_j)$ is the reduced mass of the two-particle subsystem and $\hat{\Psi}$ is the average of $\Psi$ over an infinitesimally small sphere centered at $r_{ij} = 0$. 
	\item Pack and Byers-Brown (Ref) generalized Kato's result by solving the multiparticle Schrodinger equation in the neighborhood of a two-particle coincidence. Writing the wavefunction near such a collision point, $\bm{r}_{ij}=0$, as
	\begin{align}
		\Psi = \sum_{l=l_0}^\infty \sum_{m=-l}^l f_{lm} (r) r^l Y_{lm} (\theta, \phi)
	\end{align}
	where $r=r_{ij}$; $\theta=\theta_{ij}$; $\phi=\phi_{ij}$, and the $Y_{lm} (\theta, \phi)$ are the usual spherical harmonics, and, expanding $f_{lm}(r) = \sum_{k=0}^\infty f_{lm}^{(k)} r^k$, they found for electron-electron collisions,
	\begin{align}
		f_{lm} (r) = f_{lm}^{(0)} [ 1 + \frac{1}{2(l+1)}r + O(r^2) ]
	\end{align}
	\item The terms $r^l Y_{lm} (\theta, \phi)$ are analytic functions that can be well described by the slater determinants, while the terms $ [ 1 + \frac{1}{2(l+1)}r ]$ are nonanalytic functions of the individual electron coordinates, having cusps of magnitude $\frac{1}{2(l+1)}$ (though they are of course analytic functions of the interparticle distance $r$). 
	\item The local energy at the collision point is finite provided that the cusp condition (given in the equation that expressed $f_{lm}(r)$) is obeyed for the lowest value $l_0$ appearing in the sum in the $\Psi$ expression.
	\item Some discussion of parallel/antiparallel spin cusp factor.
	\item \textbf{Results}:
	\begin{itemize}
		\item Improvements to quantum Monte Carlo wavefunctions, while staying within the form stated (Jastrow times sum of slaters), can be made by either
		\begin{itemize}
			\item Adding more determinants or by
			\item \textbf{Including higher-order correlations in the Jastrow factor}.
		\end{itemize}
		Unfortunately \textbf{there has not been sufficient systematic study of either}, in part because, in implementing either of these improvements, 
		\begin{itemize}
			\item Multiple choices have been made in choosing the precise functional form and also because
			\item \textbf{One is never completely sure that the best parameters, for a given functional form, have been obtained}.
		\end{itemize}
	\end{itemize}
	Nevertheless, some conclusions can be drawn from existing studies.
	\item Subsection which reviews a study of the improvements obtained by adding low-lying determinants (Ref) to wavefunctions of first-row homonuclear diatomic molecules 
	\item Subsection which reviews the improvements obtained from using higher-order correlations (Ref) in the Jastrow factor.
	\begin{itemize}
		\item Early quantum Monte Carlo work was done using a two-body (electron-electron) Jastrow factor. Since then it has become apparent that inclusion of three-body (electron-electron-nucleus) terms results in a large lowering of the energy (Ref). The results for molecules presented in the previous subsection were all obtained with a three-body Jastrow factor. 
		\item It is natural to ask whether it is worthwhile to include four-body ($e^3-n$) correlations in the Jastrow factors. A systematic expansion of the Jastrow factor consists of a polynomial in scaled inter-particle coordinates. By "scaled" we mean that a monotonic function is used to map the infinite range of the inter-particle coordinates onto a finite interval. As shown in table 2 the number of terms in such an expansion grows rapidly, both as a function of the body order and as a function of the polynomial order of the expansion. If some or all of the particles are identical then the theory of invariants can be used to reduce the computational effort required to evaluate the function. This was first done for identical bosons, for which it was possible to include upto 5-body correlations (REF!!!) and later for electronic structure calculations where four-body ($e^3-n$) correlations were included (Ref).
		\item In table 3 we show the variational Monte Carlo energyies $E^{VMC}$ and the standard deviation of the local energies $\sigma_{VMC}$ of the Li, Be, and Ne atoms for wavefunctions containing two-body, three-body and four-body correlations. Inclusion of the three-body correlations results in a large improvement in the energy and a large reduction in the standard deviation of the local energy. For Li, Be and Ne, 95\%, 89\% and 84\% of the part of the correlation energy missing in the two-body wavefunctions is recovered while the standard deviation of the local energy is reduced by factors of 6, 4, and 2 respectively. 
		\item Although not shown in the table, a further advantage accrues from the fact that the auto-correlation time of the local energies sampled in the VMC and in diffusion MC are somewhat reduced. 
		\item We observe from table 3 that inclusion of the four-body correlations also results in a significant but smaller improvement in both the variational energies and the fluctuations of the local energy. That improvement gets smaller with increasing atomic number and is disappointingly small for Ne.
		\item Some more discussion in relation to DMC
		\item The best possible Jastrow factor (one that includes all-body correlations and has an infinite-order polynomial) would have a VMC energy that equals the fixed-node DMC energy. Hence, a measure of the efficiency of the four-body contributions to the Jastrow factor is
		\begin{align}
			\eta = \frac{ E_{VMC}^{(n-1)-body} - E_{VMC}^{n-body} }{ E_{VMC}^{(n-1)-body} - E_{DMC} }
		\end{align}
		The percent efficiency is much larger going from two-body correlations to three-body correlations than going from three-body correlations to four-bodt correlations. In the latter case, it rapidly decreases from 60\% for Li to 31\% for Be to 9\% for Ne. The reduction in the root mean square fluctuations of the local energy is 24\% for Li, 12\% for Be and 2\% for Ne. 
		\begin{itemize}
			\item These somewhat disappointing results are probably due to flaws of the \textbf{nodal surface of the trial wavefunction}, which the four-body interaction Jastrow factor is not designed to correct.(???) That is, the approximate, fixed-node, DMC wavefunction has discontinuous derivative almost everwhere across the nodal surface. On the other hand, a Jastrow factor expressed in terms of inter-particle coordinates, will have non-analyticities only at the $(3N-3)$-dimensional surface where particles coincide, which constitute only a vanishingly small fraction of the entire $3N-1$-dimensional nodal surface.
			\item Hence, over most of the nodal surface, we are \textbf{attempting to describe a non-analytic function as a finite sum of analytic functions} and we expect the convergence to be slow. 
			\item The validity of the above argument has been tested by performing calculations for the nodeless, bosonic ground states of the Hamiltonians of the same atoms. As expected, the efficiency for the \textbf{bosonic} states is very much greater than for the fermionic states and the reader is referred to (REF!!!!!) for details.
		\end{itemize}
		\item Another interesting observation that emerges from the calculations on the bosonic states is that the fluctuations of the local energy $\sigma$ are much smaller than for the corresponding fermionic states, despite the fact that the absolute energies are larger for the bosonic states. This indicates that the \textbf{difficulty of obtaining a good parameterization of the nodal surface of a fermionic state, or more generally of any excited state, is a significant bottleneck in the search for optimized trial wavefunctions}.
	\end{itemize}
	\item For any given wavefunction it is relevant to ask whether further improvements can be made most economically by 
	\begin{enumerate}
		\item increasing the variational freedom of the determinantal part of the wavefunction (by increasing the number of single-particle basis functions and/or the number of determinants) or by 
		\item improving the Jastrow part of the wavefunction (by increasing the polynomial-order and/or the body-order).
	\end{enumerate}
	It is clear that 
	\begin{itemize}
		\item if we follow the former route to the limit of a complete basis of Slater determinants then the exact result can be obtained, even without a Jastrow factor. However, the rate of convergence would be exceedingly slow, because the Slater determinants lack singularities present in the wavefunction, namely, cusps at electron-electron coincidence points.
		\item These cusps can already be built into the wavefunction at the level of the two-body correlations by including a Jastrow factor. Incorporating three-body correlations is clearly very advantageous, but the results show that inclusion of four-body correlations may not be the most economical next step to further improvement of the wavefunction, at least for the heavier systems. 
		\item Instead, it may be preferable ti unclude more determinants in the wavefunction. Once a sufficiently large number of determinants have been included, it seems likely that it may again become more economical to improve the Jastrow part by including the four-body correlations.
		\item Such explorations are needed in order to fully exploit the flexibility that QMC offers over conventional quantum chemistry methods for the construction of accurate, yet relatively compact, wavefunctions.
	\end{itemize}
\end{itemize}

\cite{Umrigar1999}

\subsection{Gibbs Sampling}
If certain conditional probabilities are available to us we may simplify the Metropolis-Hastings algorithm by using them. We see then that the acceptance probability is one, that is, all proposed samples are accepted.

\subsection{Minimization}
Minimization is a huge field in itself. The algorithms proposed here will all have their basis in the gradient descent method. 

\subsubsection{Gradient Descent}
Given the gradient $G_i$ at iteration $i$ the updating scheme for the parameters $\alpha$ is then
\begin{align}
	\alpha_i = \alpha_i - \eta G_i
\end{align}
where $\eta$ is referred to as the step size or the learning rate, as is common in machine learning.

\subsubsection{Other}
We could write about Adaptive SGD here but probably some other like AdaGrad or Adam might be more interesting. Also there is some unclearity on whether we are doing stochastic gradient descent simply by doing stochastic samples or if we should also do mini-batches.

\subsection{The algorithm}
Putting it all together, we have the variational monte carlo method. 

\section{Quantum Dot}
The quantum dot may be modeled by an Hamiltonian with a harmonic oscillator potential and a Couloumb interaction potential in order to represent the interaction between particles.


\chapter{Statistical toolbox - inc methods shared by traditional and ML}
\section{Building blocks}
-the i.i.d. assumption, central limit theorem, how did this enter Mortens regression lectures, the thing i was wondering about
-variance
-covariance
-correlation
-autocorrelation - a type of cross-correlation
-autocorrelation time, effective number of measurements (mortens vmc pdf)
-time-displacement autocorrelation

-Bayesian inference
-Maximum Likelihood Estimation
-Its relation to regression and least squares
-Regression as the simplest form of supervised learning - laying the ground for DNNs in the ML chapter

there is a Bayesian formulation of linear regression, the method is called MLE, and from it can be derived the least squares method (does this mean MLE more general than LS? or are they equal?)
So the LS/regression cost function = the negative of the Bayesian log likelihood func

\section{Sampling}
\subsection{Markov Chain Monte Carlo}
\subsection{Metropolis}
ergodicity/detailed balance
with and without "hastings"
\subsubsection{Brute Force}
\subsubsection{Importance Sampling}

\subsection{Gibbs Sampling}
\subsection{Blocking}

\section{Optimization}
\subsection{Gradient Descent}
\subsubsection{Variations}
\subsection{Second order}



\chapter{Machine learning}
\section{Overview}
\section{Regression etc etc}
\section{DNNs - getting to neural networks}
This function can be represented in the form of a network diagram as shown in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward propagation of information through the network. It should be emphasized that these diagrams do not represent probabilistic graphical models of the kind to be consid- ered in Chapter 8 because the internal nodes represent deterministic variables rather than stochastic ones. For this reason, we have adopted a slightly different graphical notation for the two kinds of model. We shall see later how to give a probabilistic interpretation to a neural network.
http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} (used in the german master's thesis)


\section{Discriminative vs Generative}
Basically: want generative because we want to be able to sample.

Generative or Discriminative? Getting the Best of Both Worlds. Bruk denne !!!
%https://pdfs.semanticscholar.org/fcd8/9e702a15b67abbec1b4d81584d812f79dfda.pdf?_ga=2.136588272.1961892878.1525605616-1442489588.1525605616

DEEP DISCRIMINATIVE AND GENERATIVE MODELS FOR PATTERN RECOGNITION
https://pdfs.semanticscholar.org/cea9/c5f7117b3db7e62f35b4d290cfb84ddd7ba3.pdf

Generative classifiers learn a model of the joint prob, $p(x,y)$, of the inputs $x$ and the label $y$, and make their predictions by using Bayes rules to calc $p(y|x)$, and then picking the most likely label $y$. Discriminative classifiers model the posterior $p(y|x)$ directly, or learn a direct map from inputs $x$ to the class labels. 
https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf



\section{Graphical models - undirected=Bayesian networks(=belief networks, causal networks), directed=Markov networks (=Markov random fields)}
Literature:
Murphy and
http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} (used in the german master's thesis)

and this article

Exponential Family Harmoniums
with an Application to Information Retrieval
https://www.ics.uci.edu/~welling/publications/papers/GenHarm3.pdf

\subsection{Energy based (=a type of MRF?)}

http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf

\subsection{Random Markov Fields}
\begin{itemize}
	\item Connection to \textbf{hopfield networks} (introduced around 80ies, binary nodes) (a type of MRF). Unlike BM, units not stochastic.
	\item In probability theory and related fields, a \textbf{stochastic or random process} is a mathematical object usually defined as a collection of random variables. Historically, the random variables were associated with or indexed by a set of numbers, usually viewed as points in time, giving the interpretation of a stochastic process representing numerical values of some system randomly changing over time
	\item A \textbf{random field} is a generalization of a stochastic process such that the underlying parameter need no longer be a simple real or integer valued "time", but can instead take values that are multidimensional vectors, or points on some manifold.
	At its most basic, discrete case, a random field is a list of random numbers whose indices are identified with a discrete set of points in a space (for example, n-dimensional Euclidean space). When used in the natural sciences, values in a random field are often spatially correlated in one way or another. In its most basic form this might mean that adjacent values (i.e. values with adjacent indices) do not differ as much as values that are further apart. This is an example of a covariance structure, many different types of which may be modeled in a random field. More generally, the values might be defined over a continuous domain, and the random field might be thought of as a "function valued" random variable.
	\item \textbf{Markov random fields} 
	In the domain of physics and probability, a Markov random field (often abbreviated as MRF), Markov network or undirected graphical model is a set of random variables having a Markov property described by an undirected graph. In other words, a random field is said to be a Markov random field if it satisfies Markov properties.

	A Markov network or MRF is similar to a Bayesian network in its representation of dependencies; the differences being that Bayesian networks are directed and acyclic, whereas Markov networks are undirected and may be cyclic. Thus, a Markov network can represent certain dependencies that a Bayesian network cannot (such as cyclic dependencies); on the other hand, it can't represent certain dependencies that a Bayesian network can (such as induced dependencies). The underlying graph of a Markov random field may be finite or infinite.

	When the joint probability density of the random variables is strictly positive, it is also referred to as a Gibbs random field, because, according to the Hammersley–Clifford theorem, it can then be represented by a Gibbs measure for an appropriate (locally defined) energy function. The prototypical Markov random field is the Ising model; indeed, the Markov random field was introduced as the general setting for the Ising model.

	The foundations of the theory of Markov random fields may be dound in Preston (1974) and Spitzer (1971). The concept of an MRF came from attempts to put into a general probabilistic setting a very specified model named after the German phycisist Ernst Ising, a student of Lenz. Ising published his results in 1925 while a paper by Lenz in 1920 gave a sketchy idea of the model.
	\item An RMF is a type of Product of Experts
	\item A BM is a type of RMF
\end{itemize}

Boltzmann Machines are networks just like neural nets and have units that are very similar to Perceptrons, but instead of computing an output based on inputs and weights, each unit in the network can compute a probability of it having a value of 1 or 0 given the values of connected units and weights. The units are therefore stochastic - they behave according to a probability distribution, rather than in a known deterministic way. 


In order to establish the framework in which to introduce the Boltzmann Machine in the next section, we need to leave this one off with having introduced the general form of a Random Markov Field. In order to then show the BM in next chapter on this form.

Long section from http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf
\cite{Bishop2006} follows below.

We turn now to the second ma- jor class of graphical models that are described by undirected graphs and that again specify both a factorization and a set of conditional independence relations.
A Markov random field, also known as a Markov network or an undirected graphical model (Kindermann and Snell, 1980), has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes. The links are undirected, that is they do not carry arrows. In the case of undirected graphs, it is convenient to begin with a discussion of conditional independence properties.

Let us denote a clique by $C$ and the set of variables in that clique by $\bm{x}_C$. Then the joint distribution is written as a product of \textit{potential functions} $\phi$ over the maximal cliques of the graph
\begin{align}
	p(\bm{x}) = \frac{1}{Z} \prod_C \phi_C (\bm{x}_C)
\end{align} 
where $Z$, sometimes called the \textit{partition function}, is a normalization constant given by
\begin{align}
	Z = \sum_{\bm{x}} \prod_C \phi_C (\bm{x}_C)
\end{align}
By considering only potential functions which satisfy $\phi_C (\bm{x}_C) \geq 0$ we ensure that $p(\bm{x}) \geq 0$. Here $\bm{x}$ assumed to comprise discrete variables, but it can also be continuous or a combination of the two, given that the summation in $Z$ is replaced by the appropriate combination of summation and integration.

The presence of this normalization constant is one of the major limitations of undirected graphs. If we have a model with $M$ discrete nodes each having $K$ states, then the evaluation of the normalization term involves summing over $K^M$ states and so (in the worst case) is exponential in the size of the model. 

Have so far
\begin{enumerate}
	\item Discussed the notion of conditional independence based on simple graph separation
	\item Proposed a factorization of the joint distribution that is intended to correspond to this conditional independence structure
\end{enumerate}
In order to make a formal connection/ precise relationship between factorization for undirected graphs and conditional independence we need to restrict attention to potential functions $\phi_C (\bm{x}_C)$ that are \textit{strictly} positive.

Define
\begin{itemize}
	\item $\mathcal{U I}$ = the set of \textit{all possible distributions defined over a fixed set of variables corresponding to the nodes of a particular undirected graph} that are consistent with the set of conditional independence statements that can be read from the graph using graph separation.
	\item $\mathcal{U F}$ = the set of \textit{all possible distributions defined over a fixed set of variables corresponding to the nodes of a particular undirected graph} that can be expressed as a factorization of the form presented above with respect to the maximal cliques of the graph. 
\end{itemize}
The Hammersley-Clifford theorem (Clifford, 1990) states that the sets $\mathcal{U I}$ and $\mathcal{U F}$ are identical.

Because we are restricted to potential functions which are strictly positive it is convenient to express them as exponentials, so that
\begin{align}
	\phi_C (\bm{x}_C) = e^{-E_C(\bm{x}_C)}
\end{align}

where $E(\bm{x}_C)$ is called an \textit{energy function}, and the exponential representation is called the \textit{Boltzmann distribution}. The joint distribution is defined as the product of potentials, and so the total energy is obtained by adding the energies of each of the maximal cliques.

The joint distribution is then
\begin{align}
	p(\bm{x}) =& \frac{1}{Z} \prod_C \phi_C (\bm{x}_C) \nonumber \\
	=& \frac{1}{Z} \prod_C e^{-E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-\sum_C E_C(\bm{x}_C)} \nonumber \\
	=& \frac{1}{Z} e^{-E(\bm{x})}
\end{align} 

The set of random variables $\bm{x}$ then forms a Markov random field with respect to $G$ (the undirected graph to which the set of cliques $C$ belong). 






\chapter{The Restricted Boltzmann Machine}

\section{The Boltzmann Machine}

Necessity of hidden units, free energy, relation to logistic regression. \cite{Osogami2017}

A general energy expression
\begin{align}
	E_{BM}(\bm{x}, \bm{h}) =& - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j) \nonumber \\
	&- \sum_{i, m=i+1, k}^{M, M, K} \alpha_i^k (x_i) v_{im}^k \alpha_m^k (x_m)
	- \sum_{j,n=j+1,l}^{N,N,L} \beta_j^l (h_j) u_{jn}^l \beta_n^l (h_n)
\end{align}
with
\begin{itemize}
	\item $\alpha_i^k (x_i), \beta_j^l (h_j) $= One dimensional transfer functions, mapping a given input value to a desired feature value. They are sufficient statistics of the model and can be arbitrary non-parametrized functions of the input variable $x_i$ or $h_j$ respectively, but they need to be independent of the parametrization.
	\item $k, l $= These indices denote there can be multiple transfer funcs pr variable.
	\item $a_i^k,  b_j^l$= appear in the first and second term which only depends ont he visible and hidden units respectively. Thus they could be interpreted as the corresponding visible and hidden bias respectively.
	\item $w_{ij}^{kl}$ = inter layer connection term, connects the visible and hidden bias, respectively.
	\item $ v_{im}^k, u_{jn}^l$ = intra layer connection terms, connecting the visible units to each other, and the hidden units to each other, respectively.
\end{itemize}

\subsection{Restricted Boltzmann Machines}
\subsubsection{Energy Function}
OBS: from the high bias - low variance review: the free-energy of the marginalized probability shows the modeling of higher order interactions.

The Boltzmann machine is restricted by removing intra layer connections, meaning nodes within the same layer are conditionally independent. This is done setting $v_{im}$ and $u_{jn}$ to zero. The structure is then a bipartite graph. The expression for the energy of the RBM is then
\begin{align}
	E_{RBM}(\bm{x}, \bm{h}) = - \sum_{i, k}^{M, K} a_i^k \alpha_i^k (x_i)
	- \sum_{j, l}^{N, L} b_j^l \beta_j^l (h_j) 
	- \sum_{i,j,k,l}^{M,N,K,L} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)
\end{align}

%\subsubsection{Joint Probability Density Function}

\subsubsection{Marginal Probability Density Functions}
\begin{align}
	P_{RBM} =& \int P_{RBM} (\bm{x}, \tilde{\bm{h}}) \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\bm{x}, \tilde{\bm{h}}) } \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} \int e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} 
	\dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\int \prod_j^N e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) 
	+ \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)} \dif \tilde{\bm{h}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\biggl( \int e^{\sum_l b_1^l \beta_1^l (\tilde{h}_1) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i1}^{kl} \beta_1^l (\tilde{h}_1)} \dif  \tilde{h}_1 \nonumber \\
	& \times \int e^{\sum_l b_2^l \beta_2^l (\tilde{h}_2) + \sum_{i,k,l} \alpha_i^k (x_i) w_{i2}^{kl} \beta_2^l (\tilde{h}_2)} \dif  \tilde{h}_2 \nonumber \\
	& \times ... \nonumber \\
	& \times \int e^{\sum_l b_N^l \beta_N^l (\tilde{h}_N) + \sum_{i,k,l} \alpha_i^k (x_i) w_{iN}^{kl} \beta_N^l (\tilde{h}_N)} \dif  \tilde{h}_N \biggr) \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j
\end{align}

Similarly
\begin{align}
	P_{RBM} (\bm{h}) =& \frac{1}{Z_{RBM}} \int e^{-E_{RBM} (\tilde{\bm{x}}, \bm{h})} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{RBM}} e^{\sum_{j, l} b_j^l \beta_j^l (h_j)}
	\prod_i^M \int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i
\end{align}

\subsubsection{Conditional Probability Density Functions}
Using Bayes theorem:
\begin{align}
	P_{RBM} (\bm{h}|\bm{x}) =& \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)
	+ \sum_{j, l} b_j^l \beta_j^l (h_j) 
	+ \sum_{i,j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\frac{1}{Z_{RBM}} e^{\sum_{i, k} a_i^k \alpha_i^k (x_i)}
	\prod_j^N \int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j} \nonumber \\
	=& \prod_j^N \frac{e^{\sum_l b_j^l \beta_j^l (h_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)} }
	{\int e^{\sum_l b_j^l \beta_j^l (\tilde{h}_j) + \sum_{i,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (\tilde{h}_j)}  \dif \tilde{h}_j}
\end{align}
Similarly
\begin{align}
	P_{RBM} (\bm{x}|\bm{h}) =&  \frac{P_{RBM} (\bm{x}, \bm{h})}{P_{RBM} (\bm{h})} \nonumber \\
	=& \prod_i^M \frac{e^{\sum_k a_i^k \alpha_i^k (x_i)
	+ \sum_{j,k,l} \alpha_i^k (x_i) w_{ij}^{kl} \beta_j^l (h_j)}}
	{\int e^{\sum_k a_i^k \alpha_i^k (\tilde{x}_i)
	+ \sum_{j,k,l} \alpha_i^k (\tilde{x}_i) w_{ij}^{kl} \beta_j^l (h_j)} \dif \tilde{x}_i}
\end{align}


\subsection{Binary-Binary Restricted Boltzmann Machines}
\subsubsection{Energy Function}
\subsubsection{Joint Probability Density Function}
\subsubsection{Marginal Probability Density Functions}
\subsubsection{Conditional Probability Density Functions}

\subsection{Gaussian-Binary Restricted Boltzmann Machines}
\subsubsection{Energy Function}
We derive this from the general RBM by using $K=3$ and $L=1$ and setting the transfer functions 
\begin{align}
	\alpha_i^1 (x_i) &= -x_i^2 , \\
	\alpha_i^2 (x_i) &= x_i , \\
	\alpha_i^3 (x_i) &= 1 , \\
	\beta_j^1 (h_j) &= h_j , \\
\end{align}
and the parameters to
\begin{align}
	a_i^1 &= \frac{1}{2\sigma_i^2} , \\
	a_i^2 &= \frac{a_i}{\sigma_i^2} , \\ 
	a_i^3 &= -\frac{a_i^2}{2\sigma_i^2} , \\
	b_j^1 &= b_j , \\
	w_{ij}^{11} &= 0 , \\
	w_{ij}^{21} &= \frac{w_{ij}}{\sigma_i^2} , \\
	w_{ij}^{31} &= 0
\end{align}


Inserting this into $E_{RBM}(\bm{x},\bm{h})$ results in the energy
\begin{align}
	E_{GB}(\bm{x}, \bm{h}) =& \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	- \sum_j^N b_j h_j 
	-\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2} \nonumber \\
	=& \norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 - \bm{b}^T \bm{h} 
	- (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}
\end{align}

\subsubsection{Joint Probability Density Function}
The energy results in the joint probability
\begin{align}
	P_{GB} (\bm{x}, \bm{h}) =& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{- \sum_i^M \frac{(x_i - a_i)^2}{2\sigma_i^2}
	+ \sum_j^N b_j h_j 
	+\sum_{ij}^{M,N} \frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} e^{\frac{(x_i - a_i)^2}{2\sigma_i^2}
	- b_j h_j 
	-\frac{x_i w_{ij} h_j}{\sigma_i^2}} \nonumber \\
	=& \frac{1}{Z_{GB}} \prod_{ij}^{M,N} \phi_{GB_{ij}} (x_i, h_j)
\end{align}
with the partition function
\begin{align}
	Z_{GB} =& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \dif \tilde{\bm{x}} \nonumber \\
	=& \int \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} \prod_{ij}^{M, N} \phi_{GB_{ij}} (\tilde{x}_i, \tilde{h}_j) \dif \tilde{\bm{x}}
\end{align}


\subsubsection{Marginal Probability Density Functions}
\begin{align}
	P_{GB} (\bm{x}) =& \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} P_{GB} (\bm{x}, \tilde{\bm{h}}) \nonumber \\
	=& \frac{1}{Z_{GB}} \sum_{\tilde{\bm{h}}}^{\tilde{\bm{H}}} 
	e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \tilde{\bm{h}} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\tilde{\bm{h}}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) \nonumber \\
\end{align}

Furthermore we have, using the factorization property
\begin{align}
	P_{GB} (\bm{h}) =& \int P_{GB} (\tilde{\bm{x}}, \bm{h}) \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} \int e^{-\norm*{\frac{\tilde{\bm{x}} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\tilde{\bm{x}}}{\bm{\sigma}^2})^T \bm{W}\bm{h}} \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} } \int \prod_i^M
	e^{- \frac{(\tilde{x}_i - a_i)^2}{2\sigma_i^2} + \frac{\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{\sigma_i^2} } \dif \tilde{\bm{x}} \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h} }
	\biggl( \int e^{- \frac{(\tilde{x}_1 - a_1)^2}{2\sigma_1^2} + \frac{\tilde{x}_1 \bm{w}_{1\ast}^T \bm{h}}{\sigma_1^2} } \dif \tilde{x}_1 \nonumber \\
	& \times \int e^{- \frac{(\tilde{x}_2 - a_2)^2}{2\sigma_2^2} + \frac{\tilde{x}_2 \bm{w}_{2\ast}^T \bm{h}}{\sigma_2^2} } \dif \tilde{x}_2 \nonumber \\
	& \times ... \nonumber \\
	&\times \int e^{- \frac{(\tilde{x}_M - a_M)^2}{2\sigma_M^2} + \frac{\tilde{x}_M \bm{w}_{M\ast}^T \bm{h}}{\sigma_M^2} } \dif \tilde{x}_M \biggr) \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - a_i)^2 - 2\tilde{x}_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \tilde{x}_i \bm{w}_{i\ast}^T \bm{h}) + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{\tilde{x}_i^2 - 2\tilde{x}_i(a_i + \bm{w}_{i\ast}^T \bm{h}) + (a_i + \bm{w}_{i\ast}^T \bm{h})^2 - (a_i + \bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\int e^{- \frac{(\tilde{x}_i - (a_i + \bm{w}_{i\ast}^T \bm{h}))^2 - a_i^2 -2a_i \bm{w}_{i\ast}^T \bm{h} - (\bm{w}_{i\ast}^T \bm{h})^2 + a_i^2}{2\sigma_i^2} } \dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}
	\int e^{- \frac{(\tilde{x}_i - a_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}}
	\dif \tilde{x}_i \nonumber \\
	=& \frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}
	\nonumber \\
\end{align}

Thus we can calculate the partition function, using factorization..?

\subsubsection{Conditional Probability Density Functions}
\begin{align}
	P_{GB} (\bm{h}| \bm{x}) =& \frac{P_{GB} (\bm{x}, \bm{h})}{P_{GB} (\bm{x})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2}
	\prod_j^N (1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}} ) }
	\nonumber \\
	=& \prod_j^N \frac{e^{(b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j})h_j } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	=& \prod_j^N P_{GB} (h_j|\bm{x})
\end{align}

Resulting in the probability of a particular hidden unit being activated or not being
\begin{align}
	P_{GB} (h_j =1 | \bm{x}) =& \frac{e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j} } }
	{1 + e^{b_j + (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}} \nonumber \\
	&= \frac{1}{1 + e^{-b_j - (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}}
\end{align}
and
\begin{align}
	P_{GB} (h_j =0 | \bm{x}) =&
	\frac{1}{1 + e^{b_j +(\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{w}_{\ast j}}}
\end{align}

We further have
\begin{align}
	P_{GB} (\bm{x}|\bm{h})
	=& \frac{P_{GB} (\bm{x}, \bm{h})}{P_{GB} (\bm{h})} \nonumber \\
	=& \frac{\frac{1}{Z_{GB}} e^{-\norm*{\frac{\bm{x} -\bm{a}}{2\bm{\sigma}}}^2 + \bm{b}^T \bm{h} 
	+ (\frac{\bm{x}}{\bm{\sigma}^2})^T \bm{W}\bm{h}}}
	{\frac{1}{Z_{GB}} e^{\bm{b}^T \bm{h}} \prod_i^M
	\sqrt{2\pi \sigma_i^2}
	e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{- \frac{(x_i - a_i)^2}{2\sigma_i^2} + \frac{x_i \bm{w}_{i\ast}^T \bm{h}}{2\sigma_i^2} }}
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	\frac{e^{-\frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h} }{2\sigma_i^2} } }
	{e^{\frac{2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2 }{2\sigma_i^2}}}
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{- \frac{x_i^2 - 2a_i x_i + a_i^2 - 2x_i \bm{w}_{i\ast}^T\bm{h}
	+ 2a_i \bm{w}_{i\ast}^T \bm{h} +(\bm{w}_{i\ast}^T \bm{h})^2}
	{2\sigma_i^2} }
	\nonumber \\
	=& \prod_i^M \frac{1}{\sqrt{2\pi \sigma_i^2}}
	e^{ - \frac{(x_i - b_i - \bm{w}_{i\ast}^T \bm{h})^2}{2\sigma_i^2}} \nonumber \\
	=& \prod_i^M \mathcal{N}
	(x_i | b_i + \bm{w}_{i\ast}^T \bm{h}, \sigma_i^2)
\end{align}

\subsection{Gaussian-something continuous RBM?}

\section{Training the RBM}
\subsection{Sampling}
\subsection{Cost function}
\subsection{Optimization}
\subsection{Kullback-Leibler Divergence}

\section{The Restricted Boltzmann Machine - References}

\subsection{The history}

\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2004 Welling, Rosen-Zvi, Hinton: Exponential Family Harmoniums with an Application to Information Retrieval} \cite{Welling2005}
\end{itemize}
	\item First introcued in
	\begin{itemize}
		\item 1986 Smolensky: Harmoniums (Information processing in dynamical systems: foundations of harmony theory) \cite{Smolensky1986}
	\end{itemize}
	\item Later papers have studied it under various names:
	\begin{itemize}
		\item 1992 Y Freund, D Haussler: The combination machine (Unsupervised learning of distributions of binary vectors using 2-layer networks) \cite{Freund1992}
		\item 2002 Hinton: The RBM (Training product of experts by minimizing contrastive divergence) \cite{Hinton2002}
	\end{itemize}
\end{itemize}

\subsection{Gaussian-Binary RBM history}

\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2017: Wang, Melchior, Wiskott: Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image Statistics} \cite{Melchior2017}
\end{itemize}
	\item First proposed by
	\begin{itemize}
		\item 2004 Welling, Rosen-Zvi, Hinton: Exponential Family Harmoniums with an Application to Information Retrieval \cite{Welling2005}
	\end{itemize}
	\item A common choice når man trenger cont visibles, ref
	\begin{itemize}
		\item 2009 A. Krizhevsky: Learning multiple layers of features from tiny images (master's thesis) \cite{Krizhevsky2009}
		\item 2011 Cho, Ilin, Raiko: Improved learning of gaussian-bernoulli restricted boltzmann machines \cite{Cho2011}
	\end{itemize}
	\item Training known to be hard, modifiactions to improve it proposed by:
	\begin{itemize}
		\item 2007 Lee, Ekanadham, Ng: Used a sparse penalty during training, allowing them to learn meaningful features from natural image patches (Sparse deep belief net model for visual area v2) \cite{Lee2008}
		\item 2009 A. Krizhevsky: Trained GRBMs on natural images and concluded the difficulties are mainly due to the existence of high-frequency noise in the images, which further prevents the model from learning the important structures. (referenced above) \cite{Krizhevsky2009}
		\item 2011 Theis, Gerwinn, Sinz, Bethge: Illustrates that in terms of likelihood estimation GRBMs are already outperformed by simple mixture models. (In all likelihood, deep belief is not enough) \cite{Theis2011}
		\item Focus on improving the model in the view of generative models
		\begin{itemize}
			\item 2010 Ranzato, Krizhevsky, Hinton: Factored 3-way restricted boltzmann machines for modeling natural images \cite{Ranzato2010}
			\item 2010 Ranzato, Hinton: Modeling pixel means and covariances using factorized third-order boltzmann machines \cite{Ranzato2010a}
			\item 2011 Courville, Bergstra, Bengio: A spike and slab restricted boltzmann machine \cite{Courville2011}
			\item 2011 Le Roux, Heess, Shotton, Winn: Learning a generative model of images by factoring appearance and shape \cite{LeRoux2011}
		\end{itemize}
		\item 2011 Cho, Ilin, Raiko: Suggested the failure of GRBMs is due to the training algo and proposed some modifications to overcome the difficulties encountered in training GRBMs (referenced above) \cite{Cho2011}
	\end{itemize}
	\item All these studies have shown the failures of GRBMs empirically, but to our knowledge there is no analysis of GRBMs apart from out preliminary work:
	\begin{itemize}
		\item 2012 Wang, Melchior, Wiskott: (An analysis of gaussian-binary boltzmann machines for natural image) \cite{Wang2012}
	\end{itemize}
	which accounts the reasons behind these failiures. In this paper, we extend our work in which we consider GRBMs from the perspective of density models, i.e. how well the model learns the dist of the data.
	\item We show an GB-RBM can be regarded as a mixture of Gaussians, which has already been mentioned briefly in previous studies:
	\begin{itemize}
		\item 2009 Bengio: Learning deep architectures for AI \cite{Bengio2009}
		\item 2011 Theis, Gerwinn, Sinz, Bethge: referenced above \cite{Theis2011}
		\item 2011 Courville, Bergstra, Bengio: referenced above \cite{Courville2011}
	\end{itemize}
	but has gone unheeded. This formulation makes clear that GRBMs are quite limited in the way they can represent data. However we argue this fact does not necessarily prevent the model from learning the statistical structure in the data. 
	\item We present successful training of GRBMs both on a two-dimensional blind source separation problem and natural image patches, and that the results are comparable to that of independent component analysis (ICA). 
	\item Based on our analysis we propose several training recipes, which allowed successful and fast training in our experiments. 
	\item Finally, we discuss the relationship between GRBMs and above mentioned modifications of the model.
\end{itemize}


\begin{itemize}
	\item \textbf{Summary from}
	\begin{itemize}
		\item \textbf{2012 Melchoir: Learning natural image statistics with gaussian-binary restricted boltzmann machines (Master's thesis)} \cite{Melchior2012}
\end{itemize}
	\item A popular variant of RBM is GB-RBM
	\begin{itemize}
		\item 2004 Welling, Rosen-Zvi, Hinton: \cite{Welling2005} referenced above
	\end{itemize}
	\item Training difficulties
	\begin{itemize}
		\item 2010 Fischer, Igel: RBMs difficult to train (Markov-random-fields und boltzmann maschinen)
		\item 2012 Wang, Melchior, Wiskott: This even more critical when using GB-RBMs (referenced above) \cite{Wang2012}
	\end{itemize}
	Several modifications proposed to overcome training difficulties
	\begin{itemize}
		\item 2007 Lee, Ekanadham, Ng: Added a sparseness penalty on the gradient that forced the model to prefer sparse representations and seems to help learning meaningful features (referenced above) \cite{Lee2008}
		\item 2011 Cho, Ilin, Raiko: Suggested the training failure is due to the training algo and proposed several improvements to overcome the problem (referenced above) \cite{Cho2011}
		\item 2009 A. Krizhevsky: Successfully trained a deep hierarchical network and concluded that a failure is mainly because of the existence of high-frequency noise in natural images, which prevents the model from learning the important structures. (referenced above) \cite{Krizhevsky2009}
		\item Other approaches modified the model such that it is capable of modelling higher order statistics directly:
		\begin{itemize}
			\item 2011 Courville, Bergstra, Bengio: referenced above \cite{Courville2011}
			\item 2010 Ranzato, Hinton: referenced above \cite{Ranzato2010a}
			\item 2010 Ranzato, Krizhevsky, Hinton: referenced above \cite{Ranzato2010}
		\end{itemize}
		All modifications showed that BG-RBMs are in principle capable of learning features comparable to the receptive fields in the early primary visual cortex V1, but in practice this is difficult to achieve.
		\item To derive a better understanding of the limitations of the model, the authors in
		\begin{itemize}
			\item 2011 Le Roux, Heess, Shotton, Winn: ref above \cite{LeRoux2011}
		\end{itemize} 
		evaluated its capabilities from the perspective of image reconstruction. In
		\begin{itemize}
			\item 2011 Theis, Gerwinn, Sinz, Bethge: ref above \cite{Theis2011}
		\end{itemize} 
		the likelihood of the model is compared to classical machine learning methods. Although the model has been analysed to show the failures empirically, there are few works accounting for the failure analytically.
	\end{itemize}
	\item Other interesting points made:
	\begin{itemize}
		\item 1986 Hinton, Sejnowski: The Boltzmann machine (Learning and relearning in boltzmann machines) \cite{Hinton1986}
		\item 2006 Bishop: The BM is an undirected probabilistic graphical model (Pattern recognition  and Machine Learning, chapter 8) \cite{Bishop2006}
		\item with stochastic continu- ous or discrete units. It is often interpreted as a stochastic recurrent neural network where the state of each unit depends on the units it is connected to. The original BM has a fully connected graph with binary units, which turns into a Hopfield net if we choose deterministic rather than stochastic units. But in contrast to Hopfield nets, a BM is a generative model that allows to generate new samples from the learned distribution.
		\item 2009 Bengio: The BM's stackability allows for constructing deep networks \cite{Bengio2009} (ref above)
		\item 2009 Krizhevsky: BMs popular in the field of feature extraction (ref above) \cite{Krizhevsky2009}
		\item 2006 Hinton, Salakhutdinov: BMs popular in the field of dimensionality reduction (Reducing the dimensionality of data with neural networks. SCIENCE) \cite{Hinton2006}
		\item 2006 Bishop: A BM is a special case of a Markov Random Field (MRF) \cite{Bishop2006} (ref above)
		\item 2002 Hinton: An MRF is itself a special case of a Product of Experts (PoE) (ref above) \cite{Hinton2002}
		\item 2010 Fischer, Igel: A PoE model with exponential experts = an MRF with input variables $\bm{x}$ and latent variables $\bm{h}$ - This is shown by the Hammersley-Clifford Theorem (=\textbf{The fundamental theorem of random fields}, a result that gives necessary and sufficient conditions under which a positive prob dist can be represented as a Markov network/Markov random field (Wikipedia)) (ref above)
		\item While an MRF is a particular case of a PoE, a BM is an MRF with a particular energy function that leads to a complete undirected graph. This implies a fully connected network where the pairwise communication between two units is symmetrical.
		\item 2010 Ranzato, Hinton: Can make even complexer BMs where more than two units interact, named higher order BMs (ref above) \cite{Ranzato2010a}
		\item An important subclass of BMs having a restricted communication structure allows an efficient calculation of the conditional probabilities. So that fast inference is possible, which made restricted BMs become very popular over the last decade.
		\item 1985 Ackley, Hinton, Sejnowski: The original definition of BMs. Here, visible and hidden units had binary values. (A learning algorithm for boltzmann machines) \cite{Ackley1985}
		\item Discussion on options for making the visibles continuous, pros/cons of the options
		\begin{itemize}
			\item 2009 Larochelle, Bengio, Lourdaour, Lamblin: Truncated Exponential RBMs (Exploring strategies for training deep neural networks.)
		\end{itemize}
		\item GB-RBM: \textbf{We assume the visibles to be Gaussian distributed, and therefore, a distribution over} $\mathbb{R}$. Appereantly it's natural to assume continuous variables to be Gaussian distributed. But OK in my case????
		\item 2006 Hinton, Salakhutdinov: The GB-RBM. (ref above - Reducing the dimensionality of data with neural networks) \cite{Hinton2006}
		\item Presents GB-RBM energy function from the general RBM one, ending up with
		\begin{align}
			E^{GB} (\bm{x}, \bm{h}) 
			=& \sum_i^N \frac{(x_i - a_i)^2}{2\sigma_i^2} - \sum_j^M b_j h_j - \sum_{i,j}^{N, M} \frac{x_i w_{ij} h_j}{\sigma_i^2} \\
			=& \sum_i^N ||\frac{\bm{x} - \bm{b}}{2 \bm{\sigma}}||^2 - \bm{b}^T \bm{h} - (\frac{\bm{x}}{\sigma^2})^T \bm{W} \bm{h}
		\end{align}
		where the second equation is given in clearer matrix vector notation and the fraction bar denotes the component wise division.
		\item Notice that there exists a slightly different formulation of the GB-RBM energy
		\begin{itemize}
			\item 2009 Krizhevsky: ref above \cite{Krizhevsky2009}
		\end{itemize}
		where the quadratic term uses $\sigma_i$ instead of $\sigma_i^2$. But as stated in
		\begin{itemize}
			\item 2011 Cho, Ilin, Raiko: ref above \ref{Cho2011}
		\end{itemize}
		this leads to a counter intuitive scaling of the conditional mean by $\sigma_i^2$, so that in this work a GB-RBM is always considered to be defined as above.
		\item Parallel tempering: an algorithm that provides a fast mixing rate and surprisingly, we already know all concepts this algorithm is working with. First of all let us reconsider the PDF of MRFs (19) where we defined the temperature parameter $T \in [1, \infty)$,  which we discarded up to now. It scales the energy down, which leads to a regularization of the PDF’s manifold. This becomes clear if we think of that the energy is applied to an exponential function to calculate the probability. If we choose a big temperature the energy is scaled down, which leads to more equally distributed probabilities, due to nature of the exponential function.
		Therefore, we can use the temperature to generate samples, which are distributed more homogeneously.
		The idea of PT is to run several Markov chains on different temperatures. We start Gibbs sampling from the highest temperature where all samples have the same probability. While continuing the sampling procedure, the temperature is lowered, which has the effect that regions of higher density are coming up. If the decreasing of the temperatures is smooth enough, the samples will move to all regions of higher density. This generates samples that are likely from all modes of the distribution which is illustrated in Figure 12.
	\end{itemize}
	\item From the section Analysis of GB-RBMs
	\begin{itemize}
		\item In general, a profound understanding of a model, its capabilities and limitations, requires a clear understanding of how it models data. For probabilistic models like BMs, accordingly, we need to understand how the marginal probability distribution of the input data is structured.
		\item \textbf{BB-RBM}: Figure 15 shows the marginal probability density $P^{BB} (\bm{x})$ of a BB-RBM with two visible units $x_1$, $x_2$ and two hidden units $h_1$, $h_2$. The two visible units can take the four possible states $\bm{x} \in \{ 0,1 \}^2$, which correspond to the four positions on the plain. The probability for each state, illustrated as cylinders depend on the product of the visible experts, $e_{x1}$, $e_{x2}$. The experts themselves, referring to (39) are sigmoid functions, which depend on the hidden units and the corresponding weights. The steepness of the experts’ sigmoid, controlled by the weights, defines how likely it is to switch from an active to an inactive state and vice versa. Figure 15 also implies that RBMs can be universal approximators
		\begin{itemize}
			\item 2008 Le Roux, Bengio: Representational power of restricted boltzmann machines and deep belief networks. \cite{LeRoux2008} 
		\end{itemize}
		Let $N$ be the number of visible units and $K \leq \{ 0,1 \}^N$ be the total number of states of the PDF we want to learn. We are able to model the distribution exactly if we have one hidden unit per visible state plus a bias unit, hence $M=2^N + 1$ hidden units.
		\item Similar to the illustration for a BB-RBM we are able to illustrate the marginal PDF for a GB-RBM. Referring to (145), the experts marginal PDF has a rather unintuitive form where one expert is an unnormalized Gaussian with mean $\bm{b}$ and the other $M$ experts are the sum of the value of one and an exponential function. But we are able to derive a more intuitive formulation of the marginal PDF using the Bayes’theorem and the polynomial expansion as proposed in
		\begin{itemize}
			\item 2012 Wang, Melchior, Wiskott: ref above \cite{Wang2012}
		\end{itemize}
		so we get
		\begin{align}
			P(\bm{x}) &= \sum_{\bm{h}} P(\bm{x}|\bm{h}) P(\bm{h}) \\
			&= \mathcal{N} (\bm{x}; \bm{b} + \bm{w}_{})
		\end{align}
	\end{itemize}
\end{itemize}
Melchior's rewriting of $P(x)$ as a Mixture of Gaussians written out for $M=2$ and $N=2$:
\begin{align}
	P(\bm{x}) =& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \sum_{j=1}^{N=2} \eta_j \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_j, \bm{\sigma}^2) \nonumber \\
	&+ \sum_{j=1}^{N-1=1} \sum_{k>j}^{N=2} \eta_{jk} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_j + \bm{w}_k, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_2, \bm{\sigma}^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_1 + \bm{w}_2, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(x_1|b_1, \sigma_1^2)\mathcal{N}(x_2|b_2, \sigma_2^2) \nonumber \\
	&+ \eta_1 \mathcal{N} (x_1| b_1 + w_{11}, \sigma_1^2)\mathcal{N} (x_2| b_2 + w_{21}, \sigma_2^2) \nonumber \\
	&+ \eta_2 \mathcal{N} (x_1| b_1 + w_{12}, \sigma_1^2) \mathcal{N} (x_2| b_2 + w_{22}, \sigma_2^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (x_1|b_1+ w_{11} + w_{12}, \sigma_1^2) \mathcal{N} (x_2|b_2+ w_{21} + w_{22}, \sigma_2^2) \\
\end{align}
where
\begin{align}
	\eta_0 =& \frac{(\sqrt{2\pi \sigma_i^2})^M}{Z} =  \frac{2\pi \sigma_i^2}{Z} \\
	\eta_j =& \eta_0 e^{\frac{||\bm{b} + \bm{w}_j||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j} \\
	\eta_{jk} =& \eta_0 e^{\frac{||\bm{b} + \bm{w}_j + \bm{w}_k||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j + c_k}
\end{align}
giving us
\begin{align}
	P(\bm{x}) =& \eta_0 \mathcal{N}(x_1|b_1, \sigma_1^2)\mathcal{N}(x_2|b_2, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_1||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1}
	\mathcal{N} (x_1| b_1 + w_{11}, \sigma_1^2)\mathcal{N} (x_2| b_2 + w_{21}, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_2}
	 \mathcal{N} (x_1| b_1 + w_{12}, \sigma_1^2) \mathcal{N} (x_2| b_2 + w_{22}, \sigma_2^2) \nonumber \\
	&+ \eta_0 e^{\frac{||\bm{b} + \bm{w}_1 + \bm{w}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1 + c_2}
	 \mathcal{N} (x_1|b_1+ w_{11} + w_{12}, \sigma_1^2) \mathcal{N} (x_2|b_2+ w_{21} + w_{22}, \sigma_2^2)  \\
\end{align}
Or write
\begin{align}
	\bm{\mu}_0 =& \bm{b} \\
	\bm{\mu}_j =& \bm{b} + \bm{w}_j \\
	\bm{\mu}_{jk} =& \bm{b} + \bm{w}_j + \bm{w}_k \\
	\Rightarrow 
	\eta_0 =& \frac{(\sqrt{2\pi \sigma_i^2})^M}{Z} =  \frac{2\pi \sigma_i^2}{Z} \\
	\eta_j =& \eta_0 e^{\frac{||\bm{\mu}_j||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j} \\
	\eta_{jk} =& \eta_0 e^{\frac{||\bm{\mu}_{jk}||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_j + c_k}
\end{align}
then
\begin{align}
	P(\bm{x})=& \eta_0 \mathcal{N}(\bm{x}|\bm{b}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{b} + \bm{w}_2, \bm{\sigma}^2) \nonumber \\
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{b}+\bm{w}_1 + \bm{w}_2, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{\mu}, \bm{\sigma}^2)
	+ \eta_1 \mathcal{N} (\bm{x}| \bm{\mu}_1, \bm{\sigma}^2)
	+ \eta_2 \mathcal{N} (\bm{x}| \bm{\mu}_2, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_{12} \mathcal{N} (\bm{x}|\bm{\mu}_{12}, \bm{\sigma}^2) \\
	=& \eta_0 \mathcal{N}(\bm{x}|\bm{\mu}_0, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_1||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1} 
	\mathcal{N} (\bm{x}| \bm{\mu}_1, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_2||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_2} 
	\mathcal{N} (\bm{x}| \bm{\mu}_2, \bm{\sigma}^2) \nonumber \\ 
	&+ \eta_0 e^{\frac{||\bm{\mu}_{12}||^2 - ||\bm{b}||^2}{2\bm{\sigma}^2} + c_1 + c_2}
	\mathcal{N} (\bm{x}|\bm{\mu}_{12}, \bm{\sigma}^2) \\
	\text{using that } 
	\mathcal{N}(\bm{x}| \bm{\mu}, \bm{\Sigma}) 
	=& \frac{1}{\sqrt{(2\pi)^M |\Sigma|}} e^{-\frac{1}{2} (\bm{x}-\bm{\mu})^T\Sigma^{-1} (\bm{x}-\bm{\mu})} %\nonumber \\
	= \frac{1}{\sqrt{(2\pi\sigma_i^2)^M}} e^{-\frac{||\bm{x}-\bm{\mu}||^2}{2\bm{\sigma}^2}} \nonumber \\
	=& \frac{1}{Z} e^{-\frac{||\bm{x}-\bm{\mu}_0||^2}{2\bm{\sigma}^2}} \nonumber \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_1||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_1||^2}{2\bm{\sigma}^2} + c_1} \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_2||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_2||^2}{2\bm{\sigma}^2} + c_2}  \\ 
	&+ \frac{1}{Z} e^{\frac{||\bm{\mu}_{12}||^2 - ||\bm{b}||^2 - ||\bm{x}-\bm{\mu}_{12}||^2}{2\bm{\sigma}^2} + c_1 + c_2} \\
\end{align}
... and if we continue as above I suspect we'll end up with the "origininal non-MoG" form.
\subsection{More on RBMs and GB-RBMs}

\begin{itemize}
	\item \textbf{From}
	\begin{itemize}
		\item \textbf{2010 Nair, Hinton: Rectified linear units improve restricted boltzmann machines} \cite{Nair2010}
	\end{itemize}
	\item RBMs have been used as generative models of many different types of data including
	\begin{itemize}
		\item 2010 Mohamed, Hinton: Sequences of mel-cepstral coefficients that represent speech (Phone recognition using restricted boltzmann machines)
		\item 2009 Hinton, Salakhutdinov: Bags of words that represent documents (Replicated softmax: an undirected topic model)
		\item 2007 Salakhutdinov, Mnih, Hinton: User ratings of movies (Re- stricted Boltzmann machines for collaborative filtering)
		\item 2006 Taylor, Hinton, Roweis: In their conditional form they can be used to model high-dimensional temporal sequences such as video or motion capture data. (Modeling hu- man motion using binary latent variables)
		\item 2006 Hinton, Osindero, Teh: Their most important use is as learning modules that are composed to form deep belief nets (A fast learning algorithm for deep belief nets)
	\end{itemize}
	\item More
	\begin{itemize}
		\item 2002 Hinton: RBMs originally developed using binary stochastic units for both visible and hidden layers (ref above - Training product of experts by minimizing contrastive divergence) \cite{Hinton2002}
		\item 2006 Hinton, Salakhutdinov: \textbf{To deal with real-valued data such as the pixel intensities in natural images, they replaced the binary visible units by linear units with dependent Gaussian noise}. (ref above - Reducing the dimensionality of data with neural networks) \cite{Hinton2006}
		\item 1994 Freund, Haussler: \textbf{This was first suggested here} (Unsupervised learning of distributions on binary vectors using two layer networks) \cite{Freund1992}
	\end{itemize}
	\item \textbf{OBS, std only squared in one term in this GB-RBM? (the visible bias term and not the weight one. Also only visible bias one divided by 2)}
	\item It is possible to learn the variance of the noise for each visible unit but this is difficult using binary hidden units. In many applications, it is much easier to first normalise each component of the data to have zero mean and unit variance and then to use noise-free re- constructions, with the variance in equation 6 set to 1. The reconstructed value of a Gaussian visible unit is then equal to its top-down input from the binary hid- den units plus its bias. We use this type of noise-free visible unit for the models of object and face images described later.
	\item Thorough discussion of ReLUs and modification: It is possible, however, to use a fast approximation in which the sampled value of the rectified linear unit is not constrained to be an integer. Instead it is given by $max(0, x+N(0, \sigma(x)))$ where $N(0, V)$ =Gaussian noise w/zero mean and variance V. We call a unit that uses this approximation a Noisy Rectified Linear Unit (NReLU) and \textbf{this paper shows that NReLUs work better than binary hidden units for several different tasks}. We also give an approximate probabilistic interpretation for the $max(0,x)$ nonlinearity, further justifying their use.
	\item We have shown that NReLUs work well for discrimina- tion, \textbf{but they are also an interesting way of modeling the density of real-valued, high-dimensional data}.
	\begin{itemize}
		\item A standard way to do this is to use a mixture of diag- onal Gaussians. Alternatively we can use a mixture of factor analysers. Both of these models are expo- nentially inefficient if the data contains componential structure. Consider, for example, images of pairs of independent digits. If a mixture model for single digit images needs $N$ components, a single mixture model of pairs of digits needs $N^2$ components. Fortunately, this exponential growth in the number of components in the mixture can be achieved with only linear growth in the number of latent variables and quadratic growth in the number of parameters if we use rectified linear hidden units.
		\item Consider using rectified linear units with zero bias to model data that lies on the surface of a unit hyper- sphere. Each rectified linear unit corresponds to a plane through the centre of the hypersphere. It has an activity of 0 for one half of the hypersphere and for the other half its activity increases linearly with distance from that plane. $N$ units can create $2^N$ re- gions on the surface of the hypersphere (assuming the hypersphere is at least $N$-dimensional). As we move around within each of these regions the subset of units that are non-zero does not change so we have a lin- ear model, but it is a different linear model in every region. The mixing proportions of the exponentially many linear models are defined implicitly by the same parameters as are used to define $p(\bm{v}|\bm{h})$ and, unlike a directed model, the mixing proportions are hard to compute explicitly 
		\begin{itemize}
			\item 2008 Nair, Hinton: Implicit mixtures of restricted boltzmann machine
		\end{itemize}
		\item This is a much better way of implementing an exponentially large mixture of linear models with shared latent variables than the method described in
		\begin{itemize}
			\item 1999 Hinton, Sallanes, Ghahramani: A hierarchical community of experts
		\end{itemize}
		 which uses directed linear models as the components of the mixture and a separate sig- moid belief net to decide which hidden units should be part of the current linear model. In that model, it is hard to infer the values of the binary latent variables and there can be jumps in density at the boundary be- tween two linear regions. A big advantage of switch- ing between linear models at the point where a hidden unit receives an input of exactly zero is that it avoids discontinuities in the modeled probability density.
	\end{itemize}
\end{itemize}

\chapter{The RBM method for the quantum problem}
\subsection{How is $\Psi$ modeled}
\subsection{Further analysis of the GB-RBM - the marginal prob as a MoG}
\subsection{Training - now using the variational principle}
\subsection{Hyperparameters}
\subsubsection{Initialization}
\subsubsection{Sampling parameters (Metropolis)}
\subsubsection{Number of hidden nodes}
\subsubsection{Number of samples}
\subsubsection{Learning rate}
\subsection{Computational efficiency}
Choosing a constant density $\alpha$ of hidden variables pr physical positions, i.e. $M=\alpha N$, the number of variational parameters scales as $\alpha N^2$.

\part{Implementation}
\section{Workflow}
\section{How to use..?}
\subsection{Monitoring the learning progress?}

\section{Structure/any particular reason for choices..?}
Conditionals vs. polymorhisms. Polymorphism and encapsulation.

Other QMC software packages (as pr 2009 - check updates maybe?):
Some of the QMC methods are implemented in several existing codes and packages that are available for use by communities at large. We will mention the packages which are most familiar to us: CASINO [86], QMCPACK [87], QWALK [88], and CHAMP [89]. QMCPACK and QWALK are distributed under Open Source licenses.
Although not a software package, we mention here the novel approach for meeting the high computer demands of DMC by Korth et al. [90]. It is a distributed computational system that uses the power of  38000 PCs located all over the world. The system yields  15 teraflops of sustained computing power.
\cite{Lester2009}

\part{Results and analysis}
\chapter{Making the basic method stripped for extra stuff work - grid search and validation}
Jeg tenker vi tar utgangspkt i Metropolis importance sampling, den virker best, i den mest omfattende utredelsen av hyperparameter search. Så kan Gibbs og metropolis brute force være "artig å sammenligne med". Hele veien i testingen har vi mulighet til å teste med eller uten interaksjon. Men siden uten har veldig god presisjon, og med er den store utfordringen, så kan kanskje hyperparameter søket gjøres MED. Så er det bare å holde track på at ikke uten har blitt dårligere liksom.
Starter også med sigma=1
Enkleste gradient descent
\section{Hyperparameter grid search}
\begin{enumerate}
	\item The first search:
	\begin{itemize}
		\item What we keep constant:
		\begin{itemize}
			\item Sampling method: Metropolis importance sampling
			\item Hamiltonian: With interaction (still just observing that without is very precise and our choices don't "ruin" that)
			\item The $\sigma_{RBM}=1$
			\item The $\omega_{HO}=1$
			\item Optimization method: Simplest gradient descent
		\end{itemize}
		\item What we will change (either grid search or with the random method thing Treider talked about. If too many we can set one or more before/after the others):
		\begin{itemize}
			\item The Metropolis importance sampling step size $\Delta t$ (default pr now 0.45)
			\item The learning rate $\eta$ (default pr now 0.01)
			\item The number of samples $nSamples$ (default pr now $10^4$ or $10^5$)
			\item Number of hidden units (starting defualt is $N$=2)
			\item Optimization stopping criteria
		\end{itemize}
		\item When all working, test a variation of gradient descent?
	\end{itemize}
\end{enumerate}

\section{Validation and limitations given the previously found optimal parameters}
\subsection{Compared to analytical solution}
-With/without interaction
-With different numbers of particles and dimensions
-Use the blocking values to take into account the error and/or standard deviation
-plot the density thing?
-show the learning evolution in video or similar?
\subsection{Compared to the traditional methods benchmarks}
\subsubsection{Precision}
\subsubsection{Performance/computational speed/convergence rate}
\subsubsection{"Work" required by user - setting hyperparameters. And to what extent do they, and potential wrong values for them, give instability in the method}

\section{Analysis}
\subsection{What can we extract from the weighting of the hidden features. Are we actually achieving dimensionality reduction or feature extraction here? Is the model telling us anything interesting?}
\subsection{Important question: do we succeed at sampling the whole config space and consequently train on the whole config space? Relevant to both VMC and RBM I guess.}
\subsection{How does it scale with 1) particles/dimensions 2) hidden nodes}
polynomial cost, polynomial time, etc

\section{More stuff}
\subsection{Training the RBM sigma}
\subsection{Optimize performance/parallelization}
\subsection{More in depth gradient descent variations investigation?}
\subsection{Try other types of hidden nodes (ReLU)}
\subsection{Add one more hidden layer - Deep BM}
\subsection{Collaboration with Alocias}
\subsection{Implementing symmetries of the Hamiltoninan}
\subsection{Try training on sampled data from analytical, for comparison? Then also test open source package?}
\subsection{Other Hamiltonians?}

\part{Conclusions and outlook}

\begin{appendices}
\chapter{Random generators and distributions in C++}
\end{appendices}

\newpage
\bibliography{master}

\end{document}