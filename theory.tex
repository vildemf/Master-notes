\documentclass[norsk,a4paper,11pt]{article}
\usepackage[T1]{fontenc} %for å bruke æøå
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx} %for å inkludere grafikk
\usepackage{verbatim} %for å inkludere filer med tegn LaTeX ikke liker
\usepackage[font=scriptsize]{caption}
\usepackage{subcaption}
\usepackage{mdwlist}
\usepackage{bm}
\usepackage{enumerate}
\usepackage[usenames, dvipsnames]{color}
%\usepackage{hyperref}
\usepackage{url}
% By default the URLs are put in typewriter type in the body and the
% bibliography of the document when using the \url command.  If you are
% using many long URLs you may want to uncommennt the next line so they
% are typeset a little smaller.
% \renewcommand{\UrlFont}{\small\tt}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\def\mbf#1{\mathbf{#1}}
\newcommand{\VV}{\mathbf{V}}
\newcommand{\Vv}{\mathbf{v}}
\newcommand{\VX}{\mathbf{X}}
\newcommand{\Vx}{\mathbf{x}}
\newcommand{\VH}{\mathbf{H}}
\newcommand{\Vh}{\mathbf{h}}
\newcommand{\VW}{\mathbf{W}}
\newcommand{\Vwi}{\mathbf{w}_{i*}}
\newcommand{\Vwj}{\mathbf{w}_{*j}}
\newcommand{\Va}{\mathbf{a}}
\newcommand{\Vb}{\mathbf{b}}

\bibliographystyle{plain}


\title{Machine learning theory}
\author{Vilde Flusgrud}

\begin{document}
\date{\today}
\maketitle

\section{Introduction}
	\begin{itemize}
		\item Estimation (try to determine a model(?) parameter) vs. prediction (try to predict the value of a random variable)
		\item The review is organized as follows. We begin by introducing polynomial regression as a simple example that highlights many of the core ideas of ML. The next few chapters introduce the language and major concepts needed to make these ideas more precise including tools from statistical learning theory such as overfitting, the bias-variance tradeoff, regularization, and the basics of Bayesian inference. The next chapter builds on these examples to discuss stochastic gradient descent and its generalizations. We then apply these concepts to linear and logistic regression, followed by a detour to discuss how we can combine multiple statistical techniques to improve supervised learning, introducing bagging, boost- ing, random forests, and XG Boost. These ideas, though fairly technical, lie at the root of many of the advances in ML over the last decade. The review continues with a thorough discussion of supervised deep learning and neural networks, as well as convolutional nets. We then turn our focus to unsupervised learning. We start with data visualization and dimensionality reduction before proceeding to a detailed treatment of clustering. Our discussion of clustering naturally leads to an examina- tion of variational methods and their close relationship with mean-field theory. The review continues with a discussion of deep unsupervised learning, focusing on energy-based models, such as Restricted Boltzmann Ma- chines (RBMs) and Deep Boltzmann Machines (DBMs). Then we discuss two new and extremely popular model- ing frameworks for unsupervised learning, generative ad- versarial networks (GANs) and variational autoencoders (VAEs). We conclude the review with an outlook and discussion of promising research directions at the inter- section physics and ML.
	\end{itemize}
	
\section{Why is machine learning difficult?}
	\begin{itemize}
		\item Ingredients:
		\begin{itemize}
			\item $\mathbf{X}$ = the dataset
			\item $g(\mathbf{w})$ = the model = a function of the parameters $\mathbf{w}$
			\item $\mathcal{C}(\mathbf{X}, g(\mathbf{w}))$ = the cost function, allows us to judge how well the model performs on the observations.
			\item $\mathbf{X}_{train}$ = 90\% of $\mathbf{X}$
			\item $\mathbf{X}_{test}$ = 10\% of $\mathbf{X}$
			\item $\hat{\mathbf{w}} = argmin_{w}\{ \mathcal{C}(\mathbf{X}_{train}, g(\mathbf{w})) \}$ (the model is fit by minimizing the cost function)
			\item $E_{in} = \mathcal{C}(\mathbf{X}_{train}, g(\hat{\mathbf{w}}))$ (the in-sample error)
			\item $E_{out} = \mathcal{C}(\mathbf{X}_{test}, g(\hat{\mathbf{w}}))$ (The out-of-sample error. Used to evaluate the performance of the model.)
			\item We lmost always find that $E_{in} \geq E_{out}$
		\end{itemize}
		\item It may be at first surprising that the model that has the lowest out-of-sample error $E_{out}$ usually \textit{does not} have the lowest in-sample error $E_{in}$. At first glance, the observation that the model providing the best explenation for the current dataset probably will not provide the best explanation for future datasets is very counter-intuitive.

		Moreover, the discrepancy between $E_{in}$ and $E_{out}$ becomes more and more important, as the complexity of our data, and the models we use to make predictions, grows. As the number of parameters in the model increases, we are forced to work in high-dimensional spaces. The “curse of dimensionality” ensures that many phenomena that are absent or rare in low-dimensional spaces become generic. For example, the nature of distance changes in high dimensions, as evidenced in the derivation of the Maxwell distribution in statistical physics where the fact that all the volume of a $d$-dimensional sphere of radius $r$ is contained in a small spherical shell around $r$ is exploited. Almost all critical points of a function (i.e., the points where all derivatives vanish) are saddles rather than maxima or minima (an observation first made in physics in the context of the $p$-spin spherical spin glass). For all these reasons, it turns out that for complicated models studied in ML, predicting and fitting are very different things.
		\item Polynomial regression
		\begin{itemize}
			\item Probalisitc process that assigns a label $y_i$ to an observation $x_i$. Data generated by drawing samples from the equation $y_i = f(x_i) + \eta_i$
			\item $f(x_i)$ = some fixed (but possibly unkown) function. "Function used to generate the data".
			\item $\eta_i$ = a Gaussian, uncorrelated noise variable, such that
			\begin{itemize}
				\item $\langle \eta_i \rangle = 0$
				\item $\langle \eta_i \eta_j \rangle = \delta_{ij} \sigma^2$
			\end{itemize}
			\item $\sigma $ = the noise strength. ($\sigma = 0$ = noiseless case.)
			\item $g_\alpha (x; \mathbf{w}_\alpha)$ = a family of functions which depend on some parameters $\mathbf{w}_\alpha$. These functions represent the \textit{model class} that we are using to model the data and make predictions. We chose the model class without knowing $f(x)$. The model class encode the \textit{features} we choose to represent the data. For polynomial regression we will consider three different model classes:
			\begin{enumerate}[i]
				\item $g_1 (x; \mathbf{w}_1)$ = al polynomials of order 1 (two parameters)
				\item $g_3 (x; \mathbf{w}_3)$ = al polynomials up to order 3 (four parameters)
				\item $g_{10} (x; \mathbf{w}_{10})$ = al polynomials of order 10 (eleven parameters)
			\end{enumerate}
			\item The different number of parameters reflects that the three models have different \textit{model complexities}. Thinking of each term in the polynomial as a "feature", increasing the polynomial order increases the number of features.
			\item Fit the models on the generated training samples using standard least-squares regression.
			\item Observe that at small sample sizes, noise can create fluctuations in the data that look like genuine patterns. While simple models are forced to ignore them and focus on the larger trends, complex models can capture both the global trends and noise-generated patterns at the same time. The model can then be tricked into thinking the noise encodes real information = "\textbf{overfitting}" = a steep drop-off in predictive performance.
			Can guard against overfitting in two ways:
			\begin{itemize}
				\item Use less expressive models with fewer parameters
				\item Collect more data so the likelihood that the noise appears patterned decreases. 
			\end{itemize}
			This relates to the \textbf{bias-variance} tradeoff: when training data limited, one can often get better better predicitve performance by using less expressive model rather than a complex one. The simpler model has more "bias" but is less dependent on the particular realization of the training data., i. e. less "variance". Some universal lessons:
			\begin{itemize}
				\item Fitting is not predicting. Fitting existing data well is fundamentally different from from making predictions about new data. (fitting = estimation? or not?)
				\item Using a complex model can result in overfitting. Increasing a model's complexity will usually yield better results on the training data. However when the training data size is small and the data are noisy, this results in \textit{overfitting} and can substantially degrade the predictive performance of the model.
				\item For complex datasets and small training sets, simple models can be better at prediction than complex models due to the bias-variance tradeoff. It takes less data to train a simple model than a complex one. Therefore, even though the correct model is guaranteed to have better predictive performance for an infinite amount of training data (less bias), the training errors stemming from finite-size sampling (variance) can cause simpler models to out-perform the more complex model when sampling is limited. 
				\item It is difficult to generalize beyond the situations encountered in the training data.
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
\section{Basics of statistical learning theory}
	\begin{itemize}
		\item Goal: the sense in which learning is possible, with focus on supervised learning. Ingredients:
		\begin{itemize}
			\item $y = f(x)$ = an unknown function
			\item $\mathcal{H}$ = a hypothesis set that we fix, consisting of all functions we are willing to consider, defined also on the domain of $f$. The set may be uncountably infinite (e.g. if there are real-valued parameters to fit). Our choices here depends usually on our intuition about the problem.
			\item $(x_i, y_i), \quad i=1...N$ = a set of pairs produced by $f(x)$ which serve as observable data. 
			\item Our goal: find a function $h \in \mathcal{H}$ approximating $f(x)$ as best as possible, $h \approx f$ in some strict mathematical sense specificed below. Then say we \textit{learned} $f(x)$.
			\item If $f(x)$ can in principle take any value on \textit{unobserved} inputs, how is it possible to learn in any meaningful sense? Learning is possible in the restricted sense that the fitted model will probably perform approximately as well on new data as it did on training data.
			\item $E$ = appropriately chosen error function (e.g. sum of squared errors in linear regression)
			\item When we are training we only have access to $E_{in}$ (fitting). Our goal is to minimize $E_{out}$ (predicting) - the performance on new data.
			\item Can we say something about the relationship between $E_{in}$ and $E_{out}$? Yes: it's the domain of statistical learning theory.
		\end{itemize}
		\item Three schematics
		\begin{itemize}
			\item Figure 4: Shows $E_{in}$ and $E_{out}$ as functions of the amount of training data. Assumes large data amount and that the model cannot exactly fit the true function $f(x)$. In the infinite data limit, the two errors must approach the same value, which is our model's \textbf{bias} = the best our model could do given infinite training data to beat down sampling noise = a property of the kind of functions/model class we use to approximate $f(x)$. In general: more complex model class = smaller bias. But, do not have infinite data. Thus, better minimize $E_{out}$ than the bias. Can decompose $E_{out}$ into
			\begin{itemize}
				\item The bias
				\item The variance = measures the typical errors introduced in training our model due to sampling noise fraom having a finite training set.
			\end{itemize}
			Final quantity shown is the difference between $E_{out}$ (generalization) and $E_{in}$ (training). Measures difference between fitting and predicting. Models with large difference \textbf{overfit} the data. Statistical learning lesson: not enough to minimize $E_{in}$ since $E_{out}$ may still be large. This insight leads to the idea of \textbf{regularization}.
			\item Figure 5: Shows $E_{out}$ as a function of model complexity = number of parameters/features fex, but not always = model complexity is a subtle idea, defining it precisely is one of the great achievements of statistical learning theory = roughly speaking, it is a measure of the complexity of the model class used to approximate $f(x)$. Considering a training data set of fixed size, $E_{out}$ will be a non-monotonic function of the model complexity and generally minimized for models of \textit{intermediate} complexity. Because: while using a more complex model always reduces the bias, at some point the model becomes too complex for the amount of training data and the generalization error becomes large due to high variance = may be more suitable to use a more biased model with small variance than a less-biased model with large variance = the \textbf{bias-variance tradeoff}.
			\item Figure 6: Another way to visualize the bias-variance tradeoff. Shows how a complex (high variance, low-bias) model vs a simpler (low variance, high bias) model lands compared to the true model.
			\begin{itemize}
				\item The complex exhibits larger fluctuations while its average will be closer to the true model. The simpler fluctuates less, but is on avarage further from the true model. (This reminds one of the accuracy (achieved by the complex model) and precision (achieved by the simpler model) discussion in experimental physics).
				\item In general, more complex model needs more training data. This is the cause of the larger fluctuations for the complex model. However, when increasing the data the more complex eventually performs better. Thus, the choice of complexity depends on the amount of training data.
			\end{itemize}
		\end{itemize}
		\item Bias-Variance Decomposition: dig further into the central principle of the bias-variance tradeoff. Expressiveness vs sensitivity to training data fluctuations. Oftentimes in physics, we are mostly concerned with expressivity, \textit{e.g. whether the true ground state wave function can be well approximated by a class of variational wavefunctions such as a matrix product state}. In the learning context, there is the additional challenge of finding the best variational state with finite sampling. We will see that while this concept is a generally useful heuristic (technique) to keep in mind, it is a mathematically precise statement when decomposing the squared error.(what is meant by this entire sentence?) Finally, we note that a better term would be the bias-variance \textit{decomposition}, as it is possible to have high bias \textit{and} high variance.
		We'll discuss the b-v tradeoff in the continuous predictions such as regression, but many of the intuitions here also carry over to classification tasks.
		\begin{itemize}
			\item $\mathcal{L}$ = a dataset
			\item $\mathbf{X}_\mathcal{L} = \{ (y_j, \mathbf{x}_j), \quad j=1...N \}$ = the data that makes up $\mathcal{L}$
			\item $y = f(\mathbf{x}) + \epsilon$ = a noisy model from which we assume the true data is generated
			\item $\epsilon$ = normally distributed with mean zero and standard deviation $\sigma_\epsilon$
			\item $\hat{g}_\mathcal{L}(\mathbf{x})$ = a predictor that we assume we have a statistical procedure (e.g. least-squares regression) for forming. The predictor gives the prediction of our model for a new data point $\mathbf{x}$.
			\item $\mathcal{C}(\mathbf{X}, \hat{g}(\mathbf{x})) = \sum_i (y_i - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2 $ = the cost function, which we have taken to be the squared error. We choose the estimator previously mentioned estimator by minimizing this.
			\item $\{ \mathcal{L}_j \}$ = many differen data sets, not just the particular training dataset $\mathcal{L}$ that we have in hand.
			\item $E_\mathcal{L}$ = the expectation value of the cost function over $\{ \mathcal{L}_j \}$ = the generalization error on all data drawn from the true model
		\end{itemize}
		Thus can view $\hat{g}_\mathcal{L}$ as a stochastic functional that depends on the dataset $\mathcal{L}$ and can think of $E_\mathcal{L}$ as the expected value of the functional if we drew an infinite number of datasets $\{ \mathcal{L}_1, \mathcal{L}_2,... \}$.
		\begin{itemize}
			\item $E_\epsilon$ = the expectation value over $\epsilon$, as we would also like to average over different instances over this "noise". 
		\end{itemize} 
		Can thus decompose the expected generalization error in the following way, where in line three we use that $E[X + Y] = E[X] + E[Y]$, in line five use that $Var[X] = E[X^2] - E^2[X]$ and in line six use that our noise has zero mean ($E[\epsilon] = 0$).
	\end{itemize}
	{\tiny
	\begin{align}
		E_{\mathcal{L}, \epsilon} [ \mathcal{C} (\mathbf{X}, \hat{g}(\mathbf{x})) ] 
		=& E_{\mathcal{L}, \epsilon} [\sum_i (y_i - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2] \\ 
		=& E_{\mathcal{L}, \epsilon} [\sum_i (y_i -f(\mathbf{x}_i) + f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2] \\
		=& \sum_i E_\epsilon [(y_i - f(\mathbf{x}_i))^2] + E_\mathcal{L} [(f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i) )^2] \nonumber \\ 
		& + 2E_\epsilon[y_i - f(\mathbf{x}_i)] E_\mathcal{L}[f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i)] \\
		=& \sum_i E_\epsilon [\epsilon^2] + E_\mathcal{L} [(f(\mathbf{x}_i) +2E_\epsilon[\epsilon] E_\mathcal{L}[f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i)] \\
		=& \sum_i \sigma_\epsilon^2 + E_\epsilon^2 [\epsilon] + E_\mathcal{L} [(f(\mathbf{x}_i) +2E_\epsilon[\epsilon] E_\mathcal{L}[f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i)] \\
		=& \sum_i \sigma_\epsilon^2 + E_\mathcal{L} [(f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2]
	\end{align}} 
	We further decompose the second term as
	{\tiny
	\begin{align}
		E_\mathcal{L} [(f(\mathbf{x}_i) - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2]
		=& E_\mathcal{L} [(f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)] + E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)] - \hat{g}_\mathcal{L}(\mathbf{x}_i))^2] \\
		=& E_\mathcal{L} [(f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] + E_\mathcal{L}[(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] \nonumber \\
		&+ 2E_\mathcal{L} [(f(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])] \\
		=& (f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2 + E_\mathcal{L}[(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] \nonumber \\
		&+ 2(f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2 E_\mathcal{L} [\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)]] \\
		=& (f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2 + E_\mathcal{L}[(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] \nonumber \\
		&+ 2(f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2 (E_\mathcal{L} [\hat{g}_\mathcal{L}(\mathbf{x}_i)] - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)]) \\
		=& (f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2
		+ E_\mathcal{L}[(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] 
	\end{align}}
	where we used that $(f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2$ is a constant ($f(\mathbf{x}_i)$ is deterministic, which means $E[f]=f$ and the expected value of an expected value is just that, $E[E[\hat{g}_\mathcal{L}]] = E[\hat{g}_\mathcal{L}]$) and that $E[constant] = constant$.
	Now we have that the two terms we are left with are called
	\begin{align}
		Bias^2 =& \sum_i (f(\mathbf{x}_i) -E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2 \\
		Var =& \sum_i E_\mathcal{L}[(\hat{g}_\mathcal{L}(\mathbf{x}_i) - E_\mathcal{L}[\hat{g}_\mathcal{L}(\mathbf{x}_i)])^2] 
	\end{align}
	The bias measures the deviation of the expected value of our estimator (i. e. the asymptotic value of our estimator in the infinite data limit) from the true value. The variance measures how much our estimator fluctuates due to finite-sample effects. Combining we see that the expected out-of-sample error of our model can be decomposed as
	\begin{align}
		E_{out} = E_{\mathcal{L}, \epsilon} [\mathcal{C}(\mathbf{X}, \hat{g}(\mathbf{x}))] = Bias^2 + Var + Noise
	\end{align}

\section{Gradient descent and its generalizations}
	\begin{itemize}
		\item Ingerdients shared by almost every problem in ML and data science:
		\begin{itemize}
			\item $\mathbf{X}$ = a dataset
			\item $g(\bm{\theta})$ = a model = a function of the parameters $\bm{\theta}$
			\item $\mathcal{C}(\mathbf{X}, g(\bm{\theta}))$ = a cost function allowing us to judge how well the model explains the observations. We fit the model by finding the values of $\bm{\theta}$ that minimize the cost function.
		\end{itemize}
		Here we discuss one of the most powerful and widely used classes of methods for performing this minimization - gradient descent and its generalizations. Main idea: iteratively adjust the parameters in the direction where the gradient of the cost function is large and negative. In this way the training procedure ensures the parameters flow towards a \textit{local} minimum of the cost function.
		\item Underlying reason training a LM algo is hard is the cost functions we wish to optimize are usually complicated, rugged, non-convex functions in a high-dimensional space with many local minima.
		\item Gradient descent (GD) and Newton's method:
		\begin{itemize}
			\item $E(\bm{\theta})$ = function we wish to minimize. "Energy function". In ML context $E(\bm{\theta}) = \mathcal{C}(\mathbf{X}, g(\bm{\theta}))$.
			\item Almost always: $E(\bm{\theta}) = \sum_{i=1}^n e_i(\mathbf{x}_i, \bm{\theta})$ = a sum over $n$ data points. Fex: Linear regression: $e_i$= the mean square error for data point $i$. Logistic regression: $e_i$ = the cross-entropy. Call $e_i$ the energy function to make analogy to physical systems.
		\end{itemize}
		\item Simplest GD: start with an initial value $\bm{\theta}_0$, then update according to
		\begin{align}
			\mathbf{v}_t &= \eta_t \nabla_\theta E(\bm{\theta}_t) \\
			\bm{\theta}_{t+1} &= \bm{\theta}_t - \mathbf{v}_t		
		\end{align}
		where we have introduced the \textit{learning rate}, $\eta_t$, that controls how big a step we should take in the direction of the gradient at time $t$. For sufficiently small $\eta_t$ this method will converge to a $local minimum$ of the cost func. But a small $\eta_t$ comes at a computational cost. If it's smaller we need more steps to reach the minimum. But if it's too large we can overshoot the minimum and the algo becomes unstable (either oscillates or even moves away from the minimum). See fig 7. In practice, one usually specifies a "schedule" that decreases $\eta_t$ at long times. Common schedules include power law and exponential decays in time.
		\item Contrast with Newton's method to better understand this behavior of GD. In Newton's method, we choose the step $\mathbf{v}$ for the parameters in such a way as to minimize a second-order Taylor expansion to the energy function
		\begin{align}
			E(\bm{\theta} + \mathbf{v}) \approx E(\bm{\theta}) + \nabla_\theta E(\bm{\theta})\mathbf{v} + \frac{1}{2} \mathbf{v}^T H(\theta) \mathbf{v}
		\end{align}
		where $H(\theta)$ = the Hessian matrix of second derivatives. Differentiating this equation wrt $\mathbf{v}$ and noting that for the optimal value $\mathbf{v}_{opt}$ we expect $\nabla_\theta E(\bm{\theta} + \mathbf{v}_{opt}) = 0$, yields the equation
		\begin{align}
			0 = \nabla_\theta E(\bm{\theta}) + H(\theta) \mathbf{v}_{opt}
		\end{align}
		Rearranging results in the desired update rules for Newton's method
		\begin{align}
			\mathbf{v}_t &= H^{-1} (\theta_t) \nabla_\theta E(\theta_t) \\
			\bm{\theta}_{t+1} &= \bm{\theta}_t - \mathbf{v}_t
		\end{align}
		Have no guarantee the Hessian is well conditioned, so often replaces the Hessian inverse $H^{-1}(\theta_t)$ by some suitably regularized pseudo-inverse such as $[H(\theta_t)+\epsilon I ]^{-1}$ w/a small $\epsilon$ parameter.
		\item For ML, Newton's method not practical for two interrelated reasons:

		\begin{itemize}
			\item Calculating a Hessian is an extremely expensive numerical computation.
			\item Even if we employ first order approximation methods to approximate the Hessian (commonly called quasi-Newton methods), we must store and invert a matrix with $n^2$ entries, $n$=the number of parameters. For models with millions of parameters such as those commonly employed in neural networks, this is close to impossible with present-day computational power. 
		\end{itemize}
		\item Important intuition from Newton's method to modify GD: Netwon's method automatically adapts the learning rate of different parameters depending on the Hessian matrix. Whereas simple GD has the same learning rate for all the parameters. The Hessian encodes the curvature of the surface we're minimizing. Specifically, the singular values of the Hessian are inversly proportional to the squares of the local curvatures of the surface.
		{\tiny
		\begin{align}
			\text{the Hessian's singular values} \propto \frac{1}{(\text{the local curvatures of the surface})^2}
		\end{align}}

		Newton's method thus automatically adjusts the step size so that one takes larger steps in flat directions with small curvatures and smaller steps in steep directions with large curvature.
		\item Consider special case: Using GD to find minimum of a quadratic energy func of a single parameter $\theta$. Given current value of $\theta$, find $\eta_{opt}$ = the $\eta$ that lets us reach the minimum in a single step. To find it expand the energy func to second order around the current value
		\begin{align}
			E(\theta + v) = E(\theta_c) + \partial_\theta E(\theta) v 
			+ \frac{1}{2} \partial_\theta^2 E(\theta) v^2
		\end{align}
		We want to find the step $v$ such that $\theta + v$ is a stationary point. That is we seek to solve the equation that sets the derivative of this last expression wrt $v$ equal to zero:
		\begin{align}
			0 =& \partial_\theta E(\theta) + \partial_\theta^2 E(\theta) v \\
			v =& - \partial_\theta E(\theta) [\partial_\theta^2 E(\theta)]^{-1} \\
			\Rightarrow \theta_{min} =& \theta - v \\
			\theta_{min} =& \theta - [\partial_\theta^2 E(\theta)]^{-1} \partial_\theta E(\theta) 
		\end{align}
		Comparing with the previously outlined GD update rule tells us that
		\begin{align}
			\eta_{opt} = [\partial_\theta^2 E(\theta)]^{-1} 
		\end{align}
		Four qualitatively different regimes possible (fig 8):
		\begin{itemize}
			\item $\eta < \eta_{opt}$: GD will take multiple small steps to reach the bottom of the potential.
			\item $\eta = \eta_{opt}$: GD reaches the bottom of the potential in a single step
			\item $\eta_{opt} < \eta < 2\eta_{opt}$: GD oscillates across both sides of the potential before eventually converging to the minima.
			\item $\eta > 2\eta_{opt}$: GD diverges!
		\end{itemize}
		\item Straightforward to generalize to the multidimensional case: The natural multidimensional generalization of the second derivative = the Hessian $H(\theta)$. Can always perform a singular value decomposition (= a rotation bt an orthogonal matrix for quadratic minima where the Hessian is symmetric) and consider the Hessian's singular values $\{ \lambda \}$. If we use a single learning rate for all parameters, in analogy with the $\eta_{opt}$ found above, convergence requires that
		\begin{align}
			\eta < \frac{2}{\lambda_{max}}
		\end{align}
		where $\lambda_{max}$=the Hessian's largest singular value. If the minimum $\lambda_{min}$ differs significantly from $\lambda_{max}$, then convergence in the $\lambda_{min}$ direction will be extremely slow. Can show that convergence time scales w/ the condition number
		\begin{align}
			\kappa = \frac{\lambda_{max}}{\lambda_{min}}
		\end{align}

		\item Limitations of the simplest GD algo:
		\begin{itemize}
			\item \textit{GD finds local minima}
			Since GD is deterministic it converges to a local minimum of our energy func. May lead to poor performance in ML. A similar problem is encountered in physics and overcome by methods like \textbf{simulated annealing} that introduce a fictitious "temperature" which is eventually taken to zero. The "temperature" term introduces stochasticity in the form of thermal fluctuations that allow the algo to thermally tunnel over energy barriers. Suggests we should in ML modify GD to include stochasticity. 
			\item \textit{GD is sensitive to initial conditions}
			Initial conditions matter, as a consequence of GD's local nature. Thus, very important to think about how you initialize, both for simple and more complicated GD variant introduced later.
			\item \textit{Gradients are computationally expensive to calculate for large datasets}
			As previously mentioned, in many statistics and ML cases, the energy func is a sum of terms, one for each data point = to calc the gradient we must sum over all $n$ data points. Doint this at each GD point = extremely computationally expensive. An ingenious solution: calc the gradients using small subsets of data = "mini batches". This also introduces stochasticity into our algo.
			\item \textit{GD very sensitive to choices of learning rates}
			Small lr = slow, large lr = possible divergence/poor results. Also, depending on the local landscape, we have to modify lr to ensure convergence. Ideally would "adaptively" choose lr to match the landscape.
			\item \textit{GD treats all directions in parameter space uniformly}
			Unlike Netwon's method. Thus the maximum lr is set by the behavior of the steepest direction and this can significantly slow training. Would ideally like large steps in flat dir and small steps in steep dir. Since we are exploring rugged landscapes where curvatures change, this requires us to keep track of not only the gradient but second derivatives of the energy func (as discussed calc Hessian would be ideal, but proves too computationally expensive).
			\item \textit{GD can take exponential time to escape saddle points, even with random initialization}
			As mentioned extremely sensitive to initial conditions since it determines the particular local minimum GD will reach. But, even with a good initialization scheme (through the introduction of randomness) GD can still take exponential time to escape saddle points, prevalent in high-dimensional space, even for non-pathological objective functions. \textbf{There are modified GD methods developed recently to accelerate the escape, see reference}.
		\end{itemize}

		\item Stochastic gradient descent (SGD) with mini-batches:
		Stochasticity is incorporated by approximating the gradient on a subset of the data called a mini-batch. 
		\begin{itemize}
			\item Size of mini-batch almost always $\ll$ total number of data points $n$.
			\item Typical mini-batch sizes ranging from ten to a few hundred data points
			\item If there are $n$ points, and the mini-batch size is $M$, there will be $n/M$ mini-batches.
			\item Denote these mini-batches by $B_k$ where $k=1...n/M$.
		\end{itemize}
		Thus in SGD at each GD step we approximate the gradient using a minibatch $B_k$,
		\begin{align}
			\nabla_\theta = \sum_i^n \nabla_\theta e_i (\mathbf{x}_i, \bm{\theta}) \rightarrow \sum_{i\in B_k} \nabla_\theta e_i (\mathbf{x}_i, \bm{\theta})
		\end{align} 
		cycling over all $M$ minibatches. A full iteration over all $n$ data points = using all $M$ minibatches= an \textit{epoch}. Denote the minibatch approximation to the gradient by
		\begin{align}
			\nabla_\theta E^{MB} (\bm{\theta}) = \sum_{i\in B_k}^M \nabla_\theta e_i (\mathbf{x}_i, \bm{\theta})
		\end{align}
		Then SGD algo is
		\begin{align}
			\mathbf{v}_t =& \eta_t \nabla_\theta E^{MB} (\bm{\theta}) \\
			\bm{\theta}_{t+1} =& \bm{\theta}_t - \mathbf{v}_t
		\end{align}
		Two important benefits to SGD.
		\begin{itemize}
			\item Introduces stochasticity and decreases chance the fitting algo gets stuck in isolated local minima.
			\item Significantly speeds up the calc as one does not have to use all $n$ data points to calc the gradient. 
			\item Empirical and theoretical work suggets SGD has additional benefits - one significant being that introducing stochasticity is thought to act as \textbf{a natural regularizer that prevents overfitting} in deep, isolated minima.
		\end{itemize}

		\item Adding momentum:
		SGD almost always used with a "momentum"/inertia term serving as a memory of the direction we are moving in parameter space. Implemented as
		\begin{align}
			\mathbf{v}_t =& \gamma \mathbf{v}_{t-1} + \eta_t \nabla_\theta E(\bm{\theta}_t) \\
			\bm{\theta}_{t+1} =& \bm{\theta}_t - \mathbf{v}_t
		\end{align}
		where $\gamma$=a momentum parameter with $0 \leq \gamma \leq 1$ (and have dropped the explicit notation indicating the gradient is taken over a minibatch). This is gradient descent with momentum (GDM). Clear that 
		\begin{itemize}
			\item $\mathbf{v}_t$ is a running average
			\item $(1-\gamma)^{-1}$ sets the characteristic time scale for the memory used in the averaging procedure.  
			\item $\gamma=0$ reduces down to ordinary SGD
		\end{itemize}
		Equivalent way of writing updates:
		\begin{align}
			\Delta \bm{\theta}_{t+1} = \gamma \Delta \bm{\theta}_t - \eta_t \nabla_\theta E(\bm{\theta}_t)
		\end{align}
		where $\Delta \bm{\theta}_t =  \bm{\theta}_t \bm{\theta}_{t-1} $.

		Getting intuition: Consider simple physical analogy with a particle of mass $m$ moving in a viscous medium with drag coefficient $\mu$ and potential $E(\mathbf{w})$, $\mathbf{w}$=the particle's position. It's motion then described by
		\begin{align}
			m\frac{d^2 \mathbf{w}}{dt^2} + \mu \frac{d\mathbf{w}}{dt}
			= - \nabla_w E(\mathbf{w})
		\end{align}
		Discretize in usual way to get
		\begin{align}
			m\frac{\mathbf{w}_{t+\Delta t} -2\mathbf{w}_t + \mathbf{w}_{t-\Delta t}}{(\Delta t)^2} + \mu \frac{\mathbf{w}_{t + \Delta t} - \mathbf{w}_t}{\Delta t} = - \nabla_w E(\mathbf{w})
		\end{align}
		Rearrange to write as
		\begin{align}
			\Delta \mathbf{w}_{t + \Delta t} = - \frac{(\Delta t)^2}{m + \mu\Delta t} \nabla_w E(\mathbf{w}) + \frac{m}{m+\mu\Delta t} \Delta \mathbf{w}_t
		\end{align}

		Notice it's identical to our GDM update rule defined above. We may thus identify the momentum parameter and learning rate with the mass of the particle and the viscous drag:
		\begin{align}
			\gamma &= \frac{m}{m+ \mu \Delta t} \\
			\eta &= \frac{(\Delta t)^2}{m+\mu \Delta t}
		\end{align}
		Thus as suggested by the name the momentum parameter is proportional to the mass of the particle and effectively provides inertia. Also, in the large viscosity/small learning rate limit, our memory scales as $(1-\gamma)^{-1} \approx m/(\mu \Delta t)$

		Why momentum useful?
		\begin{itemize}
			\item Helps the algo gain speed in diretions with persistent but small gradients even in the presence of stochasticity, while surpressing oscillations in high-curvature directions.
			\item Has been argued first-order methods (with appropriate initial conditions) can perform comparable to more expensive second-order methods, especially in context of complex deep learning models (reference)
			\item Studies suggest benefits of momentum especially pronounced in complex models in the initial "transient phase" of training, rather than during subsequent fine-tuning of a coarse minimum. Because in the transient phase, correlations in the gradient persist across many GD steps, accentuating the role of inertia and memory.			
		\end{itemize}
		These can be even more pronounced using a slightly modified algo, Nesterov Accelerated Gradient (NAG). Rather than calc gradient at the current parameters $\nabla_\theta E(\bm{\theta}_t)$, calc the gradient at the expected value of the parameters given our current momentum $\nabla_\theta E(\bm{\theta}_t + \gamma \mathbf{v}_{t-1})$. One major advantage is it allows for larger learning rate than GDM for same choice of $\gamma$.

		\item Methods that use the second moment of the gradient:
		In SGD, with and without momentum, we still have to specify a "schedule" for tuning the learning rates $\eta_t$ as a func of time. As touched upon before this presents dilemmas. 
		\begin{itemize}
			\item The lr is limited by the steepest direction which can change depending on the current position in the landscape. To circumvent this, ideally our algo would keep track of curvature and take large steps in shallow/flat dirs and small steps in steep/narrow dirs.
			\item Second-order methods accomplish this by calc or approximating the Hessian and normalizing the lr by the curvature.
			\item But this is very computationally expensive for extremely large models.
			\item Ideally, we would like to adaptively change the step size to match the landscape without paying the steep computational price of calculating/approximating the Hessian.
			\item Recently introduced a number of methods that accomplish this by tracking not only the gradient, but also the second moment of the gradient. These include AdaGrad, AdaDelta, RMS-Prop, and ADAM. We'll discuss the last two.
		\end{itemize}

		RMS prop: In addition to keeping a running average of the first moment of the gradient, also keep track of the second moment denoted by $\mathbf{s}_t = \mathbb{E}[\mathbf{g}_t^2]$. Update rule is
		\begin{align}
			\mathbf{g}_t =& \nabla_\theta E(\bm{\theta}) \\
			\mathbf{s}_t =& \beta \mathbf{s}_{t-1} + (1-\beta)\mathbf{g}_t^2 \\
			\bm{\theta}_{t+1} =& \bm{\theta}_t - \eta_t \frac{\mathbf{g}_t}{\sqrt{\mathbf{s}_t + \epsilon}}
		\end{align}
		where
		\begin{itemize} 
			\item $\beta$ = controls the averaging time of the second moment and is typically $\beta=0.9$
			\item $\eta_t$ = a learning rate, typically $10^{-3}$
			\item $\epsilon \sim 10^{-8}$ = a small regularization constant to prevent divergences
			\item Multiplication and division by vectors is understood as element-wise operations
			\item Clear from the formula that lr reduced in dirs where the norm of the gradient is consistently large. This greatly speeds up the convergence by allowing us to use a larger lr for flat dirs. 
		\end{itemize}

		The ADAM optimizer: Keep a running average of both the first and second moment of the gradient ($\mathbf{m}_t = \mathbb{E}[\mathbf{g}_t]$ and $\mathbf{s}_t = \mathbb{E}[\mathbf{g}_t^2]$ respectively) - use this info to adaptively change the lr for different parameters. Also performs an additional bias correction to account for the fact that we're estimating the first two moments of the gradient using a running average (denoted below by the hats). Update rule is (multiplication and division by vectors again understood to be element-wise):
		\begin{align}
			\mathbf{g}_t &= \nabla_\theta E(\bm{\theta}) \\
			\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1-\beta_1) \mathbf{g}_t \\
			\mathbf{s}_t &= \beta_2 \mathbf{s}_{t-1} + (1-\beta_2) \mathbf{g}_t^2 \\
			\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1-\beta_1^t} \\
			\hat{\mathbf{s}}_t &= \frac{\mathbf{s}_t}{1-\beta_2^t} \\
			\bm{\theta}_{t+1} &= \bm{\theta}_t - \eta_t \frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{s}}_t} + \epsilon}
		\end{align}
		where
		\begin{itemize}
			\item $\beta_1$ and $\beta_2$ set the memory lifetime of the first and second moment, typically taken as 0.9 and 0.99 respectively
			\item $\eta$ and $\epsilon$ identical to RMS prop.
		\end{itemize}
		Like in RMSprop the effective step size of a parameter depends on the magnitude of its gradient squared. To better understand, let's rewrite this expression in terms of the variance $\mathbf{\sigma}_t^2 = \hat{\mathbf{s}}_t  - (\hat{\mathbf{m}}_t)^2$. Consider a single parameter $\theta_t$. Update rule of it is given by
		\begin{align}
			\Delta \theta_{t+1} = -\eta_t \frac{\hat{\mathbf{m}}_t}{\sqrt{\sigma_t^2 + \hat{\mathbf{s}}_t^2} + \epsilon}
		\end{align}
		We examine the limiting cases of this expression. 
		\begin{itemize}
			\item Assume our gradient estimates are consistent so the variance is small. Then update rule tends to $\Delta \theta_{t+1} \rightarrow -\eta_t$ (assumed $\hat{\mathbf{m}}_t \gg \epsilon$). This is equivalent to cutting off large persistent gradients at 1 and limiting the max step size in steep directions.
			\item Imagine the gradient is widely fluctuating between GD steps. Then $\sigma^2 \gg \hat{\mathbf{m}}_t^2$ so our update becomes
			$\Delta \theta_{t+1} \rightarrow -\eta_t \hat{\mathbf{m}}_t/ \sigma_t$. AKA, we adapt our lr so that
			{\tiny
			\begin{align*}
				\text{learning rate} \propto \text{signal-to-noise ratio (i.e. the mean in units of the standard deviation)}
			\end{align*}}
			\item = extremely desirable from physical point of view. The std serves as a natural adaptive scale for deciding whether a gradient is large or small. 
			\item Thus, ADAM has the beneficial effects of adapting our step size so that we cut off large gradient dirs (and hence prevent oscillations and divergences) and measuring gradients in terms of a natural length scale, the std $\sigma_t$.
			\item Above discussion also explains empirical observations showing that the performance of both ADAM and RMSprop is drastically reduced if the square root omitted in the update rule. 
			\item Also worth noting recent studies have shown adaptive methods like RMSprop, ADAM, and AdaGrad tend to generalize worse than SGD in classification tasks, though they achieve smaller training error. See refs.
		\end{itemize}

		\item Comparisons of various methods:
		Visualize the performance of the 5 discussed methods GD, GDM, NAG, ADAM and RMSprop by using Beale's function:
		{\tiny
		\begin{align}
			f(x,y) =& (1.5 - x + xy)^2
			+ (2.25 - x + xy^2)^2 + (2.625 - x+ xy^3)^2
		\end{align}}
		It has a global minimum at $(x, y) = (3, 0.5)$ and an interesting structure.
		Fig 9:
		\begin{itemize}
			\item Shows the results of using all five methods for $N_{steps}=10^4$ steps from three different initial conditions.
			\item GD, GDM, NAG learning rate: $\eta = 10^{-6}$
			\item RMSprop, ADAM learning rate: $\eta = 10^{-3}$ (can be higher due to their adaptive step sizes. thus these methods tend to be quicker at navigating the landscape.)
			\item Notice in some cases (e.g. initial condition (-1, 4)), the trajectories do not find the global min but instead follow the deep, narrow ravine that occurs along $y=1$. This kind of landscape structure is generic in high-dimensional spaces where saddle points proliferate.
		\end{itemize}

		\item Gradient descent in practice: practical tips (especially in the context of deep neural networks):
		\begin{itemize}
			\item \textit{Randomize the data when making mini-batches}
			Important to randomly shuffle the data when forming the batches. Otherwise GD can fit spurious correlations resulting from the order in which data is presented.
			\item \textit{Transform your inputs}
			As discussed learning=difficult when landscape has miz of steep and flat dirs. Trick: standardize the data by subtracting the mean and normalizing the variance of input variables. Whenever possible also decorrelate the inputs. Why helpful - consider the case of linear regression: for the squared error cost func, the Hessian of the energy matrix = the correlation matrix between inputs. = by standardizing the inputs we ensure the landscape looks homogeneous in all dirs in parameter space. Most deep networks can be viewed as linear transformations followed by a non-linearity at each layer, thus we expect this intuition to hold beyond the linear case.
			\item \textit{Monitor the out-of-sample performance}
			Always monitor the performance on a validation set = a small portion of the training data kept out of the training process to serve as a proxy for the test set. Validation error starting to increase = model being overfit. Terminate the learning. This \textit{early stopping} significantly improves performance in many settings.
			\item \textit{Adaptive optimization methods don't always have good generalization}
			As mentioned recent studies have shown adaptive methods such as ADAM, RMSprop and AdaGrad to have poor generalization campared to SGD or GDM, particularly in the high-dimensional limit = the number of parameters exceeds the number of data points. Although not clear why these methods perform so well in training deep neural networks such as generative adversarial networks (GANs), simpler procedures like properly tuned SGD may work as well or better in these applications.
		\end{itemize}

	\end{itemize}

\section{Overview of Bayesian inference}
Statistical modeling focus: estimation/prediction of unknown quantities. Bayesian methods premise: probability can be used as mathematical tool for describing uncertainty. 
Similar in spirit to physics statistical mechanics - where we use probability to describe the behavior of large systems where we cannot know the positions and momenta of all particles even if the system itself is fully deterministic (at least classically).
This section gives introduction to Bayesian inference, w/special emphasis on its logic=Bayesian reasoning and connections to ML.

\subsection{Bayes rule}
Must specify two functions
\begin{itemize}
	\item $p(\mathbf{X}|\mathbf{w})$ = the likelihood function, which describes the probability of observing a dataset $\mathbf{X}$ for a given value of the unknown parameters $\mathbf{w}$. The func should be considered a func of the parameters $\mathbf{w}$ with the data $\mathbf{X}$ held fixed.
	\item $p(\mathbf{w})$ = the prior distribution, which describes any knowledge we have about the parameters before we collect the data.
	\item with these two we can compute the posterior distribution via Baye's rule:
	\begin{align}
		p(\mathbf{w}|\mathbf{X}) = \frac{p(\mathbf{X}|\mathbf{w})p(\mathbf{w})}{\int d\mathbf{w} p(\mathbf{X}|\mathbf{w})p(\mathbf{w})}
	\end{align}
	It describes our knowledge about the unknown parameter $\mathbf{w}$ after observing the data $\mathbf{X}$. In many casing computing the normalizing constant (i.e. the partition function $p(\mathbf{X}) = \int d\mathbf{w} p(\mathbf{X}|\mathbf{w})p(\mathbf{w})$) and Markov Chain Monte Carlo (MCMC) methods are needed to draw random samples from the posterior dist.
\end{itemize}
The likelihood func = common feature in both classical statistics and Bayesian inference. Determined by the model and the measurement noise. Many statistical procedures fex least-square fitting can be cast into the formalism of \textit{Maximum Likelihood Estimation} (MLE). In MLE one chooses the parameters $\hat{\mathbf{w}}$ that maximize the likelihood (or equivalently the log-likelihood since log a monotonic func) of the observed data:
\begin{align}
	\hat{\mathbf{w}} = argmax_w \{log(p(\mathbf{X}|\mathbf{w})) \}
\end{align}
Aka, in MLE we choose the parameters that maximize the probability of seeing the observed data given our generative model. MLE is an important concept in both frequentist and Bayesian statistics.

The prior distribution, by contrast, is uniquely Bayesian. Two general classes of priors:
\begin{itemize}
	\item If we do not have any specialized knowledge of $\mathbf{w}$ before we look at the data we would like to select an \textit{uninformative} prior that reflects our ignorance
	\item Otherwise we select an \textit{informative} prior that accurately reflects the knowledge we have about $\mathbf{w}$.
\end{itemize}
There is a large literative on uninformative priors, including reparametrization invariant priors that would be of interest to physicists, but here we'll focus on \textit{informative} priors. Using this tends to decrease the variance of the posterior distribution while, potentially, increasing its bias. Beneficial if decrease in variance $\gg$ increase in bias.

In high-dimensional problems it's reasonable to assume many of the parameters not strongly relevant = many of the parameters will be zero or close to zero. Can express this belief using two commonly used priors:
\begin{itemize}
	\item $p(\mathbf{w}|\lambda) = \prod_j \sqrt{\frac{\lambda}{2\pi}} e^{-\lambda w_j^2}$ = the Gaussian prior = used to express the assumption many of the parameters will be small
	\item $p(\mathbf{w}|\lambda) = \prod_j \frac{\lambda}{2} e^{-\lambda |w_j|}$ = the Laplace prior = used to express the assumption that many of the parameters will be zero.
\end{itemize}
Come back to this in section VI.F.

\subsection{Bayesian decisions}
We have seen how to compute the posterior distribution, which expresses our knowledge about the parameters $\mathbf{w}$. Mostly however we need to summarize our knowledge and pick a single "best" value for them. In princple the specific value should be chosen to maximize a utility function. In practice however, usually use one of two choices:
\begin{itemize}
	\item $\langle \mathbf{w} \rangle = \int  d\mathbf{w}  \mathbf{w}p(\mathbf{w}|\mathbf{X})$ = the posterior mean = the Bayes estimate (minimizes the mean-squared error)
	\item $\hat{\mathbf{w}} = argmax_w p(\mathbf{w}|\mathbf{X})$ = the posterior mode = the maximum-a-posteriori = MAP estimate (easier to compute)
\end{itemize}

\subsection{Hyperparameters}
The Gaussian and Laplace prior dist. both have an extra parameter $\lambda$ = \textit{hyperparameter} = \textit{nuisance variable}. Has to be chosen somehow.
\begin{itemize}
	\item One standard approach: define another prior dist for $\lambda$, usually using an uninformative prior, and average the posterior dist over all choices of $\lambda$. = a hierarchical prior. However computing averages often requires long MCMC simulations that are computationally expensive.
	\item Simpler: find a good $\lambda$ value using an optimization procedure. Will discuss later.
\end{itemize}

\section{Linear regression}
In this section we take a closer look at the ideas of the optimal choice of predictor depending on the choice of fitted function and underlying noise level, and model complexity, the bias-variance decomposition, the statistical meaning of learning. As previously fitting a given set of samples $(y_i, \mathbf{x}_i)$ means relating the independent variables $\mathbf{x}_i$ to their responses $y_i$.

Formulating the problem:
\begin{itemize}
	\item $\mathcal{D} = \{ (y_i, \mathbf{x}_i) \}_{i=1}^n$ = a given dataset with $n$ samples, where $\mathbf{x}_i$ = the $i$-th observation vector while $y_i$ = its corresponding (scalar) response.
	\item $\mathbf{x}_i \in \mathbb{R}^p $ = assume every sample has $p$ features.
	\item $f$ = the true function/model that generated these samples via $y_i = f(\mathbf{x}_i ; \mathbf{w}_{true}) + \epsilon_i = \mathbf{x}_i^T \mathbf{w}_{true} + \epsilon_i$ for some unknown but fixed $\mathbf{w}_{true} \in \mathbb{R}^p$.
	\item $g$ = the function we wish to find, with parameters $\mathbf{w}$ fit to the data $\mathcal{D}$, that can best approximate $f$. When we have a $\hat{\mathbf{w}}$ such that $g(\mathbf{x}; \hat{\mathbf{w}})$ yields our best estimate of $f$, we can use this $g$ to make predictions about the response $y_0$ for a new data point $\mathbf{x}_0$.
	\item $L^p$ for any real number $p \geq 1$ = the norm of a vector $\mathbf{x} = (x_1, ..., x_d) \in \mathbb{R}^d$, defined as
	\begin{align}
		||\mathbf{x}||_p = (|x_1|^p + ... + |x_d|^p)^\frac{1}{p}
	\end{align}
\end{itemize}

\subsection{Least-square regression}
\textit{Ordinary least squares linear regression} (OLS) = the minimization of the $L_2$ norm of the difference between the response $y_i$ and the predictor $g(\mathbf{x}_i ; \mathbf{w}) = \mathbf{x}_i^T \mathbf{w}$:
\begin{align}
	min_{\mathbf{w} \in \mathbb{R}^p} ||\mathbf{X}\mathbf{w} - \mathbf{y}||_2^2 = min_{\mathbf{w} \in \mathbb{R}^p} \sum_{i=1}^n (\mathbf{x}_i^T \mathbf{w} - y_i)^2
\end{align}
Geometrically, the predictor func $g$ defines a hyperplane in $\mathbb{R}^p$. Thus minimizing least-square error = minimizing the sum of all projections (=residuals) for all points $\mathbf{x}_i$ to this hyperplane (fig 10). Denote the solution to this as $\hat{\mathbf{w}}_{LS}$:
\begin{align}
	\hat{\mathbf{w}}_{LS} = argmin_{\mathbf{w} \in \mathbb{R}^p} ||\mathbf{X}\mathbf{w} - \mathbf{y}||_2^2
\end{align}
which after differentiation leads to
\begin{align}
	\hat{\mathbf{w}}_{LS} = (X^T X)^{-1} X^T \mathbf{y}
\end{align}
where we assumed $X^T X$ is invertible. 
\begin{itemize}
	\item Often the case when $n \gg p$. Formally: if $rank(X) = p$, namely, the predictors $X_1, ..., X_p$ (=the columns of $X$) are linearly independent, then $\hat{\mathbf{w}}_{LS}$ is unique.
	\item $rank(X) < p$: happens when $p>n$. $X^T X$ singular. Implying there are infinitely many solutions to the least squares problem. Can then show that if $\mathbf{w}_0$ a solution, then also $\mathbf{w}_0 + \eta$ also a solution for any $\eta$ which satisfies $\mathbf{X}\eta = 0$ (= $\eta \in null(X)$). 
\end{itemize}
Having the solution, we can calculate $\mathbf{y}$, the best fit of our data $X$, as
\begin{align}
	\hat{\mathbf{y}} &= X \hat{\mathbf{w}}_{LS} = P_X \mathbf{y} \\
	\text{where} \quad P_X &= X(X^T X)^{-1} X^T
\end{align}
Geometrically, $P_X$ = the projection matrix which acts on $\mathbf{y}$ and projects it onto the column space of $X$, which is spanned by the predictors $X_1, ..., X_p$ (fig 11).

Notice: we found the optimal solution $\hat{\mathbf{w}}_{LS}$ in one shot, no iterative optimization.

Have explained the difference between \textit{learning} and \textit{fitting} lies in the prediction on "unseen" data. Must thus examine out-of-sample error. Following our previos definitions in section 3 of $\bar{E}_{in}$ and $\bar{E}_{out}$, the average in-sample and out-of-sample error can be shown to be
\begin{align}
	\bar{E}_{in} =& \sigma^2 (1 - \frac{p}{n}) \\
	\bar{E}_{out} =& \sigma^2 (1 + \frac{p}{n})
\end{align}
provided we obtain $\hat{\mathbf{w}}_{LS}$ from i.i.d. samples $X$ and $\mathbf{y}$ generated through $\mathbf{y} = X \mathbf{w}_{true} + \epsilon$ (this requires $\epsilon$ is a noise vector whose elements are i.i.d. of 0 mean nad variance $\sigma^2$, and is independent of the samples $X$). Can thus calc the average generalization error explicitly:
\begin{align}
	|\bar{E}_{in} - \bar{E}_{out}| = 2\sigma^2 \frac{p}{n} 
\end{align} 
This imparts important message:
\begin{itemize}
	\item $p \gg n $ (= high dimensional data): the generalization errror is extremely large = the model is not learning
	\item $p \approx n$: even now we might still not learn well due to the intrinsic noise $\sigma^2$. One way of amelioration = reularization. Mainly two forms:
	\begin{itemize}
		\item \textit{Ridge regression}: employs an $L_2$ penalty
		\item \textit{LASSO}: uses an $L_1$ penalty
	\end{itemize} 
\end{itemize}

\subsection{Ridge-regression}
We here study effect of adding to the least squares loss function a \textit{regularizer} defined as the $L_2$ norm of the parameter vector we want to optimize over = wish to solve the following \textit{penalized} regression problem, \textit{Ridge regression}:
\begin{align}
	\hat{\mathbf{w}}_{Ridge} (\lambda) = argmin_{\mathbf{w} \in \mathbb{R}^p}  (||X \mathbf{w} - \mathbf{y}||_2^2 + \lambda ||\mathbf{w}||_2^2)
\end{align}
Equivalent to the following \textit{constrained} optimization problem
\begin{align}
	\hat{\mathbf{w}}_{Ridge} (t) = argmin_{\mathbf{w} \in \mathbb{R}^p : ||\mathbf{w}||_2^2 \leq t} ||X \mathbf{w} - \mathbf{y}||_2^2 
\end{align}
Means for any $t\geq 0$ and $\hat{\mathbf{w}}_{Ridge}$ in the last equation there exists a $\lambda \geq 0$ such that $\hat{\mathbf{w}}_{Ridge}$ solves the second last equation, and vice versa (note: equivalence between the penalized and the constrained (regularized) form of least square does not always hold. It holds for Ridge and LASSO, but not for best subset selection defined by choosing a $L^0$ norm). 

With this equivalence it's obvious that by adding a regularization term $\lambda ||\mathbf{w}||_2^2$ to our ls loss func, we are effectively constraining the magnitude of the parameter vector learned from the data. To see this, let's evaluate the second last equation explicitly. Differentiating w.r.t $\mathbf{w}$ gives
\begin{align}
	\hat{\mathbf{w}}_{Ridge} (\lambda) &= (X^T X + \lambda I_{p \times p})^{-1} X^T \mathbf{y} \\
	\text{If X orthogonal:} \quad &= \frac{\hat{\mathbf{w}}_{LS}}{1 + \lambda} 
\end{align}
Implies the ridge estimate is merely the least squares estimate scaled by a factor $(1 + \lambda)^{-1}$.

Can we derive similar relation between the fitted vector $\hat{\mathbf{y}} = X \hat{\mathbf{w}}_{Ridge}$ and the prediction made by ls linear regression? We do a singular value decomposition (SVD) on $X$.  The SVD of an $n\times p$ matrix $X$ has the form
\begin{align}
	X = UDV^T
\end{align}
where $[U]_{n\times p}$ and $[V]_{p\times p}$ are orthogonal matrices such that the columns of $U$ span the column space of $X$ while the columns of $V$ span the row space of $X$. $[D]_{p\times p} = diag(d_1, d_2, ..., d_p)$ is a diagonal matrix with entries $d_1 \geq d_2 \geq ... d_p \geq 0$ called the singular values of $X$. $X$ is singular if there is at least one $d_j=0$. By writing $X$ in terms of its SVD, one can recast the Ridge estimator as
\begin{align}
	\hat{\mathbf{w}}_{Ridge} = V(D^2 + \lambda I)^{-1} D U^T \mathbf{y}
\end{align}
which implies that the Ridge predictor satisfies 
\begin{align}
	\hat{\mathbf{y}}_{Ridge} =& X \hat{\mathbf{w}}_{Ridge} \\
	=& UD(D^2 + \lambda I)^{-1} DU^T \mathbf{y} \\
	=& \sum_{j=1}^p \mathbf{u}_j \frac{d_j^2}{d_j^2 + \lambda} \mathbf{u}_j^T \mathbf{y} \\
	\leq& UU^T \mathbf{y} \\
	=& X \hat{\mathbf{y}} \equiv \hat{\mathbf{y}}_{LS}
\end{align}
where in the inequality step we used SVD to simplify $\hat{\mathbf{w}}_{LS}$ and asumed $\lambda \geq 0$. Comparing the third last line and last line it's clear that 
\begin{itemize}
 \item In order to compute the fitted vector $\hat{\mathbf{y}}$, both Ridge and ls linear regression have to project $\mathbf{y}$ to the column space of $X$. 
 \item Only difference: Ridge regression further shrinks each basis component $j$ by a factor $d_j^2/(d_j^2 + \lambda)$
\end{itemize}


\subsection{LASSO and sparse regression}
Here study the effects of adding an $L_1$ regularization penalty, called \textit{LASSO} (=least absolute shrinkage and selection operator). LASSO in the penalized form is defined by the following regularized regression problem:
\begin{align}
	\hat{\mathbf{w}}_{LASSO} (\lambda) = argmin_{\mathbf{w} \in \mathbb{R}^p} || X \mathbf{w} - \mathbf{y} ||_2^2 + \lambda ||\mathbf{w}||_1
\end{align}
As with Ridge there is another formulation based on constrained optimization:
\begin{align}
	\hat{\mathbf{w}}_{LASSO} (t) = argmin_{\mathbf{w} \in \mathbb{R}^p : ||\mathbf{w}||_1 \leq t} || X \mathbf{w} - \mathbf{y} ||_2^2
\end{align}

\begin{itemize}
	\item The equivalence interpretation same as in Ridge, namely: for any $t \geq 0$ and solution $\hat{\mathbf{w}}_{LASSO}$ in the last equation there is a $\lambda \geq 0$ such that $\hat{\mathbf{w}}_{LASSO}$ solves the second last equation. And vice versa.
	\item However, to get the analytical solution of LASSO cannot simply take gradient wrt $\mathbf{w}$, since the $L_1$-regularizer is not everywhere differentiable, in particular at any point where $w_j = 0$ (fig 13). 
	\item Nonetheless, LASSO is a \textbf{convex} problem. Can thus invoke the \textit{subgradient optimality condition} in optimization theory to obtain solution. 
\end{itemize}
For simple notation, only show the solution for when $X$ is orthogonal:
\begin{align}
	\text{X orthogonal:} \quad \hat{w}_j^{LASSO} (\lambda) = sign(\hat{w}_j^{LS}) (|\hat{w}_j^{LS}| - \lambda)_+ 
\end{align}
where $(x)_+$ = the positive part of $x$.

Fig 12 compares the Ridge and LASSO solutions.
\begin{itemize}
	\item As mentioned, Ridge is the LS solution scaled by a factor $(1 + \lambda)$. LASSO does something called "soft-thresholding". 
	\item LASSO tends to give sparse solutions, meaning many components of $\hat{\mathbf{w}}_{LASSO}$ are zero. Fig 13 gives intuitive explanation of this. In short:
	\begin{itemize}
		\item To solve a constrained optimization problem with a fixed regularization strength $t \geq 0$ one first carves out the "feasible region" specified by the regularizer in the $\{ w_1, ..., w_d \}$ space = a solution $\hat{\mathbf{w}}_0$ only legitimate if falls within this region. 
		\item Then one plots the contours of the ls regressors in an increasing manner until the contour touches the feasible region. 
		\item The point where this occurs is the solution to our optimization problem. 
		\item Loosely speaking, since the $L_1$ regularizer of LASSO has sharp protrusions(=vertices) along the axes, and because the regressor contours are in the shape of ovals (it's quadratic in $\mathbf{w}$), their intersection tends to occur at the vertex of the feasibility region = implying the solution vector will be sparse. (the vertices correspond to parameter vectors $\mathbf{w}$ with only one non-vanishing component).
	\end{itemize}
\end{itemize}
See fig 14 and 15 for comparison of Ridge and Lasso on Diabetes dataset.
\subsection{Using linear regression to learn the Ising Hamiltonian}
Task: learning the Hamiltonian for the Ising model. Given an ensamble of random spin configurations, and assigned to each state its energy, generated from the 1D Ising model:
\begin{align}
	H = -J \sum_{j=1}^L S_j S_{j+1}
\end{align}
where
\begin{itemize}
	\item $J$ = the nearest-neighbour spin interaction
	\item $S_j \in \{ \pm 1 \}$ = a spin variable
	\item Assume data was generated with $J=1$
	\item You are handed the dataset $\mathcal{D} = (\{ S_j \}_{j=1}^L, E_j)$ without knowing what the $E_j$ number mean
	\item The $\{ S_j \}_{j=1}^L$ cofiguration can be interpreted in many ways: outcome of coin tosses, black-and-white image pixels, binary representation of integers, etc.
	\item Goal: learn a model that predicts $E_j$ from the spin configs.
\end{itemize}
Solving the problem with linear regression (lr):
\begin{itemize}
	\item Without any prior knowledge of the dataset's origin, physics intuition may suggest: look for a spin model w/pairwise interactions between every pair of variables. That is we choose the following model class:
	\begin{align}
		H_{model} [S^i] = - \sum_{j=1}^L \sum_{k=1}^L J_{j,k} S_j^i S_k^i
	\end{align}
	\item Goal: Determine the interaction matrix $J_{j,k}$ by applying linear regression on the dataset $\mathcal{D}$ = a well defined problem, since the unknown $J_{j,k}$ enters linearly into the definition of the Hamiltonian.
	\item To this end we cast the above ansatz into the more familiar linear regression form:
	\begin{align}
		H_{model} [S^i] = \mathbf{X}^i \cdot \mathbf{J}
	\end{align}
	\item Where: $\mathbf{X}^i$ represent all two-body interactions $\{ S_j^i S_k^i\}_{j,k=1}^L$, and $i$ runs over the samples in the dataset.
	\item To complete analogy: can represent the dot product by a single index $p= \{ j, k \}$, i. e. $\mathbf{X}^i \cdot \mathbf{J} = X_p^i J_p$.
	\item Note the regression model doesn't include the minus sign.
\end{itemize}
In the following we apply ordinary least squares (OLS), Ridge and Lasso regression to the problem, and compare.
\begin{itemize}
	\item Fig 16 shows the $R^2$ (=coefficient of determination, a regression performance measure. Optimal performance: $R^2 = 1$) of the three regression models.
	\begin{align}
		R^2 = 1 - \frac{\sum_{i=1}^n |y_i^{true} - y_i^{pred}|^2 }{\sum_{i=1}^n |y_i^{true} - \frac{1}{n} \sum_{i=1}^n y_i^{pred}|^2 }
	\end{align}
	\item A few remarks:
	\begin{itemize}
		\item The regularization parameter $\lambda$ affects the Ridge and LASSo regressions at scales seperated by a few orders of magnitude. Therefore, considered good practice to always check performance for the given model and data as a func of $\lambda$. 
		\item While the OLS and Ridge regression test curves are monotonic, the LASSO test curve is not - suggesting an optimal LASSO regularization parameter is $\lambda \approx 10^{-2}$. At this sweet spot, the Ising interaction weights $\mathbf{J}$ contains only nearest-neighbour terms (as did the model the data was generated from).
	\end{itemize}
	\item Choosing between Ridge and LASSO in this case = similar to fixing gauge degrees of freedom. 
	\begin{itemize}
		\item Recall the uniform nearest-neighbour interaction strength $J_{j,k} = J$ which we used to generate the data, was set to unity, $J=1$. 
		\item Moreover, $J_{j,k}$ was NOT defined to be symmetric (we only used the $J_{j,j+1}$, but never the $J_{j,j-1}$ elements).
	\end{itemize}
	\item Fig 17 shows the matrix representation of the learned weights $J_{j,k}$. OSL and Ridge learn nearly symmetric weights $J \approx -0.5$. Not surprising, since it amounts to taking into account both the $J_{j,j+1}$ and $J_{j,j-1}$ terms, and the weights are distributed symmetrically between them.
	\item LASSO, on the other hand, tends to break this symmetry.
	\item Thus, we see how different regularization schemes can lead to learning equivalent models but in different "gauges". 
	\item Any info we have about symmetry of the unknown model that generated the data should be reflected in the definition of the model and the choice of regularization. 
\end{itemize}

\subsection{Convexity of a regularizer}
Mentioned previously the analytical LASSO solution can be found by invoking its convexity. We here provide an intro to \textbf{convexity theory}.
Recall
\begin{itemize}
	\item A set $C \subseteq \mathbb{R}^n$ is \textit{convex} if for any $x,y \in \mathcal{C}$ and $t \in [0,1]$,
	\begin{align}
		tx + (1-t)y \in C
	\end{align}
	Aka, every line segment joining $x, y$ lies entirely in $C$. A func $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is called convex if its domain, $dom(f)$, is a convex set, and for any $x,y \in dom(f)$ and $t\in[0,1]$ we have
	\begin{align}
		f(tx + (1-t)y) \leq tf(x) + (1-t)f(y)
	\end{align}

	That is, the func lies on or below the line segment joining its evaluation oat $x$ and $y$. This $f$ is called \textit{strictly convex} if this inequality holds strictly for $x \neq y$ and $t\in (0, 1)$.

	\item It turns out that \textit{for convex functions, any local minimizer is a global minimizer}.
	\item Algorithmically, this means that in the optimization procedure, as long as we are "going down the hill" and agree to stop when we reach the minimum, then we have hit the global minumum.
	\item In addition to this, there is an abundance of rich theory regarding convex duality and optimality, which allow us to understand the solutions even before solving the problem itself.
	\item Let's examine the two regularizers Ridge and Lasso. They are both convex problems, but only Ridge is a strictly convex problem (assuming $\lambda > 0$). 
	\item From convex theory this means we always have a unique sol for Ridge, but not necessarily for LASSO.
	\item It was recently shown that under mild conditions, the LASSO sol is indeed unique. Apart from this theoretical characterization, people have also introduced the notion of Elastic Net to retain the desirable properties of both LASSO and Ridge, which is now one of the standard tools for regression analysis and ML.
\end{itemize}


\subsection{Bayesian formulation of linear regression}
We formulate ls from a Bayesian point of view. We'll see that regularization in learning will emerge naturally as part of the Bayesian inference procedure.

From the linear regression setup, the data $\mathcal{D}$ used to fit the regression model is generated through $y = \mathbf{x}^T \mathbf{w} + \epsilon$. We often assume $\epsilon$ is a Gaussian noise w/men zero and variance $\sigma^2$. To connect linear regression to the Bayesian framework, we often write the model as
\begin{align}
	p(y|\mathbf{x}, \bm{\theta}) = \mathcal{N} (y| \mu(\bm{x}), \sigma^2(\bm{x}))
\end{align}
Aka, our regression model is defined by a conditional probability that depends not only on data $\bm{x}$, but on some model parameters $\bm{\theta}$. Fex, if the mean a linear func of $\bm{x}$ given by $\mu = \bm{x}^T \bm{w}$, and he variance is fixed $\sigma^2(\bm{x}) = \sigma^2$, then $\bm{\theta} = (\bm{w}, \sigma^2)$.

In statistics, many problems rely on estimation of some parameters of interest. Fex: suppose we're given the height data of 20 junior students from a regional high school, but we're interested in the average height of all high school juniors in the country. It's concievable the data we're given are not representative of the student population as a whole. = necessary to devise a systematic way to perform reliable estimation. We here present the \textbf{Maximum Likelihood Estimation} (MLE), and show that least squares can be derived from this framework. 

MLE is defined by
\begin{align}
	\hat{\bm{\theta}} = \text{argmax}_\theta \text{log} p(\mathcal{D} | \bm{\theta})
\end{align} 
Using the assumption that samples are i.i.d., we can write the \textit{log-likelihood} as
\begin{align}
	l (\bm{\theta}) \equiv \text{log}p(\mathcal{D} | \bm{\theta}) = \sum_{i = 1}^n \text{log} p(y_i | \bm{x}_i , \bm{\theta})
\end{align}
Inserting $p$ as defined above we get
\begin{align}
	l(\bm{\theta}) =& - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bm{x}_i^T \bm{w})^2 - \frac{n}{2} \text{log} (2\pi \sigma^2) \\
	=& - \frac{1}{2 \sigma^2} ||X \bm{w} - \bm{y}||_2^2 + \text{const}
\end{align}
By comparing this to the expression for $\hat{\bm{w}}_{LS}$ it's clear that performing least squares is the same as maximizing the log-likelihood of this model.

What about adding regularization? We earlier introduced the \textit{maximum a posteriori probability (MAP) estimate}. We now show it actually corresponds to regularized linear regression, where the choice of prior determines the regularization. Recall Baye's rule
\begin{align}
	p(\bm{\theta}| \mathcal{D}) \propto p(\mathcal{D} | \bm{\theta}) p(\bm{\theta})
\end{align} 

Now instead of maximizing the log-likelihood $l(\bm{\theta}) = \text{log} p(\mathcal{D} | \bm{\theta})$, let's maximize the log posterior $\text{log} p(\bm{\theta} | \mathcal{D})$. Invoking Baye's rules, the MAP estimator becomes
\begin{align}
	\hat{\bm{\theta}}_{MAP} \equiv \text{argmax}_\theta \text{log} p(\mathcal{D} | \bm{\theta}) + \text{log} p(\bm{\theta})
\end{align}

{\tiny
Review of Bayes terms:
\begin{itemize}
	\item Likelihood function = $p(\bm{X}|\bm{w})$
	\item Prior distribution = $p(\bm{w})$
	\item Posterior distribution = $p(\bm{w}|\bm{X})$
\end{itemize}
}

Suppose we use the Gaussian prior (= the \textit{conjugate prior} that gives a Gaussian poserior. For a given likelihood, conjugacy guarantees the preservation of prior distribution at the posterior level. Fex: for a Gaussion(Geometric) likelihood with a Gaussian(Beta) prior, the posterior distribution is still Gaussian(Beta) dist.) with zero mean and variance $\tau^2$, namely $p(\bm{w}) = \prod_j \mathcal{N}(w_j | 0, \tau^2)$, we can recast the MAP estimator into (constant terms that don't depend on the parameters have been dropped)
\begin{align}
	\hat{\bm{\theta}}_{MAP} &= \text{argmax}_\theta [- \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \bm{x}_i^T \bm{w})^2 -\frac{1}{2\tau^2} \sum_{j=1}^n w_j^2  ] \\ 
	&= \text{argmax}_\theta [- \frac{1}{2\sigma^2} ||X \bm{w} - \bm{y}||_2^2 -\frac{1}{2\tau^2} ||\bm{w}||_2^2  ]
\end{align}
The equivalence between MAP estimation with a Gaussian prior and Ridge regression is established by comparing this expression to the one for $\hat{\bm{w}}_{Ridge} (t)$ with $\equiv \sigma^2 / \tau^2 $. There's an analogous derivation for LASSO.


\subsection{Recap and a general perspective on regularizers}
In this section we
\begin{itemize}
	\item Explored least square regression with and without regularization
	\item Motivated the need for regularization due to poor generalization, in particular the "high-dimensional limit" ($p \gg n$).
	\item Instead of showing the in-sample and out-of-sample errors explicitly, we conducted numerical experiments in Notebook 3 on the diabetes dataset and showed that regularization typically leads to better generalization.
	\item Due to the equivalence between the constrained and penalized form of regularized regression (in LASSO and Ridge, but not generally true in cases such as $L^0$ penalization), we can regard the regularized regression problem as an un-regularized problem but on a constrained set of parameters. 
	\item Since the size of the allowed parameter space (e. g. $\bm{w} \in \mathbb{R}^p$ when un-regularized vs. $\bm{w} \in C \subset \mathbb{R}^p$ when regularized) is roughly a proxy for model complexity, solving the regularized problem is in effect solving the un-regularized problem with a smaller model complexity class = we're less likely to overfit.
	\item We also showed connection (useing a regularization function $\Leftrightarrow$ the use of priors in Bayesian inference). It gives us more intuition about why regularization implies we're less likely to overfit the data: 
	{\tiny
	\begin{itemize}
		\item Say you're a physics student taking a lab class. Experiment goal is to measure the behavior of different pendula and use it that to predict the formula/model that determines the period of oscillation. You would record many things (inc the lenght and mass) to give yourself the best possible chance of determining the unknown relationship, the room temperature, air currents, table vibrations etc. 
		\item = You have a high dimensional dataset. Even higher - you're aware of time of day, that it's Wednesday, your friends in attendance, the location, etc. You didn't write it down because of strongly held prior beliefs that none of those things affect the physics taking place in the room. 
		\item What's serving you here is the intuition that probably only a few things matter in the physics of pendula. You're approaching the experiment with prior beliefs about how many features you'll need to pay attention to in order to predict what'll happen when you swing an unknown pendulum. 
	\end{itemize}}
	Point being that we live in a high-dimensional world of info and while we have have good intuition in the pendulum case/well-known problems, often in the ML field we cannot say with any confidence a priori \textit{what} the small list of things to write down will be, but we can at least use regularization to help us enforce that the list not be too long so that we don't end up predicting that the pendulum's period depends on Bob having a cold on Wednesdays.
	\item In both LASSO and Ridge there is a parameter $\lambda$ involved. In principle, this \textbf{hyperparameter} is usually predetermined = not part of the regression process. We saw our learning performance and solution depends strongly on $\lambda$ = vital to choose it properly. As discussed, one approach is to assume an \textit{uninformative prior} on the hyperparameters, $p(\lambda)$, and average the posterior over all choices of $\lambda$ following this distribution. Hoewever, large computational cost. = simpler to choose the regularization parameter through some optimization procedure.
	\item Emphasize: linear regression can be applied to model non-linear relationship between input and response. Done by replacing the input $\bm{x}$ w/some nonlinear func $\phi(\bm{x})$. Doing so preserves the linearity as a func of the parameters $\bm{w}$, since model is defined by the inner product $\phi^T (\bm{x}) \bm{w}$. This method = \textit{basis function expansion}. 
	\item Recent years: surge of interest in understadning generalized linear regression models from a statistical physics perspective. Much has focused on understanding high-dimensional linear regression and compressed sensing. On a technical level, this research imports and extends the machinery of spin glass physics (replica method, cavity method, and message passing) to analyze high-dimensional linear models. A rich area of activity at the intersection of physics, computer science, information theory and ML.
\end{itemize}


\section{Logistic regression}
Have so far focused on continuous output. Now: classification: outcomes=discrete variables (i.e. categories). Fex, detect whether cat/dog in a picture. Or given a spin config, say the 2D Ising model, identify it's phase (e.g. ordered/disordered). Logistic regression deals with binary, dichotomous(= dividing into two parts) outcomes (e. g. true/false, success/failure, etc). Worth noting: logistic regression also commonly used in modern supervised deep learning models.
Section structure:
\begin{itemize}
	\item Define logistic regression and derive its corresponding cost function (the cross entropy) using a Bayesian approach and discuss its minimization.
	\item Generalize logistic regression to the case of multiple categories which is called \textit{Sofrmax regression}. 
	\item Demonstrate logistic regression via application to three different problems:
	\begin{itemize}
		\item Classifying phases of the 2D Ising model
		\item The SUSY dataset
		\item MNIST handwritten digit classification
	\end{itemize}
\end{itemize}

In this section, we consider the case where
\begin{itemize}
	\item The dependent variables $y_i \in \mathbb{Z}$ are discrete and only take values from $m=0, ..., M-1$ (i.e. M classes)
	\item Goal: predict the output classes from the design matrix $X \in \mathbb{R}^{n\times p}$ made of $n$ samples, each of which bears $p$ features. Of course primary goal = identify the classes to which new unseen samples belong.
\end{itemize}
Helpful to consider a slightly simple classifier before logistic regression:
\begin{itemize}
	\item A linear classifier that categorizes examples using a weighted linear-combination of the features and an additive offset:
	\begin{align}
 		s_i = \bm{x}_i^T \bm{w} + b_0 \equiv \text{\textbf{x}}_i^T \text{\textbf{w}}
 	\end{align}
 	where $\text{\textbf{x}}_i = (1, \bm{x}_i)$ and $\text{\textbf{w}}_i = (b_0, \bm{w}_i)$.
 	\item This func takes values on the entire real axis. With logistic regression however the labels $y_i$ are discrete variables. 
 	\item One simple way to get a discrete output is to have sign functions that map the output of a linear regressor to $\{ 0, 1 \}$, $f(s_i) = sign(s_i) = 1$ if $s_i \geq 0$ and 0 otherwise. 
 	\item = The "perceptron" in ML.
 \end{itemize}
 Extremely simple model, and it's favorable in many cases (e.g. noisy data) to have a "soft" classifier that outputs the probability of a given category:
 \begin{itemize}
 	\item Fex: Given $\text{\textbf{x}}_i$, the classifier outputs the probability of being in category $m$.
 	\item One such func: The logistic (or \textbf{sigmoid}) function:
 	\begin{align}
 		f(s) = \frac{1}{1+e^{-s}}
 	\end{align}
 	Note that $1-f(s) = f(-s)$ will be useful shortly.
\end{itemize}

\subsection{The cross-entropy as a cost function for logistic regression}
\begin{itemize}
	\item Perceptron = example of a "hard classification": each datapoint deterministically assigned to a category.
	\item In many cases favorable to have a "soft" classifier that outputs the probability of a given category rather than a single value. Logistic regression is the most canonical example of a soft classifier. 
	\item In logistic regression, the prob that a data point $\bm{x}_i$ belongs to a category $y_i = \{ 0, 1\}$ is given by
	\begin{align}
		P(y_i = 1|\bm{x}_i, \bm{\theta}) &= \frac{1}{1 + e^{-\text{\textbf{x}}_i^T \text{\textbf{w}}}} \\
		P(y_i = 0|\bm{x}_i, \bm{\theta}) &= 1 - P(y_i = 1|\bm{x}_i, \bm{\theta})
	\end{align}
	where $\bm{\theta} = \text{\textbf{w}}$ are the weights we wish to learn from the data. 
	\item For intuition: 
	\begin{itemize}
		\item Consider a collection of non-interacting two-state systems coupled to a thermal bath (e.g. a collection of atoms that can be in two states).
		\item Denote the state of system $i$ by a binary variable: $y_i \in \{ 0, 1\}$. 
		\item From elementary statistical mechanics, we know that if the two states have energies $\epsilon_0$ and $\epsilon_1$ the prob for finding the system in a state $y_i$ is just:
		\begin{align}
			P(y_i = 0) &= \frac{e^{-\beta \epsilon_0}}{e^{-\beta \epsilon_0} + e^{-\beta \epsilon_1}} = \frac{1}{1 + e^{-\beta \Delta \epsilon}} \\
			P(y_i = 1) &= 1 - P(y_i = 0)
		\end{align}
		\item Notice in these expressions, as is often the case in physics, only energy differences are observable. 
		\item If the difference in energies between two states is given by $\Delta \epsilon = \text{\textbf{x}}_i^T \text{\textbf{w}}$, we recover the expressions for logistic regression.
	\end{itemize}
	\item We'll use this mapping between partition functions and classification to generalize the logistic regressor to soft-max regression later this section.
	\item Notice in terms of the logistic func, we can write
	\begin{align}
		P(y_i = 1) = f(\text{\textbf{x}}_i^T \text{\textbf{w}}) = 1-P(y_i = 0)
	\end{align}

	\item We now define the cost func for logistic regression usng Maximum Likelihood Estimation (MLE). In MLE we choose parameters to maximize the prob of seeing the observed data.
	\begin{itemize}
		\item Consider a dataset $\mathcal{D} = \{ (y_i , \bm{x}_i)\}$ w/binary labels  $y_i \in \{ 0, 1\}$ where the data points are drawn independently.
		\item The likelihood of seeing the data under our model is
		\begin{align}
			P(\mathcal{D}|\text{\textbf{w}}) = \prod_{i=1}^n [f(\text{\textbf{x}}_i^T \text{\textbf{w}})]^{y_i} [1 - f(\text{\textbf{x}}_i^T \text{\textbf{w}})]^{1-y_i}
		\end{align} 
		from which we compute the log-likelihood
		\begin{align}
			l(\text{\textbf{w}}) &= \text{log}P(\mathcal{D}|\text{\textbf{w}}) \\
			&= \sum_{i=1}^n y_i \text{log} f(\text{\textbf{x}}_i^T \text{\textbf{w}}) + (1-y_i) \text{log} [1 - f(\text{\textbf{x}}_i^T \text{\textbf{w}})]
		\end{align}
		\item The maximum likelihood estimator = the set of parameters that maximize the log-likelihood:
		\begin{align}
			\hat{\text{\textbf{w}}}
			= \text{argmax}_\theta \sum_{i = 1}^n y_i \text{log} f(\text{\textbf{x}}_i^T \text{\textbf{w}}) + (1-y_i) \text{log} [1 - f(\text{\textbf{x}}_i^T \text{\textbf{w}})]
		\end{align}

		\item Since the cost (error) func = the negative log-likelihood, we have
		\begin{align}
			\mathcal{C} =& -l (\text{\textbf{w}}) \\
			=& \sum_{i=1}^n - y_i \text{log} f(\text{\textbf{x}}_i^T \text{\textbf{w}}) - (1-y_i) \text{log} [1 - f(\text{\textbf{x}}_i^T \text{\textbf{w}})]
		\end{align}
		This is known in statistics as the \textit{cross-entropy}.
		\item Final note: Just as in linear regression we usually supplement the cross-entropy with additional regularization terms, usually $L_1$ and $L_2$ regularization.
	\end{itemize}
\end{itemize}

\subsection{Minimizing the cross entropy}
Cross entropy = a convex function of the weights $\text{\textbf{w}}$ = any local minimizer is a global minimizer. Minimizing the cost func leads to the equation
\begin{align}
	\bm{0} = \bm{\nabla} \mathcal{C} (\text{\textbf{w}}) = \sum_{i=1}^n [f(\text{\textbf{x}}_i^T \text{\textbf{w}}) - y_i] \text{\textbf{x}}_i
\end{align}
where used the logistic func identity $\partial_z f(z) = f(z)[1 - f(z)]$. This equation defines a transcendental equation for $\text{\textbf{w}}$, the solution of which, unlike linear regression, cannot be written on a closed form. $\rightarrow$ Must use numerical methods such as introduced in the GD chapter to solve this optimization problem.

\subsection{Examples of binary classification}
\subsubsection{Identifying the phases of the 2D Ising model}
\begin{itemize}
	\item Goal: show how to employ logistic regression to classify the states of the 2D Ising model according to their phase of matter.
	\item Hamiltonian for the classical Ising model:
	\begin{align}
		H = -J \sum_{\langle ij \rangle} S_i S_j, \quad S_j \in \{ \pm 1 \}
	\end{align}
	where the lattice site indices $i, j$ run over all nearest neighbors of a 2D square lattice, and $J$ is an interaction energy scale. Adopt periodic boundary conditions. 
	\item Onsager proved this model undergoes a phase transition in the thermodynamic limit from an ordered ferromagnet with all spins alligned to a disordered phase at the critical temperature $T_c/J = 2/\text{log} (1 + \sqrt{2}) \approx 2.26$. For any finite system size, this critical point is expanded to a critical region around $T_c$. 
	\item Can one train a statistical classifier to distinguish between the two phases of the Ising model? This could be used to locate the position of the critical point in more complicated models.
	\item Given an Ising state, we would like to classify whether it belongs to the ordered or disordered phase, without other info than the spin config itself. This categorical ML problem is well suited for logistic regression and will thus consist of recognizing whether a given state is ordered by looking at its bit configs.
	\item For the purposes of applying logistic regression, the 2D spin state will be flattened out to a 1D array, so won't be possible to learn info on the structure of the contiguous (=sharing a common border/touching) ordered 2D domains. Such info can be incorporated using deep CNNs, see later chapter.
	\item We consider a 2D Ising model on a $40\times 40$ square lattice, and use Monte-Carlo (MC) sampling to prepare $10^4$ states at every fixed temperature $T$ out of a pre-defined set. We also assign a label to each state according to its phase: 0=disordered, 1=ordered.
	\item Near $T_c$ the ferromagnetic correlation length diverges, leading to, amongst other things, critical slowing of the MC algo. Identifying phases could also be harder in this region perhaps. With this in mind, consider the following three types of states:
	\begin{itemize}
		\item Ordered ($T/J < 2.0 $)
		\item Near-critical ($2.0 \leq T/J \leq 2.5 $)
		\item Disordered ($T/J > 2.5$)
	\end{itemize}
	Use ordered and disordered states to train, then evaluate the performance on unseen ordered, disordered and near-critical states.

	\item We deploy the \textit{liblinear} routine (= default for Scikit's logistic regression) and stochastic gradient descent (SGD) to optimize the logistic regression cost func with $L_2$ regularization.
	\item We define accuracy of the classifier = the percentage of correctly classified data points. Comparing the accuracy on the training and test data we can study the degree of overfitting. 
	\item See Fig 21: Notice the small degree of overfitting as suggested by the training and testing accuracy curves being close together.
	\item The liblinear minimizer outperforms SGD on the training and test data, but not on the near-critical data for certain values of the regularization strength $\lambda$.
	\item Similar to the linear regression examples, we find there exists a sweet spot for the SGD regularization strength $\lambda$ that results in optimal performance of the logistic regressor, at about $\lambda \approx 10^{-1}$
	\item Does the difficulty of the phase recognition depend on the temp of the queried sample? For states in the near-critical temp region (see fig 20) it's no longer easy for the human eye to distinguish between ordered and disordered $\rightarrow$ interesting to compare the training and test accuracies to the accuracy of the near-critical state predictions (recall: model has not been trained on near-critical states). Indeed the liblinear accuracy is about 7\% smaller for the critical states compared to the test data.
	\item Important to note all of Scikit's logistic regreesion solvers have in-built regularizers. Crucial in order to prevent overfitting. 
\end{itemize}

\subsubsection{SUSY}
\begin{itemize}
	\item In high energy physics experiments we hope to discover new particles. Need to sift through events and classify as either a signal of a new process or particle, or as a background event from already understood Standard Model processes. We don't know for sure what process has occured - only know the final state particles. But, we can try to determine parts of phase space that will have a high percentage of signal events. 
	\item Typically done by using a series of simple requirements on the kinematic quantities of the final state particles, fex having one or more leptons w/large momentums that are transverse to the beam line ($p_T$).
	\item Instead, here we'll use logistic regression in an attempt to find the relative prob that an event is from a signal or background event. Rather than using the kinematic quantities of final state particles directly, we'll use the output of our logistic regression to define a part of phase space that is enriched in signal events.
	\item The dataset has been produced using Monte Carlo simulations to contain events with two leptons (electrons or muons). Each event has the value of 18 kinematic variables ("features"): first 8 = direct measurements of final state particles (in this case the $p_T$, pseudo-rapidity $\eta$, and azimuthal angle $\phi$ of two leptons in the event and the amount of missing transverse momentum (MET) together with its azimuthal angle), last 10 = functions of the first 8 - high level features derived by physicists to help discriminate between the two classes. These high level features can be viewed as the physicist's attempt to use non-linear functions to classify the events, having been developed with formidable theoretical effort. We'll later revisit this problem with the tools of Deep Learning.
	\item Since we don't know the true underlying process, so our goal in these types of analysis is to find regions enriched in signal events. If we find an excess of events above what is expected, we can have confidence they come from the signal type we're searching for. $\rightarrow$ The two metrics of import are
	\begin{itemize}
		\item The efficiency of signal selection
		\item The background rejection achieved
	\end{itemize} 
	\item Often rather than thinking about just a single working point, performance is characterized by Reciever Operator Curves (or ROC curves). They plot signal efficiency (true-positives) vs. background rejection (true-negatives) as a func of some continuous variable such as a threshold. Here that variable will be the output signal probability of our logistic regression. 
	\item Fig 22 shows examples of these outputs (Using $L^2$ regularization w/ a regularization parameter of $10^{-5}$): x-axis=logsitic regression model's output "probability of this being a true signal event", y-axis=frequency. Two figures: one where the actual event inputs are signal events (meaning the p=1 point on the x-axis counts true-positive cases, while the p=0 point counts false-negative cases (Type II error)) and one plot where the event inputs are background and not signal (meaning the p=1 point on the x-axis counts false-positive cases (Type I error), while the p=0 point counts true-negative cases). $\rightarrow$ Some signals events even look background like (false-false), and some background events look signal like (false-true) = further reason to characterize performance in terms of ROC curves.
	\item Fig 23: Examples of ROC curves using $L^2$ regularization for many different regularization parameters using either TensorFlow (top) or SciKit learn (bottom) when using the full set of 18 input variables. Notice
	\begin{itemize}
		\item Minimal overfitting, partly because such a large training dataset (4.5 million events). How do they see this from the ROC curves..??
		\item More importantly, the underlying data we're working with: each input variable is an important feature.
	\end{itemize}
	\item Is there utility to this increased sophistication?
	\begin{itemize}
		\item Recall, even to the learning algo, some signal events and background events look similar. Can illustrate this by a plot comparing the $p_T$ spectrum of the leading and subleading leptons for both signal and background events. Fig 24 shows these two distributions. While \textit{some} signal events are easily distinguished, many live in the same part of phase space as the background.  This effect can also be seen in Fig 22.
		\item How much discrimination power is obtained by simply putting different requirements on the input variables rather than using ML techniques? In order to compare this (called cut-based strategy in the HEP field) to regression, different ROC curves have been made for 
		\begin{itemize}
			\item Logistic regression with just the simple kinematic variables, 
			\item Logistic regression w/the full set of variables, and 
			\item Just putting requirements on the leading lepton $p_T$
		\end{itemize}
		\item Fig 25 shows a clear performance benefit from using logistic regression.
		\item Note in the cut-based approach we have only used one variable where we could have put requirements on all of them. While putting more requirements would increase background rejection, it would also decrease signal efficiency. $\rightarrow$ The cut-based approach will never yield as strong discrimination as logistic regression.
		\item Also interesting in fig 25 - the higher-order variables noticably help the ML techniques. 
	\end{itemize}
\end{itemize}

\subsection{Softmax regression}
\begin{itemize}
	\item We generalize here from binary to multi-class classification.
	\item One approach is: treat the label as a vector $y_i \in \mathbb{Z}_2^M$ = a binary bit string of length $M$. Fex $\bm{y}_i = (1,0,...,0)$ means the sample $\bm{x}_i$ belongs to class 1.
	\item The prob of $\bm{x}_i$ being in class $m'$ is given by
	\begin{align}
		P(y_{im'} = 1|\bm{x}_i, \bm{\theta}) = \frac{e^{-\text{\textbf{x}}_i^T \text{\textbf{w}}_{m'}}}{\sum_{m=0}^{M-1} e^{-\text{\textbf{x}}_i^T \text{\textbf{w}}_m}}
	\end{align} 
	where $y_{im'} \equiv [\bm{y}_i]_{m'}$ refers to the $m'$-th component of vector $\bm{y}_i$. This is the \textbf{softmax} function.
	\item Therefore, the likelihood of this $M$-class classifier is simply
	\begin{align}
		P(\mathcal{D}| \{ \bm{w}_k \}_{k=0}^{M-1}) 
		=& \prod_{i=1}^n \prod_{m=0}^{M-1} [P(y_{im} = 1| \bm{x}_i, \text{\textbf{w}}_m)]^{y_{im}} \nonumber \\ 
		&\times [1 - P(y_{im} = 1| \bm{x}_i, \text{\textbf{w}}_m)]^{1 - y_{im}}
	\end{align}
	\item From this we similarly define the cost func:
	\begin{align}
		E(\text{\textbf{w}}) =& - \sum_{i=1}^n \sum_{m=0}^{M-1} y_{im} \text{log} P(y_{im} = 1| \bm{x}_i, \text{\textbf{w}}_m) \nonumber \\
		&+ (1 - y_{im}) \text{log} (1 - P(y_{im} = 1| \bm{x}_i, \text{\textbf{w}}_m))
	\end{align}
	As expected, for $M=1$ we recover the cross entropy for logistic regression.
\end{itemize}



\subsection{An example of SoftMax classification: MNIST digit classification}
\begin{itemize}
	\item The MNIST dataset: 70000 handwritten digits, each of which is laid out on a $28 \times 28$-pixel grid.
	\item Every pixel assumes one of 256 grayscale values, interpolating between white and black.
	\item 10 categories for the digits 0 through 9 $\rightarrow$ SoftMax regression with $M=10$
	\item Fig 26 shows the learned weights $\bm{w}_k$ where $k$ corresponds to class labels (i.e. digits). 
	\item We'll come back to SoftMax in chapter 9.
\end{itemize}


\section{Combining models}
\begin{itemize}
	\item One of most powerful and widely applied ideas in modern ML: ensamble methods = combine predictions from multiple, often weak, statistical models. They undergird many of the winning entries in data science competitions such as Kaggle, especially on structured datasets (neural networks generally perform better than enasmble methods on unstructured data, images, and audio).
	\item Even in neural nets context, it's common to combine predictions from multiple nerual nets to increase performance on tough image classification tasks.
	\item On one hand, idea of training multiple models, then using a weighted sum of their predictions, very natural. On the other hand, can imagine the ensamble predictions can be much worse than that of the individual models - especially when pooling reinforces weak but correlated deficiencies in each individual predictor. $\rightarrow$ Important to understand when we expect ensamble methods to work.
	\item To that end, will revisit the bias-variance tradeoff, generalize it to an ensamble of classifiers. Will show key to determine when ensamble methods work = the degree of correlation between the models in the ensamble.
\end{itemize}

\subsection{Revisiting the bias-variance tradeoff for ensembles}
\begin{itemize}
	\item This tradeoff summarizes the fundamental tension in ML between (model complexity $\Leftrightarrow$ amount of training data needed to fit it).
	\item Key property emerging from this analysis: the correlation between models that constitue the ensamble. Important for two distinct reasons:
	\begin{itemize}
		\item  Holding the ensemble size fixed, averaging the predicitons of correlated models reduces the variance less than averaging uncorrelated models.
		\item In some cases, correlations between models within an ensemble can result in an \textit{increase} in bias, offsetting any potential reduction in variance gained from ensemble averaging. We'll discuss this in the context of bagging below. One of the most dramatic examples of increased bias from correlations = the catastrophic predictive failure of almost all derivative models used by Wall Street during 2008 financial crisis.
	\end{itemize}
\end{itemize}

\subsubsection{Bias-variance decomposition for ensembles}
\begin{itemize}
	\item We'll discuss bias-variance tradeoff in context of continuous predictions such as regression, but many ideas carry over to classification tasks.
	\item Review of the bias-variance tradeoff in context of a single model:
	\begin{itemize}
		\item $\bm{X}_\mathcal{L} = \{ (y_j , \bm{x}_j), j=1...N \}$ = data in dataset
		\item $y= f(\bm{x}) + \epsilon$ = noisy model from which we assume the true data is generated from. $\epsilon$ = normally distributed w/mean zero and standard deviation $\sigma_\epsilon$. 
		\item $\hat{g}_\mathcal{L} (\bm{x})$ = a predictor formed by a statistical procedure we assume we have, e.g. least squares regression. The predictor gives our model's prediction using a dataset $\mathcal{L}$. 
		\item This estimator is chosen by minimizing a cost func which, for the sake of correctness, we take to be the squared error
		\begin{align}
			\mathcal{C} (\bm{X}, g(\bm{x})) = \sum_i (\bm{y}_i - \hat{g}_\mathcal{L} (\bm{x}_i))^2
		\end{align}
		\item $\mathcal{L}$ = dataset drawn from some underlying distribution that describes the data.
		\item $\{ \mathcal{L}_j \}$ = many different datasets of the same size as $\mathcal{L}$ drawn from this distribution.
		\item The corresponding $\hat{g}_{\mathcal{L}_j} (\bm{x})$ will differ from each other due to stochastic effects arising from the sampling noise. $\rightarrow$ Can view our estimator $\hat{g}_\mathcal{L} (\bm{x})$ as a random variable (technically functional) and define an expectation value $E_\mathcal{L}$ the usual way.
		\item $E_\mathcal{L}$ = expectation value computed by drawing infinitely many different datasets $\{ \mathcal{L}_j \}$ of the same size, fitting the corresponding estimator, then averaging over the results. 
		\item $E_\epsilon$ = the expectation value over different instances of the "noise" $\epsilon$.
		\item Can then decompose the expected generalization error as
		\begin{align}
			E_{\mathcal{L}, \epsilon} [\mathcal{C} (\bm{X}, g(\bm{x}))] = Bias^2 + Var + Noise
		\end{align}
		where the bias=the deviation of the expectation value of our estimator (=the asymptotic value of our estimator in the limit of infinite data) from the true value:
		\begin{align}
			Bias^2 = \sum_i (f(\bm{x}_i) - E_\mathcal{L} [\hat{g}_\mathcal{L} (\bm{x}_i)])^2
		\end{align}
		The variance = how much our estimator fluctuates due to finite-sample effects:
		\begin{align}
			Var = \sum_i E_\mathcal{L} [(\hat{g}_{\mathcal{L}} (\bm{x}) - E_\mathcal{L} [\hat{g}_{\mathcal{L}} (\bm{x})] )^2]
		\end{align}
		The noise term = part of the error due to intrinsic noise in the data generation process that no statistical estimator can overcome:
		\begin{align}
			Noise = \sum{\sigma_{\epsilon_i}^2}
		\end{align}
	\end{itemize}
\end{itemize}
Let's generalize this to ensembles of estimators:
\begin{itemize}
	\item $\bm{X}_\mathcal{L}$ = a given dataset
	\item $\theta$ = given hyper-parameters that parametrize member of our ensemble
	\item We'll consider a procedure that deterministically generates a model $\hat{g}_\mathcal{L} (\bm{x}_i , \theta)$ given $\bm{X}_\mathcal{L}$ and $\theta$.
	\item Assume $\theta$ includes some random parameters that introduce stochasticity into our ensemble (e.g. an initial condition for stochastic gradient descent or a random subset of features or data points used for training)
	\item We'll be concerned with the expected prediction error of the \textit{aggregate ensemble predictor}
	\begin{align}
		\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta \}) = \frac{1}{M} \sum_{m=1}^M \hat{g}_\mathcal{L} (\bm{x}_i , \theta_m)
	\end{align}
	\item For future reference we'll define the mean, variance, covariance (= the connected correlation function in the language of physics) and the normalized correlation coefficient wrt $\theta$ of the estimators in our ensemble as
	\begin{align}
		E_\theta [\hat{g}_\mathcal{L} (\bm{x} , \theta)] =& \mu_{\mathcal{L}, \theta} (\bm{x}) \nonumber \\
		E_\theta [\hat{g}_\mathcal{L} (\bm{x} , \theta)^2] - E_\theta [\hat{g}_\mathcal{L} (\bm{x} , \theta)]^2 =& \sigma_{\mathcal{L}, \theta}^2 (\bm{x}) \nonumber \\
		E_\theta [ \hat{g}_\mathcal{L} (\bm{x} , \theta_m) \hat{g}_\mathcal{L} (\bm{x} , \theta_{m'})] - E_\theta [\hat{g}_\mathcal{L} (\bm{x} , \theta_m)]^2 =& C_{\mathcal{L}, \theta_m, \theta_{m'}} (\bm{x}) \nonumber \\
		\rho(\bm{x}) =& \frac{C_{\mathcal{L}, \theta_m, \theta_{m'}} (\bm{x})}{\sigma_{\mathcal{L}, \theta}^2}
	\end{align}
	By definition we assume $m \neq m'$ in $C_{\mathcal{L}, \theta_m, \theta_{m'}}$.
\end{itemize}
We can now ask about the expected generalization (out-of-sample) error for the ensemble
\begin{align}
	E_{\mathcal{L}, \epsilon, \theta} [\mathcal{C} (\bm{X}, \hat{g}_\mathcal{L}^A (\bm{x}))] = E_{\mathcal{L}, \epsilon, \theta} [\sum_i (\bm{y}_i - \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2]
\end{align}

As in the single estimator case, we decompose the error into a noise, bias and variance term. To see this, note that

\begin{align}
	E_{\mathcal{L}, \epsilon, \theta} [\mathcal{C} (\bm{X}, \hat{g}_\mathcal{L}^A (\bm{x}))] 
	=& E_{\mathcal{L}, \epsilon, \theta} [\sum_i (\bm{y}_i -f(\bm{x}_i) + f(\bm{x}_i)- \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2] \\
	=& \sum_i E_{\mathcal{L}, \epsilon, \theta} [ (\bm{y}_i -f(\bm{x}_i))^2 + (f(\bm{x}_i)- \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2 \nonumber \\
	&+ 2(\bm{y}_i -f(\bm{x}_i))(f(\bm{x}_i)- \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})) ] \\
	=& \sum_i \sigma_{\epsilon_i}^2 + \sum_i E_{\mathcal{L}, \theta} [(f(\bm{x}_i)- \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2]
\end{align}
where in the last line we used that $E_\epsilon [y_i] = f(\bm{x}_i)$ (why is only $\theta$ in the subindex of the $E$ here?? How do the arithmetics of expec. values over several variables work? well probably the logical way, i.e. $E_{\mathcal{L}, \epsilon, \theta} = (E_\mathcal{L} + E_\epsilon + E_\theta)/3$ and probably $\epsilon$ was the only one they bothered to include here b.c. $\epsilon$ is the only variable we would expect $y= f(\bm{x}) + \epsilon$ to change over/be affected by) to eliminate the last term. Further decompose the second term as
\begin{align}
	E_{\mathcal{L}, \theta} [(f(\bm{x}_i)- \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2] 
	=& E_{\mathcal{L}, \theta} [(f(\bm{x}_i)- E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] \nonumber \\
	&+ E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] -   \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2] \\
	=& E_{\mathcal{L}, \theta} [(f(\bm{x}_i)- E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] )^2] \nonumber \\
	&+ E_{\mathcal{L}, \theta} [(E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] -   \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))^2] \nonumber \\
	&+ 2E_{\mathcal{L}, \theta} [(f(\bm{x}_i)- E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] ) \nonumber \\
	&\quad (E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] -   \hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}))] \\
	=& (f(\bm{x}_i)- E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})] )^2 \nonumber \\
	&+E_{\mathcal{L}, \theta} [(\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\}) - E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}_i, \{ \theta\})  ] )^2] \\
	=& Bias^2(\bm{x}_i) + Var(\bm{x}_i)
\end{align}
where we defined the bias and variance of the aggregate predictor as
\begin{align}
	Bias^2 (\bm{x}) \equiv & (f(\bm{x})- E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\})] )^2 \\
	Var(\bm{x}) \equiv & E_{\mathcal{L}, \theta} [(\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\}) - E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\})  ] )^2] 
\end{align}
So far, ensemble calc almost identical to that of a single estimator. But, since the aggregate estimator is a sum of estimators, its variance implicitly depends on the correlations between the individual estimators in the ensemble. Using the definition of the aggregate estimator $\hat{g}_\mathcal{L}^A$ and those of the mean, variance, covariance and normalized correlation coefficients given earlier, we get
\begin{align}
	Var(\bm{x}) =& E_{\mathcal{L}, \theta} [(\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\}) - E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\})  ] )^2] \\
	&= \frac{1}{M}[\sum_{m, m'} E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \theta_m) \hat{g}_\mathcal{L}^A (\bm{x},\theta_{m'})] - M^2 \sum_i [\mu_{\mathcal{L}, \theta} (\bm{x})]^2  ] \\
	&= \rho(\bm{x}) \sigma_{\mathcal{L}, \theta}^2 + \frac{1- \rho(\bm{x})}{M}  \sigma_{\mathcal{L}, \theta}^2
\end{align}

This formula = key to udnerstand the power of random ensembles. By using large ensembles ($M \rightarrow \infty $), can significantly reduce the variance, and for completely random ensembles where the models are uncorrolated ($\rho(\bm{x}) = 0$), maximally surpress the variance! $\rightarrow$ Using the aggregate predictor beats down fluctuations due to finite-sample effects. Key = decorrelate the models as much as possible while still using a very large ensemble. 

Can be worried this comes at the expense of a large bias. But no. When models in the ensemble completely random, the bias of the aggregate predictor = the expected bias of a single model:
\begin{align}
	Bias^2(\bm{x}) =& (f(\bm{x}) - E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \{ \theta\})])^2 \\
	=& (f(\bm{x})- \frac{1}{M} \sum_{m=1}^M E_{\mathcal{L}, \theta} [\hat{g}_\mathcal{L}^A (\bm{x}, \theta_m)] )^2 \\
	=& (f(\bm{x}) - \mu_{\mathcal{L}, \theta} )^2
\end{align}
$\rightarrow$ for a random ensemble one can always add more models without increasing the bias. This observation lies behind the immense power of random forest methods. For other methods inc bagging, we'll see that the bootstrapping procedure actually does increase the bias - but in many cases the increase is negligible compared to the reduction in variance.

\subsubsection{Summarizing the theory and intuitions behind ensembles}
Three distinct shortcomings fixed by ensemble methods: statistical, computational, representational. 
\begin{itemize}
	\item Statistical: When learning set too small, learning algo can typically find several models in the hypothesis space $\mathcal{H}$ that all give the same performance on training data. Provided their predictions uncorrelated, averaging several models reduces risk of choosing wrong hypothesis.
	\item Computational: Many learning algos rely on some greedy assumption/ local search that may get stuck in local optima. $\rightarrow$ An ensemble made of individual models built from many different starting points may provide a better approximation of the true unknown function than any of the single models.
	\item Representational: In most cases, for a learning set of finite size, the true func cannot be represented by any of the candidate models in $\mathcal{H}$. By combining several models in an ensemble, may be possible to expand the space of representable funcs and better model the true func.
\end{itemize}
Combining models may come at the price of introducing mroe parameters to our learning procedure. But if the problem itself can never be learned through a simple hypothesis, then no reason to avoid applying a more complex model. Ensemble methods reduce the variance and often easier to train than a single complex model = powerful way of increasing representational power/expressivity.

Our analysis gives several intuitions for how we should construct ensembles. 
\begin{itemize}
	\item Should try to randomize ensemble construction as much as possible to reduce correlations between predictors in the ensemble. Ensures our variance reduced while minimizing increase in bias due to correlated errors.
	\item The ensembles will work best for procedures where the error of the predictor is dominated by the variance and not the bias. $\rightarrow$ These methods are especially well suited for unstable procedures whose results sensitive to small changes in the training data.
\end{itemize}
The ideas that using an ensemble allows us to reduce variance and that the procedure works best for unstable predictors which errors are dominated by variance caused by finite sampling rather than bias, may be carried over to classification tasks, even if this section has focused on continuous predictors such as regression.

\subsection{Bagging}
BAGGing = Bootstrap AGGregation = one of the simplest and most used ensemble methods.
Imagine we have
\begin{itemize}
	\item $\mathcal{L}$ = a very large dataset that we could partition into $M$ smaller datasets which we label $\{\mathcal{L}_1, ..., \mathcal{L}_M \}$. If each partition is sufficiently large to learn a predictor, we can create an ensemble aggregate predictor, composed of predictors trained on each subset of the data.
	\item For continuous predictors like regression, this is just the average of all the individual predictors:
	\begin{align}
		\hat{g}_\mathcal{L}^A = \frac{1}{M} \sum_{i=1}^M g_{\mathcal{L}_i} (\bm{x})
	\end{align}
	\item For classification tasks where each predictor predicts a class label $j \in \{ 1, ..., J \}$, this is just the majority vote of all the predictors,
	\begin{align}
		\hat{g}_\mathcal{L}^A (\bm{x}) = \text{argmax}_j \sum_{i=1}^M I[g_{\mathcal{L}_i} (\bm{x}) = j]
	\end{align}
	where $I[g_{\mathcal{L}_i} (\bm{x}) = j]$ is an indicator func that is equal to one if $g_{\mathcal{L}_i} (\bm{x}) = j$ and zero otherwise. From our theoretical discussion we know this may significantly reduce the variance without increasing the bias.
	\item This form of aggregation clearly only works if enough data in each partition set $\mathcal{L}_i$. Consider the extreme limit where $\mathcal{L}_i$ contains exactly one point. In this case, the base hypothesis $g_{\mathcal{L}_i} (\bm{x})$ (e.g. linear regressor) becomes extremely poor and the procedure above fails. One way to circumvent this shortcoming: resort to \textbf{empirical bootstrapping}, a resampling technique in statistics. Idea is to use sampling w/replacement to create new "bootstrapped" datasets $\{\mathcal{L}_1^{BS}, ..., \mathcal{L}_M^{BS} \}$ from our original datasets $\mathcal{L}$. These bootstrapped datasets share many points, but due to the sampling w/replacement, are all somewhat different from each other.
	\item In the bagging procedure, we create an aggregate estimator by replacing the $M$ independent datasets by the $M$ bootstrapped estimators:
	\begin{align}
		\hat{g}_\mathcal{L}^{BS} (\bm{x}) = \frac{1}{M} \sum_{i=1}^M g_{\mathcal{L}_i^{BS}} (\bm{x})
	\end{align}
	and
	\begin{align}
		\hat{g}_\mathcal{L}^{BS} (\bm{x}) = \text{argmax}_j \sum_{i=1}^M I [g_{\mathcal{L}_i^{BS}} (\bm{x}) = j]
	\end{align}
	\item This bootstrapping procedure allows us to construct an approximate ensemble and thus reduce variance. For unstable predictors, this can significantly improve performance. 
	\item Price we pay for using bootstrapped training datasets is an increase in the bias of our bagged estimators: note that as the number of datasets $M \rightarrow \infty$, the expectation wrt the bootstrapped samples converges to the empirical distribution describing the training data set $p_\mathcal{L}(\bm{x})$ (e.g. a delta func at each datapoint in $\mathcal{L}$) = in general different from the true generative distribution for the data $p(\bm{x})$.
	\item Fig 29: Bagging with a perceptron (linear classifier) as the base classifier that constitutes the elements of the ensemble. See that although each individual classifier in the ensemble performs poorly, bagging these estimators yields reasonably good performance. Raises questions like \textit{why bagging works} and \textit{how many bootstrap samples are needed}. As mentioned,
	\begin{itemize}
		\item Bagging is effective on "unstable" learning algos = small changes in the training set result in large changes in predictions.
		\item When the procedure is unstable, the prediction error is dominated by the variance and one can exploit the aggregation component of bagging to reduce the prediction error.
		\item In contrast, for a stable procedure the accuracy is limited by the bias introduced by using bootstrapped datasets.
		\item = There is an instability-stability transition point beyond which bagging stops improving our prediction.
	\end{itemize}
\end{itemize}
Brief introduction to bootstrapping:
\begin{itemize}
	\item $\mathcal{D} = \{ X_1, ..., X_n \}$ = a finite set of $n$ data points given as training samples
	\item Our job: construct measures of confidence for our sample estimates (e.g. the confidence interval, or mean-squared error of sample median estimator).
	\item One then first samples $n$ points \textbf{with replacement} from $\mathcal{D}$ to get a new set $\mathcal{D}^{\star (1)} = \{ X_1^{\star (1)}, ...,  X_n^{\star (1)} \}$, called a \textbf{bootstrap sample}, which possibly contains repetitive elements.
	\item Then repeat the same procedure to get in total $B$ such sets:
	$\mathcal{D}^{\star (1)}, ..., \mathcal{D}^{\star (B)}$.
	\item Next step: use these $B$ bootstrap sets to get the \textbf{bootstrap estimate} of the quantity of interest.
	\item Fex: let $M_n^{\star (k)} = Median(\mathcal{D}^{\star (k)})$ be the sample median of bootstrap data $\mathcal{D}^{\star (k)}$.
	\item Then we can construct the variance of the distribution of bootstrap medians as:
	\begin{align}
		\hat{Var}_B (M_n) = \frac{1}{B-1} \sum_{k=1}^B (M_n^{\star (k)}-\bar{M}_n^\star )^2
	\end{align}
	where
	\begin{align}
		\bar{M}_n^\star = \frac{1}{B} \sum_{k=1}^B M_n^{\star (k)}
	\end{align}
	is the mean of the median of all bootstrap samples.
	\item Specifically it has been shown that in the $n\rightarrow \infty$ limit, the distribution of the bootstrap estimate will be a Gaussian centered around $\hat{M}_n (\mathcal{D}) = Median (X_1, ..., X_n)$ w/standard deviation $\propto$ $1\sqrt{n}$.
	\item $\rightarrow$ The bootstrap distribution $\hat{M}_n^\star - \hat{M}_n$ approximates fairly well the sampling distribution $\hat{M}_n - M$ from which we obtain the training data $\mathcal{D}$. 
	\item Note: $M$ = the median based on which the true dist $\mathcal{D}$ is generated. Aka, if we plot the histogram of $\{ M_n^{\star (k)} \}_{k=1}^B$, we'll see that in the large $n$ limit it can be well fitted by a Gaussian which sharp peaks at $\hat{M}_n (\mathcal{D})$ and vanishing variance defined by $\hat{Var}_B$ above.
	\item Fig 28: illustration.
	\item An interpretation of all the $M$s involved here:
	\begin{itemize}
		\item $M$ = the median based on which the true distribution/sample $\mathcal{D}$ is generated
		\item $M_n$ = ?
		\item $\hat{M}_n$ = $\hat{M}_n (\mathcal{D}) = Median(X_1, ..., X_n)$ =  the median of the sample $\mathcal{D}$
		\item $M_n^{\star (k)} = Median(\mathcal{D}^{\star (k)})$ = the sample median of bootstrap data $\mathcal{D}^{\star (k)}$.
		\item $\bar{M}_n^\star = \frac{1}{B} \sum_{k=1}^B M_n^{\star (k)}$ = the mean of the median of all bootstrap samples.
		\item $\hat{M}_n^\star$ = ?
		\item $\hat{M}_n - M$ = the sampling distribution from which we obtain the training data $\mathcal{D}$ = ?
		\item  $\hat{M}_n^\star - \hat{M}_n$ = bootstrap distribution = $\approx \hat{M}_n - M$ = ?
	\end{itemize}
\end{itemize}



\subsection{Boosting}
\begin{itemize}
	\item In bagging, the contribution of all predictors weighted equally in the bagged (aggregate) predictor. But in principle a myriad ways to combine different predictors. Sometimes might prefer autocratic approach that emphasizes the best predictors, other times may be better to opt for more "democratic" ways as is done in bagging.
	\item In boosting en ensemble of weak classifiers $\{ g_k(\bm{x}) \}$ is combined into an aggregate, boosted classifier. But unlike bagging, each classifier is associated with a weight $\alpha_k$ indicating how much it contributes to the aggregate classifier
	\begin{align}
		g_A (\bm{x}) = \sum_{K=1}^M \alpha_k g_k (\bm{x})
	\end{align}
	where $\sum_k \alpha_k = 1$. 
	\item For already discussed reasons, boosting, like all ensemble methods, works best when combining simple, high-variance classifiers into a more complex whole.
	\item Here focus on "adaptive boosting" or AdaBoost. Basic idea: form the aggregate classifier in an iterative process. 
	\item Important: at each iteration we reweight the error function to reinforce data points where the aggregate classifier performs poorly. In this way we can successively ensure our classifier has good performance over the whole dataset.
	\item Discussion of AdaBoost procedure in greater detail:
	\begin{itemize}
		\item $\mathcal{L} = \{ (\bm{x}_i, y_i), i=1,...,N \}$ a given dataset where $\bm{x}_i \in \mathcal{X}$ and $y_i \in \mathcal{Y} = \{ +1, -1 \}$
		\item Our objective: find an optimal hypothesis/classifier $g: \mathcal{X} \rightarrow \mathcal{Y} $ to classify the data.
		\item Let $\mathcal{H} = \{ g : \mathcal{X} \rightarrow \mathcal{Y} \}$ be the family of classifiers available in our ensemble.
		\item In the AdaBoost we're concerned with the classifiers that perform somehow better than "tossing a fair coin". = For each classifier, the family $\mathcal{H}$ can predict $y_i$ correctly at least half the time.
		\item Construct the boosted classifier as follows:
		\begin{itemize}
			\item \textbf{Initialize} $w_{t=1} (\bm{x}_n) = 1/N, n=1,...,N$
			\item \textbf{For} $t=1,..., T$, \textbf{do}:
			\begin{itemize}
				\item Select from $\mathcal{H}$ a hypothesis $g_t$ that minimizes the weighted error 
				\begin{align}
					\epsilon_t = \sum_{i=1 : g_t(\bm{x}_i) \neq y_i}^N w_t (\bm{x}_i)
				\end{align}
				\item Let $\alpha_t = \frac{1}{2} \text{ln} \frac{1-\epsilon_t}{\epsilon_t}$, update the weight for each data $\bm{x}_n$ by
				\begin{align}
					w_{t+1} (\bm{x}_n) \leftarrow w_t(\bm{x}_n) \frac{e^{-\alpha_t y_n g_t (\bm{x}_n)}}{Z_t}
				\end{align} 
				where $Z_t = \sum_{n=1}^N w_t (\bm{x}_n) e^{-\alpha_t y_n g_t (\bm{x}_n)}$ ensures all weights add up to unity.
				\item \textbf{Output} $g_A (\bm{x}) = sign(\sum_{t=1}^T \alpha_t g_t (\bm{x}))$
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Random forests}
\begin{itemize}
	\item A random forest is composed of a family of (randomized) tree-based classifier decision trees (= high-variance, weak classifiers that can be easily randomized, and as such, are ideally suited for ensemble-based methods).
	\item A \textbf{decision tree} uses a series of questions to hierarchically partition the data. Each branch consists of a question that splits the data into smaller subsets (e.g. is some feature larger than a given number?), with leaves (end points) of the tree corresponding to the ultimate partitions of the data.
	\item Goal: construct trees such that the partitions are informative about the class label (see fig 30). More complex decision trees lead to finer partitions that give improved performance on the training set. But, this generally leads to overfitting, limiting the out-of-sample performance. $\rightarrow$ Almost all decision trees use some form of regularization (e.g. maximum depth of the tree) to control complexity and reduce overfitting.
	\item Decision trees have extremely high variance, and often extremely sensitive to many details of the training data. Not surprising since they're learned by partitioning the training data. $\rightarrow$ Individual trees are weak classifiers. But, these same properties make them ideal for incorporation in an ensemble method.
	\item To create ensemble of trees, must introduce a randomization procedure: Power of ensembles to reduce variance only manifests when randomness reduces correlations between the classifiers within the ensemble. Randomness usually introduced into random forests in one of three distinct ways.
	\begin{itemize}
		\item First: Use bagging and simply "bag" the decision trees by training each decision tree on a different bootstrapped dataset. Strictly speaking this procedure does not constitute a random forest but rather \textbf{bagged decision trees}.
		\item Second: Only use a different random subset of the features at each split in the tree. This "feature bagging" is the distinguishing characteristic of random forests. It reduces correlations between decision trees that can arise when only a few features are strongly predictive of the class label. 
		\item Finally: Extremized random forests (ERFs) combine ordinary and feature bagging with an extreme randomization procedure where splitting is done randomly instead of using optimality criteria. Even though this reduces the predictive power of each individual decision tree, it still often improves the predictive power of the ensemble because it dramatically reduces correlations between members and prevents overfitting.
	\end{itemize} 
	\item Fig 31: Examples of the kind of decision surfaces found by decision trees, random forests, and Adaboost.
\end{itemize}

\subsection{Gradient boosted trees and XGBoost}
\begin{itemize}
	\item Idea: Use intuition from boosting and gradient descent (in particular Newton's method) to construct ensembles of decision trees. As in boosting, the ensembles are created by iteratively adding new decision trees to the ensemble.
	\item In gradient boosted trees, a central role is played by a cost func measuring the performance of our ensemble. At each step, we compute the gradient of the cost func wrt the predicted value of the ensemble and add trees that move us in the dir of the gradient.
	\item Ofc, this requires clever way of mapping gradients to decision trees. We give a brief overview of how this is done within XGboost.
	\item Starting point: A clever parametrization of decision trees (dts). We here use notation where the dt make cont. predictions (regression trees), though this can easily be generalized to classification tasks.
	\item Parametrize a decision tree $g_j (\bm{x})$ with $K$ leaves by two quantities:
	\begin{itemize}
		\item $q(\bm{x})$ = a func that maps each data point to one of the leaves of the tree, $q : \bm{x} \in \mathbb{R}^d \rightarrow \{ 1, 2,...,K \}$
		\item $\bm{w} \in \mathbb{R}^T$ that assigns a predicted value to each leaf.
	\end{itemize}
	In other words, the dt's prediction for the datapoint $\bm{x}_i$ is simply:
	$q(\bm{x}_i) = w_{q(\bm{x}_i)}$
	\item In addition to a parametrization of dts, we also must specify a cost func which measures predictions. The prediction of our ensemble for a datappoint $(y_i, \bm{x}_i)$ is given by
	\begin{align}
		\hat{y}_i = g_A (\bm{x}_i) = \sum_{j=1}^M g_j (\bm{x}_i)
	\end{align}
	where $g_j(\bm{x}_i)$ is the prediction of the $j$-th dt on on datapoint $\bm{x}_i$, and $M$ is the number of members in the ensemble. 

	As discussed in the context of random trees above, without regularization, dts tend to overfit the data by dividing it into smaller and smaller partitions. Thus, our cost func is generally composed of two terms: 
	\begin{itemize}
		\item A term that measures the goodness of predictions on each datapoint, $l_i (y_i, \hat{y}_i)$, which is assumed to be differentiable and convex
		\item And for each tree in the ensemble, a regularization term $\Omega(g_j)$ that does not depend on the data
	\end{itemize}
	\begin{align}
		\mathcal{C} (\bm{X}, g_A) = \sum_{i=1}^N l(y_i, \hat{y}_i) + \sum_{j=1}^M \Omega(g_j)
	\end{align}
	where index $i$ runs over data points and $j$ runs over dts in our ensemble. In XGBoost, the regularization func is chosen to be
	\begin{align}
		\Omega(g) = \gamma T + \frac{\lambda}{2} \sum_{l=1}^T w_l^2
	\end{align}
	with $\gamma$ and $\lambda$ regularization parameters that must be chosen appropriately. Notice this regularization penalizes both large weights on the leaves (similar to $L^2$-regularization) and having large partitions w/many leaves.
	\item As in boosting, we form the ensemble iteratively. For this reason we define a family of predictors $\hat{y}_t$ as 
	\begin{align}
		\hat{y}_i^{(t)} = \sum_{j=1}^t g_j (\bm{x}_i) = \hat{y}_i^{(t-1)} + g_t (\bm{x}_i)
	\end{align}
	Note: by definition $y_i^{(M)} = g_A (\bm{x}_i)$. The central idea is that for large $t$, each dt is a small perturbation to the predictor (of order $1/K$) and hence we can perform a Taylor expansion on our loss func to second order:
	\begin{align}
		\mathcal{C}_t =& \sum_{i=1}^N l(y_i, \hat{y}_i^{(t-1)} + g_t(\bm{x}_t)) + \Omega (g_t) \\
		\approx & \mathcal{C}_{t-1} + \Delta \mathcal{C}_t
	\end{align}
	with
	\begin{align}
		\Delta \mathcal{C}_t = b_i l(y_i, \hat{y}_i^{(t-1)}) g_t(\bm{x}_i) + \frac{1}{2}a_i g_t (\bm{x}_i)^2 + \Omega(g_t)
	\end{align}
	where
	\begin{align}
		a_i =& \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)}) \\
		b_i =& \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})
	\end{align}
	We then choose the $t$-th dt $g_t$ to minimize $\Delta \mathcal{C}_t$. Almost identical to how we derived the Newton method update in the gradient descent section.

	\item Can actually derive an expression for the parameters of $g_t$ that minimize $\Delta \mathcal{C}_t$ analytically. To simplify notation: define
	\begin{itemize} 
		\item The set of points $\bm{x}_i$ that get mapped to leaf $j: I_j = \{ i : q_t(\bm{x}_i) = j \}$
		\item The functions $B_j = \sum_{i\in I_j} b_i$ and $A_j = \sum_{i\in I_j} a_i$. 
	\end{itemize}
	In terms of these quantities we can write 
	\begin{align}
		\Delta \mathcal{C}_t = \sum_{j=1}^T [B_j w_j + \frac{1}{2} (A_j + \lambda_j) w_j^2] + \lambda T
	\end{align}
	where we made the $t$-dependence of all parameters implicit. 
	\item To find optimal $w_j$, just as in Newton's method take gradient of the above expression wrt $w_j$ and set this equal to zero, to get
	\begin{align}
		w_j^{opt} = - \frac{B_j}{A_j + \lambda}
	\end{align}
	plugging back into $\Delta \mathcal{C}_t$ gives
	\begin{align}
		\Delta \mathcal{C}_t^{opt} = -\frac{1}{2} \sum_{j=1}^K \frac{B_j^2}{A_j + \lambda } + \gamma T
	\end{align}
	$\rightarrow$ Clear that $\Delta \mathcal{C}_t^{opt}$ measures the in-sample performance of $g_t$ and we should find the dt that minimizes this value. Could in principle enumerate all possible trees over the data and find the tree that minimizes $\Delta \mathcal{C}_t^{opt}$.
	\item However, in practice, this is impossible. Instead, an approximate greedy algo is run that optimizes one level of the tree at a time by trying to find optimal splits of the data. Leads to a tree that is a good local minimum of $\Delta \mathcal{C}_t^{opt}$ which is then added to the ensemble. 
	\item Emphasize this is only high level sketch of how the algo works. In practice, additional regularization such as shrinkage and feature subsampling is also used. In addition, there are many numerical and technical tricks used for the approximate algo and how to find splits of the data that give good dts.
\end{itemize}

\subsection{Application to the Ising model and Supersymmetry Datasets}
\begin{itemize}
	\item Now illustrate with using two physics examples (we previously analyzed both using logistic regression):
	\begin{itemize}
		\item Classifying the phases of the spin configs of the 2D-Ising model above and below the critical temp using random forests and
		\item Classifying Monte-Carlo simulations of collision events in the SUSY dataset as supersymmetric or standard using an XGBoost implementation of gradient-boosted trees.
	\end{itemize}
	\item We show that on the Ising dataset, the RFs perform significantly better than logistic regression models whereas gradient boosted trees seem to yield an accuracy of about 80\%, comparable to published results.
	\item Ising: We assign a label to each state according to its phase: 0=disordered, 1=ordered. Divide the dataset into three categories according to the temp at which samples are drawn: ordered ($T/J < 2.0$), near-critical ($2.0 \leq T/J \leq 2.5$) and disordered ($T/J > 2.5$).
	\item Use the ordered and disordered states to train a random forest, evaluate our learned model on a test set of unseen ordered and disordered states. Also ask how well our RF can predict the phase of samples drawn in the region (=predict whether the temp of a critical sample is above or below the critical temp). Since our model never trained on critical region samples, prediction in this region is a test of the algo's ability to generalize to new regions in phase space.
	\item Result in Fig 32. Used two types of RFs: one where the ensemble consists of coarse dts w/few leaves, another w/finer dts w/many leaves. Extremely high accuracy on training and test sets (over 99\% ) for both. 
	\item However, notice the RF consisting of coarse trees perform extremely poorly on samples from the critical region whereas RF w/fine trees classifies critical samples w/accuracy of nearly 85\%. 
	\item Interestingly, and unlike with logistic regression, this performance in the critical region requires almost no parameter tuning. This because, as discussed above, RFs are largely immune to overfitting problems even as the number of estimators in the ensemble becomes large. Increasing the number of estimators in the ensemble does increase performance - but at a large cost in computational time.
	\item SUSY: used the XGBoost implementation of gradient boosted trees to classify Monte-Carlo collisions from the SUSY dataset.
	\item With default parameters using a small subset of the data (100 000 out of the full 5 million samples), we achieved accuracy of 79\%, which could be improved to nearly 80\% after some fine tuning.
	\item Comparable to published results and those we obtained using logistic regression.
	\item One nice feature of ensemble methods such as XGBoost is that they automatically allow us to calc feature scores (Fscores) that rank the importance of various features for classification. Higher the Fscore, the more important the feature. Fig 33 shows feature scores for the production of electrically-charged supersymmetric particles ($\chi \pm$) which decay into $W$ bosons and an electrically neutral supersymmetric particle $\chi^0$, which is invisible to the detector.
	\item The features are a mix of eight directly measurable quantities, as well as ten hand crafted features chosen using physics knowledge. Consistent with the physics of these supersymemtric decays in the lepton channel, we find the most informative features for classification are the missing transverse energy along the vector defined by the charged leptons (Axial MET) and the missing energy magnitude due to $\chi_0$.
\end{itemize}

\section{An introduction to Feed-Forward Deep Neural Networks (DNNs)}
\begin{itemize}
	\item Long history, but remerged to prominence after a rebranding as "Deep Learning" in the mid 2000s.
	\item Truly caught attention in 2012 when Alex Krizhevsky, Ilya Sutskever and Geoff Hinton used a GPU-based DNN model (AlexNet) to lower the error rate on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) by an incredible twelve percent from 28\% to 16\%. Three years later an ML group from Microsoft achieved an error of 3.57\% using an ultra-deep residual neural network (ResNet) with 152 layers. Since then, DNNs have become the workhorse technique for many image and speech recognition based ML tasks. Given rise to a number of high-level libraries and packages (Caffe, Kera, Pytorch, TensorFlow).
	\item Conceptually helpful to divide neural networks into four categories:
	\begin{itemize}
		\item General purpose neural networks for supervised learning
		\item Neural networks designed specifically for image processing, the most prominent example of this class being Convolutional Neural Networks (CNNs)
		\item Neural networks for sequential data such as Recurrent Neural Networks (RNNs) 
		\item Neural networks for unsupervised learning such as Deep Boltzmann Machines.
	\end{itemize}
	In this chapter we'll discuss the first, and CNN and DPM in later chapters. While increasingly important for many applications, we omit RNNs.
	\item As with most intellectual fields experiencing rapid expansion, many commonly accepted heuristics turn out not to be as powerful as thought, and widely held beliefs not as universal as once imagined. Especially true in modern neural networks where results are largely empirical and heuristic and lack the firm footing of many earlier machine learning methods. Because of this, in this review have chosen to emphasize tried and true fundementals, while pointing out what, from current vantage point, seem like promising new techniques. 
	\item In physics, DNNs and CNNs have already found numerous applications:
	\begin{itemize}
		\item Statistical physics: Detect phase transitions in 2D Ising and Potts models, lattece gauge theories, and different phases of polymers
		\item Shown that DNNs can learn free-energy landscapes
		\item Methods from statistical physics have been applied to the deep learning field to 
		\begin{itemize}
			\item Study the thermodynamic efficiency of learning rules to explore the hypothesis space that DNNs span, 
			\item Make analogies between training DNNs and spin glasses, and 
			\item To characterize phase transitions w/respect to network topology in terms of errors. 
		\end{itemize}
		\item In relativistic hydrodynamics, deep learning has been shown to capture features of non-linear evolution and hast the potential to accelerate numerical simulations
		\item In mechanics CNNs have been used to predict eigenvalues of photonic crystals
		\item Recently, DNNs have been used to improve the efficiency of Monte-Carlo algorithms.
		\item Deep learning has found interesting applications in quantum physics.
		\begin{itemize}
			\item Various quanutm phase transitions can be detected and studied using DNNs and CNNs, including 
			\begin{itemize}
				\item The transverse-field Ising model
				\item Topological phases
				\item And even non-equilibrium many-body localization
			\end{itemize}
			\item Representing quantum states as DNNs and quantum state tomography
			\item Study quantum and fault-tolerant error correction
			\item Estimate rates of coherent and incoherent quantum processes
			\item Recognition of state and charge configurations and auto-tuning in quantum dots
			\item In quantum information theory, it's been shown one can perform gate-compositions with the help of neural nets
			\item In lattice quantum chromodynamics, DNNs have been used to learn action parameters in regions of parameter space where PCA fails
			\item Study of quantum control
			\item Scattering theory to learn s-wave scattering length of potentials
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Neural network basics}
Neural networks/neural nets = neural-inspired nonlinear models for supervised learning. Natural, more powerful extensions of supervised learning methods such as linear and logistic regression and soft-max methods.

\subsubsection{The basic building block: neurons}
\begin{itemize}
	\item The net's basic unit = a stylized "neuron" $i$ that takes a vector of inputs $\bm{x} = (x_1, x_2, ..., x_d)$ and produces a scalar output $a_i(\bm{x})$.
	\item A neural network consists of many such neurons stacked into layers, w/the output of one layer serving as the input of the next.
	\item First layer = input layer; middle layers= hidden layers; final layer = output layer.
	\item The exact function $a_i$ varies depending on type of non-linearity used in the neural network. But, in essentially all cases $a_i$ can be decomposed into a linear operation that weights the relative importance of the various inputs and a non-linear transformation $\sigma_i(z)$ which is usually the same for all neurons.
	\item The linear trans in almost all neural nets takes the form of a dot product with a set of neuron-specific weights $\bm{w}^{(i)} = (w_i^{(i)}, w_i^{(i)}, ..., w_d^{(i)})$ followed by re-centering with a neuron-specific bias $b^{(i)}$
	\begin{align}
		z^{(i)} = \bm{w}^{i} \cdot \bm{x} + b^{(i)}
	\end{align}
	\item In terms of $z^{(i)}$ and the non-linear function $\sigma_i(z)$, we can write the full input-output function as
	\begin{align}
		a_i(\bm{x}) = \sigma_i(z^{(i)})
	\end{align}
	\item Historically, common choices of nonlinearities included step-functions (perceptrons), sigmoids (i.e. Fermi functions), and the hyperbolic tangent.
	\item More recently, has become more common to use rectified linear functions (ReLUs), leaky rectified linear units (leaky ReLUs), and exponential linear units (ELUs).
	\item Different choices of non-linearities lead to different computational and training properties of neurons. Underlying reason is that we train neural nets using gradient descent based methods that require us to take derivatives of the neural input-output function with respect to the weights $\bm{w}^{(i)}$ and the bias $b^{(i)}$. 
	\item Notice that the derivatives of the aforementioned non-linearities $\sigma(z)$ have very different properties. The derivative of the perceptron is zero everywhere except where the input is zero. $\rightarrow$ This discontinuous behavior makes it impossible to train perceptrons using gradient descent. 
	\item For this reason, until recently, the most popular choice of non-linearity was the tanh function or a sigmoid/Fermi function. But, this non-linearity choice has a major drawback. When the input weights become large, as they often do in training, the activation function \textbf{saturates} and the derivative of the output with respect to the weights tend to zero since $\frac{\partial \sigma}{\partial z} \rightarrow 0$ for $z \gg 1$.
	\item Such "vanishing gradients" are a feature of any saturating activation function (perceptron $\Theta (z)$, sigmoid $\frac{1}{1 + e^{-z}}$, tanh $tanh(z)$), making it harder to train deep networks.
	\item In contrast, for a non-saturating activation function (such as ReLUs $max(0, z)$, leaky ReLUs ($0.1z$ if $z\leq 0$, $z$ if $z\geq 0$) or ELUs ($e^z - 1$ if $z\leq 0$, $z$ if $z\geq 0$)), the gradients stay finite even for large inputs.
\end{itemize}

\subsubsection{Layering neurons to build deep networks: network architecture}
\begin{itemize}
	\item Basic idea behind all neural networks: layer neurons in a hierarchical fashion, general structure of which is known as the network architecture.
	\item In the simplest feed-forward networks, 
	\begin{itemize}
		\item each neuron in the \textit{input layer} of the neurons takes the inputs $\bm{x}$ and produces an output $a_i(\bm{x}= \sigma_i(z^{(i)}))$ that depends on its current weights. 
		\item The outputs are then treated as inputs to the next \textit{hidden layer}. 
		\item This is usually repeated several times until one reaches the top or \textit{output layer}. The output layer is almost always a simple classifier of the form discussed earlier: a logistic regression or soft-max function in the case of categorical data (i.e. discrete labels) or a linear regression layer in the case of continuous outputs. 
	\end{itemize}
	Thus, the whole neural net can be thought of as a complicated nonlinear trans of the inputs $\bm{x}$ into an output $\hat{y}$ that depends on the weights and biases of all the neurons in the input, hidden and output layers.
	\item Hidden layers greatly expands the representational power of a neural net when compared to a simple soft-max or linear regreesion network.
	\item Perhaps the most formal expression of the increased representational power of neural networks (=expressivity) is the universal approximation theorem, stating: A neural network with a single hidden layer can approximate any continuous, multi-input/multi-output function with arbitrary accuracy. \textbf{The reader is strongly urged to read the beautiful graphical proof of the theorem in free online book (link)}. 
	\begin{itemize}
		\item Basic idea of the proof: Hidden neurons allow neural networks to generate step functions with arbitrary offsets and heights. These can then be added together to approximate arbitrary functions. 
	\end{itemize}
	The proof also makes clear that the more complicated a function, the more hidden units (and free parameters) are needed to approximate it. Hence, the applicability of the approximation theorem to practical situations should not be overemphasized.
	\item In physics, a good analogy are \textbf{matrix product states}, which can approximate any quantum many-body state to an arbitrary accuracy, provided the bond dimension can be increased arbitrarily - a severe requirement not met in any practical implementation of the theory.
	\item Modern neural networks generally contain multiple hidden layers (= the 'deep' in deep learning). There are many ideas why deep architectures favorable for learning.
	\begin{itemize}
		\item Increasing number of layers = increases the number of parameters = increases the representational power.
		\item Recent numerical experiments suggests that as long as the number of parameters is larger than the number of data points, certain classes of neural nets can fit arbitrarily labeled random noise samples. Suggests large neural nets of the kind used in practice can express highly complex functions.
		\item Adding hidden layers is also thought to allow neural nets to learn more complex features from the data. Work on CNNs suggests the first few layers learn simple, "low level" features that are then combined into higher-level, more abstract features in the deeper layers.
		\item Other works suggest it's computationally and algorithmically easier to train deep netowrks rather than shallow, wider nets, though this is still an area of major controversy and active reasearch (\textbf{PAPER}). 
	\end{itemize}
	\item Choosing the exact network architecture remains an art that requires extensive numerical experimentation and intuition, and is often-times problem-specific. Both number of hidden layers and number of neurons in each layer can affect the performance of a neural network. Seems to be no single recipe for the right architecture for a neural net that works best. But, general rule of thumb that seems to be emerging: the numb of parameters in the neural net should be large enough to prevent \textit{underfitting} (\textbf{PAPER}).
	\item Empirically, the best architecture depends on
	\begin{itemize}
		\item The task
		\item The amount and type of data available
		\item The computational resources at one's disposal.
	\end{itemize}
	Certain architectures are easier to train, while others might be better at capturing complicated dependencies in the data and learning relevant input features.
	\item Finally, have been numerous works that move beyond the simple deep, feed-forward neural nets discussed here. Fex: Modern neural nets for image segmentation often incorporate "skip connections" that skip layers of the neural net, allowing information to directly propagate to a hidden or output layer, bypassing intermediate layers and often improving performance.
\end{itemize}


\subsection{Training deep networks}
\begin{itemize}
	\item Basic procedure the same as we used for training simpler supervised learning algos, such as logistic and linear regression: Construct a cost/loss function and then use gradient descent to minimize the cost function. Neural nets differ from these simpler supervised procedures in that generally they contain multiple hidden layers that make taking the gradient more computationally difficult. Will return to this later when discussing the "backpropagation" algo for computing gradients.
	\item Like all supervised learning procedures, must first specify loss func.
	\begin{itemize}
		\item Given a data point $(\bm{x}_i, y_i)$, the neural net makes a prediction$\hat{y}_i (\bm{w})$, where $\bm{w}$ are the parameters of the neural network.
		\item Recall that in most cases, the top output layer is either a continuous predictor or a classifier. Depending on this one must utilize a different loss function.
		\item For continuous data:
		\begin{itemize}
			\item The loss funcs commonly used are identical to those in linear regression, include the mean squared error
			\begin{align}
				E(\bm{w}) = \frac{1}{N} \sum_i (y_i - \hat{y}_i (\bm{w}))^2
			\end{align}
			where $N$ is the number of data points, and the mean absolute error (i.e. $L_1$ norm)
			\begin{align}
				E(\bm{w}) = \frac{1}{N} \sum_i |y_i - \hat{y}_i (\bm{w})|
			\end{align}
			The full cost-func often includes additional terms that implement regularization (e.g. $L_1$ or $L_2$ regularizers).
		\end{itemize}
		\item For categorical data:
		\begin{itemize}
			\item Most commonly loss-func is the cross-entropy, since the output layer is often taken to be a logistic classifier for binary data w/two types of labels, or a soft-max classifier if there are more than two types of labels. Cross-entropy already discussed extensively in logistic regression chapter.
			\item Recall that for binary data classification, the output of the top layer is the probability $\hat{y}_i(\bm{w}) = p(y_i = 1 | \bm{x}_i ; \bm{w})$ that datapoint $i$ is predicted to be in category 1.
			\item The cross-entropy between the true labels $y_i \in \{0,1\}$ and the predictions is given by 
			\begin{align}
				E(\bm{w}) = - \sum_{i=1}^n y_i \text{log} \hat{y}_i (\bm{w}) + (1-y_i)\text{log} [1 - \hat{y}_i (\bm{w})]
			\end{align} 
			\item More generally, for categorical data, $y$ can take on $M$ values so that $y \in \{ 0, 1, ..., M-1 \}$. For each datapoint $i$, define a vector $y_{im}$ called a 'one-hot' vector, such that
			\[ y_{im} = \begin{cases}
				 1, & \text{if } y_{i} = m \\
				 0, & \text{otherwise.}  
			\end{cases} \]
			Can also define the prob that the net assigns a datapoint to category $m$: $\hat{y}_{im}(\bm{w}) = p(y_i = m | \bm{x}_i ; \bm{w})$. The categoricla cross-entropy is then
			\begin{align}
				E((\bm{w})) = - \sum_{i=1}^n \sum_{m=0}^{M-1} y_{im} \text{log} \hat{y}_{im} (\bm{w}) + (1-y_{im}) \text{log} [1 - \hat{y}_{im} (\bm{w})]
			\end{align}
			As in linear and logistic regr, this loss func often supplemented by additional terms that implement regularization.
		\end{itemize}
	\end{itemize}
	\item Having identified architecture and cost func, time to train. Use gradient descent based (GD) methods to optimize cost func.
	\item Recall basic idea of GD = update the parameters $\bm{w}$ to move in the dir of the gradient of the cost func, $\nabla_{\bm{w}} E(\bm{w})$. As discussed there are numerous variants. Most modern NN packages such as Kera allow user to specify which of these optimizers they would like to use in order to train the NN.
	\item Depending on the architecture, data, computational resources, different optimizers may work better, though vanilla SGD a good first choice.
	\item Unlike in linear and logistic regression, calc the gradients for a NN requires a special algo= backpropagation = backprop, forming the heart of the NN training.
	\item Backprop has been discovered multiple times independently, but was popularized for modern NNs in 1985. 
\end{itemize}

\subsection{High-level specification of a neural network using Keras}
\begin{itemize}
	\item Load the required packages
	\item Load the data (the MNIST digit data)
	\item Data preprocessing:
	\begin{itemize}
		\item Split between training and testing, reshape
		\item Standardize = normalize the input greyscale integer values to values between [0, 1]. 
		\item Encode the classification labels using one-hot vectors, rather than integers (Keras provides function for doing this, "keras.utils.to\_categorical")
	\end{itemize}
	\item Build the network
	\begin{itemize}
		\item Create an instance of Kera's \texttt{Sequential()} class, and call it \texttt{model}. Allows us to build DNN's layer by layer. Use \texttt{add()} to attach layers to our \texttt{model}.
		\item We focus on \texttt{Dense} layers for simplicity, but in subsequent examples, we'll see how to add dropout regularization and convolutional layers.
		\item Every \texttt{Dense()} layer accepts as its first required argument an integer specifying the number of neurons. 
		\item Type of activation for the layer is defined usingt he \texttt{activation} optional argument, its input being the name of the activation function, fex \texttt{'relu'}, \texttt{'tanh'}, \texttt{'elu'}, \texttt{'sigmoid'}, \texttt{'softmax'}.
		\item Must ensure number of input and output neurons for each layer match. $\rightarrow$ specify the shape of the input in the first layer of the model explicitly. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. $\rightarrow$ we only need to specify the size of the softmax output layer to match the number of categories. 
	\end{itemize}
	\item Next, choose loss func to use for training. Chooses \texttt{categorical\_entropy} defined in Keras' \texttt{losses} module (because we're dealing with classification i.e. cross-entropy, and because output data was cast in categorical form).
	\item Choose SGD for optimization, available in Keras' \texttt{optimizers} module. Pass \texttt{lr} (learning rate) and \texttt{momentum} as optional arguments to the \texttt{SGD} func.
	\item To test performance, look at a particular \texttt{metric} of performance. For instance, in categorical tasks typically looks at their \texttt{'accuracy'}, which is defined as the percentage of the correctly classified data points.
	\item Use \texttt{compile()} to complete the model, with optional arguments for the \texttt{optimizer}, \texttt{loss} and the validation \texttt{metric}.
	\item Training the DNN: a one-liner using the \texttt{fit()} method of the \texttt{Sequential} class. Two first reguired arguments = input and output data. Optional arguments: the mini-\texttt{batch\_size}, the number of training \texttt{epochs}, and the test or \texttt{validation\_data}. To monitor the training procedure for every epoch, we set \texttt{verbose=True}.
\end{itemize}


\subsection{The backpropagation algorithm}
Training requires us to calc the derivative of the cost func wrt all the parameters (the weight and biases of all neurons in all layers). A brute force calc is \textbf{out of the question} since it requires us to calc as many gradients as parameters at each step of the gradient descent. Backprop = clever procedure that exploits layered structure of NNs to more efficiently compute gradients.


\subsubsection{Deriving and implementing the backpropagation equations}
\begin{itemize}
	\item Backprop at its core: The ordinary chain rule for partial differentiation: if $z$ is a function of $n$ variables $x_1, x_2, ..., x_n$ and each of these variables are in turn functions of $m$ variables $t_1, t_2, ..., t_m$. Then for any variable $t_i$, $i=1,2,...,m$ we have that
	\begin{align}
		\frac{\partial z}{\partial t_i} = \frac{\partial z}{\partial x_1} \frac{\partial x_1}{\partial t_i} + \frac{\partial z}{\partial x_2} \frac{\partial x_2}{\partial t_i} + ... + \frac{\partial z}{\partial x_n} \frac{\partial x_n}{\partial t_i}
	\end{align}	
	Backprop can be summarized using four equations.

	\item First: establish useful notation.
	\begin{itemize}
		\item Assume there are $L$ layers, with $l=1,...,L$ indexing the layer. 
		\item Denote $w_{jk}^l$ the weight for the connection from the $k$-th neuron in layer $l-1$ to the $j$-th neuron in layer $l$. 
		\item Denote the bias of this neuron by $b_j^l$. 
		\item The activation $a_j^l$ of the $j$-th neuron in the $l$-th layer can, by construction in a feed-forward NN, be related to the activities of the neurons in the layer $l-1$ by the equation
		\begin{align}
			a_j^l = \sigma (\sum_k w_{jk}^l a_k^{l-1} + b_j^l) = \sigma(z_j^l)
		\end{align}
	\end{itemize}
	\item We now define the four eqations making up backprop:
	\begin{enumerate}
		\item By definition, the cost func $E$ depends directly on the activities of the output layer $a_j^L$ (which is our estimated response $\hat{y}$?), but also indirectly on lower layer neuron activations, since $a_j^L$ is calc from them.
		Define the error of neuron $j$ in layer $l$, $\Delta_j^l$, as the change in the cost func (why?) wrt the weighted input $z_j^l$:
		\begin{align}
			\Delta_j^l = \frac{\partial E}{\partial z_j^l} = \frac{\partial E}{\partial a_j^l} \sigma' (z_j^l)
		\end{align}
		where $\sigma'(x)$ = the derivative of the non-linearity $\sigma(\cdot)$ wrt its input evaluated at $x$.
		\item Notice the error func $\Delta_j^l$ can also be interpreted as the partial derivative of the cost func wrt the bias $b_j^l$, since
		\begin{align}
			\Delta_j^l = \frac{\partial E}{\partial z_j^l} = \frac{\partial E}{\partial b_j^l} \frac{\partial b_j^l}{\partial z_j^l} = \frac{\partial E}{\partial b_j^l}
		\end{align}
		where we used that $\frac{\partial b_j^l}{\partial z_j^l} = 1$.
		\item Derive the final two eq. using the chain rule. Since the error depends on neurons in layer $l$ only through the activation of neurons in the subsequent layer $l+1$, can use chain rule to write
		\begin{align}
			\Delta_j^l =& \frac{\partial E}{\partial z_j^l} \\
			&= sum_k \frac{\partial E}{\partial z_k^{l+1}} \frac{\partial z_k^{l+1}}{\partial z_j^l} \\
			&= sum_k \Delta_k^{l+1} \frac{\partial z_k^{l+1}}{\partial z_j^l} \\
			&= (sum_k \Delta_k^{l+1} w_{kj}^{l+1})\sigma' (z_j^l)
		\end{align}

		\item Derive the final eq by differentiating the cost func wrt $w_{jk}^l$
		\begin{align}
			\frac{\partial E}{\partial w_{jk}^l} = \frac{\partial E}{\partial z_j^l} \frac{\partial z_j^l}{\partial w_{jk}^l} = \Delta_j^l a_k^{l-1}
		\end{align}
	\end{enumerate}
	\item Together, these are the four backprop equations that relate the gradients of the activations $a_j^l$, the weighted inputs $z_j^l$ and the errors $\Delta_j^l$. They can be combined into a simple, computationally efficient algo to calc the gradient wrt all parameters: \textbf{The Backpropagation Algorithm}
	\begin{enumerate}
		\item \textbf{Activation at input layer}: Calc the activations $a_j^1$ of all the neurons in the input layer
		\item \textbf{Feedforward}: starting w/the first layer, exploit the feed-forward architecture to compute $z^l$ and $a^l$ of each subsequent layer.
		\item \textbf{Error at top layer}: calc the error of the top layer using equation 1.
		\item \textbf{"Backpropagate" the error}: Use equation 3 to backpropagate the error backwards and calculate $\Delta_j^l$ for all layers.
		\item \textbf{Calculate gradient}: Use eq. 2 and 4 to calc $\frac{\partial E}{\partial b_j^l}$ and $\frac{\partial E}{\partial w_{jk}^l}$.
	\end{enumerate}
	\item These basic ideas also underly almost all modern automatic differentiation packages such as Autograd (Pytorch).
\end{itemize}

\subsubsection{Computing gradients in deep networks: what can go wrong with backprop?}
\begin{itemize}
	\item Even with backprop, gradient descent on large networks = extremely computationally expensive. The great advances in computational hardware (and the widespread use of GPUs) has made this a much less vexing problem than even a decade ago.
	\item Gradients can vanish and explode. \textit{The problem of vanishing or exploding gradients}. Especially pronounced in NNs trying to capture long-range dependencies such as RNNs for sequential data. Can illustrate this by considering a simple network w/one neuron in each layer. Assume all weights equal, $w$. Behavior of the backprop for such a network can be inferred from repeatedly using eq. 3:
	\begin{align}
		\Delta_j^1 = \Delta_j^L (w)^L \prod_{j=0}^{L-1} \sigma' (z_j)
	\end{align} 
	Assume the magnitude $\sigma' (z_j)$ fairly constant, so we can approx $\sigma' (z_j) \approx \sigma_0 '$.
	Then, notice that for large $L$, the error $\Delta_j^1$ has very different behavior depending on the value of $w \sigma_0'$:
	\begin{itemize}
		\item $w \sigma_0' > 1$: the errors of the gradient blow up.
		\item $w \sigma_0' < 1$: the errors of the gradient vanish.
		\item $w \sigma_0' \approx 1$ only now, and for neurons not saturated, will the gradient stay well behaved for deep netowrks.
	\end{itemize}
	This behavior holds true even in more complicated networks.
	\item Rather than considering a single weight, can ask about the eigenvalues (or singular values) of the weight matrices $w_{jk}^l$. In order for gradients to be finite for deep networks, we need these eigenvalues to stay near unity even after many gradient descent steps.
	\item In modern feedforward and ReLU NNs, this achieved by initializing the weights for the gradient descent in clever ways, and using non-linearities that don't saturate, such as ReLUs (recall: for saturating functions, $\sigma' \rightarrow 0$, causing the grad to vanish). 
	\item Proper initialization and regularization schemes such as gradient clipping (cutting-off gradients w/very large values), and batch normalization also help mitigate the vanishing and exploding gradient problem.
\end{itemize}


\subsection{Regularizing neural networks and other practical considerations}
\begin{itemize}
	\item DNNs, like all supervised learning algos, must navigate the bias-variance tradeoff.
	\item Regularization techniques play important role in ensuring DNNs generalize well to new data.
	\item Last five years seen a wealth of new specialized regularization techniques for DNNs beyond the simple $L_1$ and $L_2$ penalties, including Dropout and Batch Normalization. Also, DNNs seem especially well-suited for implicit reularization that already takes place in the SGD. The implicit stochasticity and local-nature of SGD often prevents overfitting and spurious correlatios in the training data, especially when techniques such as early stopping. We'll here give a brief overview over these techniques.
\end{itemize}

\subsubsection{Implicit regularization using SGD: initialization, hyper-parameter tuning, and early stopping}
\begin{itemize}
	\item SGD = most commonly employed and effective optimizer for training NNs
	\item Acts as an implicit regularizer by introducing stochasticity that prevents overfitting.
	\item Important that weight initialization be chosen randomly, in order to break any leftover symmetries. 
	\begin{itemize}
		\item One common choice: drawing weights from a Gaussian centered around zero with some variance that scales inversley with number of inputs to the neuron. Since SGD a local procedure, as networks get deeper, \textit{choosing a good weight initialization becomes increasingly important} to ensure the gradients are well behaved.
		\item Choosing initialization with a variance too large or small causes gradients to vanish and the network to train poorly - even a factor of 2 can make huge difference in practice \textbf{PAPER}.  $\rightarrow$ \textbf{important to experiment with different initialization variances}. 
	\end{itemize}
	\item Second important thing: appropriately choose the learning rate/step size by searching over five logarithmic grid points \textbf{PAPER}. If best performance occurs at edge of grid, repeat procedure until optimal learning rate is in the middle of the parameters.
	\item Common to center or whiten the input data (as we did for linear and logistic regression).
	\item Early stopping
	\begin{itemize}
		\item Divide the training data into two portions, the dataset we train on and a smaller validation set that serves as a proxy for out-of-sample performance on the test set.
		\item As we train model, we plot both training and validation error. We expect training error to continuously decrease during training. But validation error will eventually increase due to overfitting. 
		\item Hault the training procedure when validation error starts to rise. This "early stopping" procedure ensures we stop training and avoid overfitting sample specific features in the data.  
	\end{itemize}
\end{itemize}

\subsubsection{Dropout}
\begin{itemize}
	\item Basic idea: prevent overfitting by reducing spurious correlations between neurons by introducing a radnomization procedure similar to that underlying ensemble models such as Bagging.
	\item Recall the basic idea behind ensemble methods: train an ensemble of methods that are created using a randomization procedure to ensure that the members of the ensemble are uncorrelated. This reduces the variance of statistical predictions without creating too much additional bias.
	\item Extremly costly to train an ensemble of NNs, both due to data needed and computational resources and parameter tuning. 
	\item Droput circumvents this, by randomly dropping out neurons (and their connections) from the NN during each step of the training.
	\item Typically, for each mini-batch in the grad descent step, a neuron is dropped from the NN with a probability $p$. The grad descent step then performed only on the weights of the "thinned" network of individual predictors. 
	\item Since in the training, on average weights only present a fraction $p$ of the time, predictions are made reweighing the weights by $p$: $\bm{w}_{test} = p \bm{w}_{train}$. 
	\item The learned weigths can be viewed as some "average" weight over all possible thinned NN. This averaging of weights is similar in spirit to the Bagging procedure discussed in the ensemble methods context.
\end{itemize}

\subsubsection{Batch Normalization}
\begin{itemize}
	\item Basic inspiration: the long-known observation that training in NNs works best when the inputs are centered around zero wrt the bias. Because: it prevents neurons from saturating and gradients from vanishing in deep nets.
	\item In the absence of such centering, changes in parameters in lower layers can give rise to saturation effects in higher layers, and vanishing gradients. 
	\item Idea: Introduce additional new "BatchNorm" layers that standardize the inputs by the mean and variance of the mini-batch. 
	\item Consider a layer $l$ with $d$ neurons whose inputs are $(z_1^l, ..., z_d^l)$. We standardize each dimension so that
	\begin{align}
		\hat{z}_k^l = \frac{z_k^l - E[z_k^l]}{\sqrt{Var[z_k^l]}}
	\end{align} 
	where the expectation and variance are taken over all samples in the mini-batch. 
	\item One problem: this may change the representational power of the NN. Fex, for tanh non-linearities, it may force the network to live purely in the linear regime around $z=0$. Since non-linearities are crucial to the representational power of DNNs, could dramatically alter its power.
	\item Thus, one introduces two new parameters $\gamma_k^l$ and $\beta_k^l$ for each neuron that can shift and scale the normalized input 
	\begin{align}
		\hat{y}_k^l = \gamma_k^l \hat{z}_k^l + \beta_k^l
	\end{align}
	These new parameters are then learned just like the weights and biases (just an extra layer for the backprop chain rule). We initialize the NN so at the beginning of training the inputs are standardized. Backprop then adjusts $\gamma$ and $\beta$ during training.
	\item In practice, this method considerably improves learning speed by preventing vanishing gradients. But, it also seems to serve as a powerful regularizer for reasons not fully understood. One plausible explanation: in this method, the gradient for a sample depends not only on the sample itself, but also on all the properties of the mini-batch. A single sample can occur in different mini-batches $\rightarrow$ this introduces additional randomness into the training, which seems to help regulize training.
\end{itemize}

\subsection{Deep neural networks in practice: examples}
How to use NNs in practice.

\subsubsection{Deep learning packages}
\begin{itemize}
	\item There are DNN packages in other languages than Python, fex Caffe in C++.
	\item Keras = high level framwork, does not require any knowledge about inner workings of the underlying deep learning algos. But, for advanced applications, which may require more direct control over the operations in between layers, Keras' high level design may prove insufficient.
	\item Keras wraps the functionality of another package - TensorFlow.
	\item TensorFlow has become the preferred deep learning library. In it one constructs data flow graphs, where nodes=mathematical operations, edges=multidimensional tensors/data arrays. A DNN then thought of as a graph with a particular architecture. One needs to understand this concept well before one can truly unleash TensorFlow's full potential = steep learning curve and requires some time/perseverance.
	\item There are many other open source packages allowing for control over inter- and intra-layer operations, without need to introduce computational graphs. Fex: Pytorch. Offers libraries for automatic differentiation of tensors at GPU speed. As discussed, mainpulating NNs boils down to fast array multiplication and contraction operations $\rightarrow$ the \texttt{torch.nn} library often does the job providing enough access and controllability to manipulate the linear algebra operations underlying DNNs.
\end{itemize}

\subsubsection{Approaching the learning problem}
\begin{itemize}
	\item Typical procedure for using NNs:
	\begin{enumerate}
		\item \textbf{Collect and pre-process the data.}
		\item \textbf{Define the model and its architecture.}
		\item \textbf{Choose the cost function and the optimizer.}
		\item \textbf{Train the model.}
		\item \textbf{Evaluate and \textit{study} the model performance on the validation and test data.}
		\item textbf{Adjust the hyperparameters (and, if necessary, network architecture) to optimize performance for the specific dataset.}
	\end{enumerate}
	\item Step 1: Getting data into appropriate form = inseparable part of the learning process. 
	\begin{itemize}
		\item One of the first questions = how to choose size of training vs test data.
		\begin{itemize}
			\item MNIST dataset, 10 classification categories: 80\% training, 20\% testing.
			\item ImageNet datset, 100 categories: 50\% training, 50\% testing.
		\end{itemize}
		Rule of thumb: more classification categories = closer should size of training and test data be, in order to prevent overfitting.
		\begin{itemize}
			\item Once size of training data set, common to reserve 20\% of it for validation, used for fine-tuning the hyperparameters of the model.
		\end{itemize}
		\item How to choose the right hyperparameters to begin training with. 
		\begin{itemize}
			\item Fex: according to Bengio, optimal learning rate = an order of magnitude lower than the smallest learning rate that blows up the loss.
			\item Also keep in mind, training the model can take a lot of time. This can severely slow down any progress on improving the model in Step 6. $\rightarrow$ usually a good idea to play with a small enough percentage of the training data to get a rough feeling about the correct hyperparameter regimes, the usefulness of the DNN architecture, and to debug one's code. Size of the "play set" should be such that training on it can be done fast and in real time to allow quickly adjusting the hyperparameters.
		\end{itemize}
		\item Standardization of the dataset.
		\begin{itemize}
			\item Shown empirically that if the original values of the data differ by orders of magnitude, training can be slowed/impeded. Related to the vanishing/exploding gradient problem. $\rightarrow$ two tricks here:
			(i) All data should be mean-centered, i.e. from every data point we subtract the mean of the entire dataset
			(ii) Rescale the data (ensures the weights of the DNN are of a similar order of magnitude), for which there are two ways: if the data is approximately normally distributed, one can rescale by the standard deviation. Otherwise, it's typically rescaled by the max absolute value so the rescaled data lies in the interval $[-1, 1]$. 
		\end{itemize}
		\item Often, insufficient data serves as a major bottleneck on the ultimate performance of DNNs. Then one can consider data augmentation, i.e. distorting data samples from the existing dataset in some way to enhance size the dataset. If one knows how to do this, one already has partial information about the important features in the data.
	\end{itemize}
	\item It's only when steps 1-5 are put together in step 6 that the real benefit of deep learning is revealed, compared to less sophisticated methods such as regression or bagging. The optimal choice of network architecture, cost func, and optimizer is determined by the properties of the training and test data, which are only revealed when we try to improve the model. A typical strategy for exploring the hyperparameter landscape is to use grid searches. 
	\item This procedure can be applied to other ML tasks, not just DNNs. See chapter 11 for more useful hints on how to use the validation data.
\end{itemize}

\subsubsection{SUSY dataset}
\begin{itemize}
	\item Look at the dataset we have previously studied with logistic regression and bagging. There is an interest in using deep learning methods to automatically discover collision features. Benchmark results using Bayesian Decision Trees from a standard physics package, and five-layer NNs using the dropout algorithm were presented in the original paper (reference) to compare the ability of deep learning to bypass the need of using such high-level features (what is meant by this sentence...?).
	\item Our goal: Study systematically the accuracy of a DNN classifier as a func of the 
	\begin{itemize}
		\item Learning rate
		\item Dataset size
	\end{itemize}
	We here use Pytorch.
	\item We construct a DNN with two dense hidden layers of 200 and 100 neurons, respectively. We use ReLU activation between the input and hidden layers, and a softmax output layer. Apply dropout regularization on the weights of the DNN. Similar to MNIST, we use cross-entropy as cost func and minize it using SGD with batches of size 10\% of the training data size. We train the DNN over 10 epochs. 
	\item Fig 40 shows accuracy of DNN on the test data as a func of the learning rate and the size of the dataset.
	\begin{itemize}
		\item Considered good practice to start with a logarithmic scale to search through the hyperparameters, to get an overall idea for the order of magnitude of the optimal values. 
		\item In this example, performance peaks at the largest size of the dataset and a learning rate og 0.1, and is of the order of 80\%. 
		\item For comparison, in the original study, the authors achieved $\approx$ 89\% by using the entire dataset with 5,000,000 points and a more sophisticated network architecture, trained using GPUs.
	\end{itemize}
\end{itemize}

\subsubsection{Phases of the 2D Ising model}
\begin{itemize}
	\item Study the problem of classifying the states of the 2D Ising model with a DNN, focusing on the model performance as a func of both the number of hidden neurons and the learning rate.
	\item We construct a minimalistic model for a DNN with a single hidden layer containing a number of hidden neurons. The network architecture thus includes a ReLU-activated input-layer, the hidden layer, and the softmax output layer. We pick the categorical cross-entropy as cost func and minimize it using SGD with mini-batches of size 100. We train the DNN over 100 epochs.
	\item Fig 41 show the outcome of the grid search over a log-spaced learning rate and the number of neurons in the hidden layer. See that about 10 neurons are enough at learning rate of 0.1 to get a very high accuracy on the test set. But, if we aim at capturing the physics close to criticality, clearly more neurons are required to reliably learn the more complex correlations in the Ising states.
\end{itemize}


\section{Convolutional Neural Networks (CNN)}
\begin{itemize}
	\item Core lesson in physics: exploit symmetries and invariances. Properties such as locality and translational invariance are often built directly into the physical laws. Our statistical physics model often directly incorporate everything we know about the physical system being analyzed. Fex: we know in many cases it's sufficient to consider only local couplings in our Hamiltonians, or work directly in momentum space if the system is translationally invariant. 
	\item Like physical systems, many datasets and supervised learning tasks also possess additional symmetries and structure. Fex: consider a supervised learning task where we want to label images from some dataset as being pictures of cats or not. Our model must first learn features associated with cats. Because a cat a physical object, we know these features likely to be local (groups of neighboring pixels in the 2D image corresponding to whiskers, tails, eyes, etc.). Also know the cat can be anywhere in the image $\rightarrow$ doesn't really matter where in the picture these features occur (though relative positions of features likely do matter). = a manifistation of translational invariance built into our supervised learning task. 
	\item The all-to-all coupled NNs in the previous section fail to exploit this additional structure. Fex, consider image of the digit 'four' from the MNIST dataset. In the all-to-all coupled NNs used there, the $28\times 28$ image was considered a 1D vector of size $28^2 = 796$. $\rightarrow$ throws away lots of the spatial information. 
	\item The NN community realized these problems, designed a class of NN architecture: CNNs, that take advantage of this additional structure (locality and translational invariance) \textbf{PAPER}. 
	\item Interesting for physics: has been recently shown these CNN architectures are intimately related to models such as tensor networks \textbf{PAPER} and, in particular, MERA-like architectures that are commonly used in physical models for quantum condensed matter systems \textbf{PAPER}.

\end{itemize}

\subsection{The structure of convolutional neural networks}
\begin{itemize}
	\item CNN = translationally invariant NN that respects locality of the input data. Backbone of many modern deep learning applications.
	\item Reader encouraged to consult notes from the Stanford C231n CNN class this section has been based on
	\item There are two kinds of basic layers that make up a CNN:
	\begin{itemize}
		\item A convolution layer that computes the convolution (a mathematical operations, see practical guide to image kernels) of the input with a bank of filters
		\item Pooling layers that coarse-grain the input while maintaining locality and spatial structure.
	\end{itemize}
	For 2D data, a layer $l$ is characterized by three numbers: height $H_l$, width $W_l$ and depth $D_l$ = the number of filters in that layer. All neurons corresponding to a particular filter have the same parameters (i.e. shared weights and bias).
	\item In general, will be concerned with local spatial filters (often called a receptive field in analogy with neuroscience) that takes as inputs a small spatial patch of the previous layer at all depths. Fex: a square filter of size $F$ = a 3D array of size $F\times F\times D_{l-1}$. The convolution consists of running this filter over all locations in the spatial plane.
	\item To demonstrate: 
	\begin{itemize}
		\item Consider simple example consisting of a 1D input of depth 1 (fig 43). A filter of size $F\times 1 \times 1$ can be specified by a vector of weights $w$ of length $F$. 
		\item The stride, $S$, encodes by how many neurons we translate the filter by when performing the convolution. In addition, it's common to pad the input with $P$ zeros. For an input of width $W$, the number of neurons (outputs) in the layer is given by $(W-F + 2P)/S + 1$. 
		\item See link: visualization of the convolution procedure for a square input of unit depth. 
		\item After computing the filter, the output is passed through a non-linearity, a ReLU in fig 43. In practice, one often inserts a BatchNorm layer before the non-linearity.
	\end{itemize}
	\item These convolutional layers are interspersed with pooling layers that coarse-grain spatial information by performing a subsampling at each depth. 
	\begin{itemize}
		\item One common operation = max pool = the spatial dimensions are coarse grained by replacing a small region (say $2\times 2$ neurons) by a single neuron whose output is the max value of the output in the region. 
		\item In \textbf{physics}, this pooling step very similar to the decimation step of RG. This generally reduces the output dimensions.
		\item Fex: If region we pool over is $2\times 2$, then both the height and width of the output layer will be halved. 
		\item Generally, pooling do not reduce the depth of the convolutional layers because pooling is performed seperately at each depth. 
		\item Some studies suggests pooling might not be necessary, but it remains a staple of most CNNs.
	\end{itemize}
	\item The convolutional and max-pool layers are generally followed by an all-to-all connected layer and a high level classifier such as soft-max. $\rightarrow$ Allows us to train CNNs as usual using the backprop algo. From a backprop perspective: CNNs almost identical to fully connected NN architectures except with tied parameters.
	\item Apart from introducing additional structure, such as translational invariance and locality, this conv structure also has important practical and computational benefits.
	\begin{itemize}
		\item All neurons at given layer represent the same filter $\rightarrow$ can all be described by a single set of weights and biases. Reduces the number of free parameters by a factor of $H\times W$ at each layer.
		\item Fex: For a layer with $D=10^2$ and $H=W=10^2$, this gives a reduction in parameters of nearly $10^6$!
		\item $\rightarrow$ Can train much larger models than would otherwise be possible with fully connected layers.
		\item Similar phenomena in physics: e.g. in translationally invariant systems we can parametrize all eigenmodes by specifying their momentum (wave number) and functional form (sin, cos, etc), while without translation invariance much more information is required.
	\end{itemize}
\end{itemize}

\subsection{Example: CNNs for the 2D Ising model}
\begin{itemize}
	\item Inclusion of spatial structure = important feature that can be exploited when designing NNs for studying physical systems.
	\item Used Pytorch to implement a simple CNN composed of 
	\begin{itemize}
		\item A single conv layer followed by a soft-max layer. 
		\item We varied the depth of the CNN layer from unity - a single set of weights and one bias - to a depth of 50 distinct weights and biases.
		\item Trained using SGD for five epochs using a training set consisting of samples from far in the paramagnetic and ordered phases. 
	\end{itemize}
	\item Fig 45 results: T
	\begin{itemize}
		\item The CNN achieved a 100\% accuracy on the test set for all architectures, even for CNN of depth one.
		\item Also checked the performance on samples drawn from the near-critical region of temperatures $T$ slightly above and below the critical temperature $T_c$. The CNN performed admirably even on these critical samples with accuracy between 80\% and 90\%. 
		\item As with all ML and NNs, performance on parts of data missing from the training data considerably worse than on test data similar to training data. Highlights the importance of properly constructing an accurate training dataset and the considerable obstacles of generalizing to novel situations. 
	\end{itemize}
	\item Regarding the SUSY dataset, we stress that the absence of spatial locality in the collision features renders applying CNNs to that problem inadequate. (Do they mean that this particular provided input data don't include spatial info, or that spatial info is actually not part of a collision? I'm assuming the first.)
\end{itemize}

\subsection{Pre-trained CNNs and transfer learning}
\begin{itemize}
	\item The immense success of CNNs for image recognition $\rightarrow$ training of huge networks on enormous datasets often by large industrial research teams from Google, Microsoft, Amacon etc. Many of these models known by name: AlexNet, GoogLeNet, ResNet, InceptionNet, VGGNet, etc.
	\item The trained models have been released, now available in standard packages such as the Torch Vision library in Pytorch, or the Caffe framework. They can be used directly as a basis for fine-tuning in different supervised image recognition tasks through a process called transfer learning.
	\item Transfer learning basic idea: The filters (receptive fields) learned by the conv layers of these networks should be informative for most image recognition based tasks, not just the ones the were originally trained for. This turns out to be true in practice for many tasks one might be interested in. 
	\item Three distinct ways one can take a pretrained CNN and repurpose it for a new task:
	\begin{itemize}
		\item \textbf{Use CNN as fixed feature detector at top layer.}
		If new dataset we want to train on small and similar to original dataset, can simply use the CNN as a fixed feature detector and retrain our classifier = remove the classifier (soft-max) layer at the top of the CNN and replace it with a new classifier (liner SVM or soft-max) relevant to our problem. Here, the CNN serves as a fixed map from images to relevant features (the outputs of the top fully-connected layer right before the original classifier). This prevents overfitting on small, similar datasets. Often useful starting point for transfer learning.
		\item \textbf{Use CNN as fixed feature detector at intermediate layer.}
		If dataset small and different from the one used to train the original model, the features at the top level might not be suitable for our dataset. $\rightarrow$ may want to instead use features in the middle of the CNN to train our new classifier. These features thought to be less fine-tuned and more universal (e.g. edge detectors). This is motivated by the idea that CNNs learn increasingly complex features the deeper one goes in the network (see discussion on representational learning in next section)
		\item \textbf{Fine-tune the CNN.}
		If dataset large: in addition to replacing and training the classifier in the top layer, can also fine-tune the weights of the original CNN using backprop. May choose to freeze some of the weigths in the CNN during the procedure or retrain all of them simulatneously. 
	\end{itemize}
	\item All this can be carried out easily using packages such as Caffe or the Torch Vision library in PyTorch.
\end{itemize}



\section{High-level concepts in Deep Neural Networks}

\subsection{Organizing deep learning workflows using the bias-variance tradeoff}
\begin{itemize}
	\item Imagine you're given some data and asked to design an NN for learning how to perform a supervised learning task. What are the best practices for organizing a systematic workflow that allows us to efficiently do this?
	\item Here we present a simple deep learning workflow inspired by thinking about the bias-variance tradeoff (fig 46). This section draws heavily on tutorial from the Deep Learning School.
	\item \textbf{First thing} we would like to do: divide data in three: training set, validation/development/dev set, and test set. Use validation error as proxy for the test error in order to make tweaks to our model. OBS: do not use any test data to train the algo. This is a cardinal sin in ML.
	\item \textbf{Estimate optimal error rate (Bayes rate)}
	\begin{itemize}
		\item Establish the difficulty of the task and the best performance one can expect to achieve. No algo can do better than the "signal" in the dataset. Fex: it's likely much easier to classify objects in high resolution images than in blurry, low-resolution ones. $\rightarrow$ Must establish a proxy or baseline for the optimal performance that can be expected from any algo. 
		\item In Bayesian statistics, this known as the Bayes rate. Since we don't know this \textit{a priori}, we must get an estimate. For many tasks inc speech and object recognition, can approximate this by humans' performance on it. For more specialized task, we would like to ask how well experts, trained at the task, perform. This expert formance then serves as proxy for our Bayes rate.
	\end{itemize}
	\item \textbf{Minimize underfitting (bias) on training data set.}
	\begin{itemize}
		\item Having the Bayes rate, want to ensure we are using a sufficiently complex model to avoid underfitting on the training data.
		\item In practice this means: comparing the training error to the Bayes rate. Since the training error doesn't care about generalization (variance), our model should approach the Bayes rate in the training set.
		\item If not, bias of the DNN is too large and should try training the model longer and/or use larger model.
		\item If none of these techniques work, likely the model architecture not well suited to the data, should modify the architecture in some way to better reflect the underlying structure of the daya (symmetries, locality, etc.)
	\end{itemize}
	\item \textbf{Make sure you are not overfitting.}
	Next, run our algo on the validation/dev set.
	\begin{itemize}
		\item If error similar to training error rate and Bayes rate, we're done.
		\item If not, we are overfitting the training data. Possible solutions include: regularization and, importantly, collecting more data.
		\item If none of these work: likely has to change the DNN architecture.
	\end{itemize}
	\item The result
	\begin{itemize}
		\item If validation and test set drawn from same distributions, good performance on validation set should $\Leftrightarrow$  good performance on the test set. (But typically slightly worse because the hyperparameters were fit to the validation data). 
		\item But, sometimes training and test data differ in subtle ways. Fex: they're collected using slightly different methods, or it's cheaper to collect data one way vs another. (But why on earth would you then divide training/test data based on this difference? Wouldn't you randomize it?) Rectification: Make two validation/dev sets, one from training data and one from test data. The difference in performance on the two quantifies the train-test mismatch. This can serve as another impoertant diagnostic when using DNNs.
	\end{itemize}
\end{itemize}

\subsection{Why neural networks are so successful: three high-level perspectives on neural networks}
As pointed out earlier, the field is rapidly expanding, and many of these perspective may out to be only partially true or even false. Nonetheless, included as guidepost for readers.

\subsubsection{Neural networks as representation learning}
\begin{itemize}
	\item Powerful aspect of deep learning: The ability to learn relevant features with relatively little domain knowledge/minimal hand-crafting. Power of deep learning often stems from its ability to act like a black box: take in a large stream of data - find good features capturing the properties of the data we're interested in.
	\item This ability to learn good representations with very little hand-tuning = one of the most attractive properties of DNNs. 
	\item Many of the other supervised-learning algos discussed (regression-based models, ensemble methods inc random forests or gradient-boosted trees) perform comparably or even better than NNs - but when using hand-crafted features with small-to-intermediate sized datasets.
	\item The hierarchical structure of deep learning = thought to be crucial to their ability to represent complex, abstract features. Fex: how analysis of CNNs for image classification suggests the lower-levels of the net learn elementary features, such as edge detectors, which are then combined into higher levels of the net into more abstract, higher-level features (e.g. the famous example of a neuron that "learned to respond to cats"). 
	\item Has been shown more recently that CNNs can be thought of as \textbf{performing tensor decompositions on the data similar to those commonly used in numerical methods in modern quantum condensed matter. PAPER}.
	\item Interesting consequence of this thinking: One can train a CNN on one large dataset and the features it learn should also be useful for other supervised tasks $\rightarrow$ the ability to learn important and salient features directly from the data, then transfer the knowledge to new task. This ability to learn important, higher-level, coarse grained features is reminiscent of \textbf{ideas like the renormalization group (RG) in physics where the RG flows seperate out relevant and irrelevant directions, and certain unsupervised deep learning architectures have a natural interpretation in terms of variational RG schemes. PAPER}. 
\end{itemize}

\subsubsection{Neural networks can exploit large amounts of data}
\begin{itemize}
	\item Data explosion
	\item DNNs are able to exploit the additional signal in large datasets for difficult supervised learning tasks.
	\item Fundamentally, modern DNNs are unique in that they contain millions of parameters, yet can still be trained on existing hardwares. Complexity of DNNs (in terms of parameters) combined with their simple architecture (layer-wise connections) hit a sweet spot between expressivity (ability to represent very complicated functions) and trainability (ability to learn millions of parameters).
	\item Indeed, DNN's ability to exploit large datasets thought to differ from many other commonly employed supervised learning methids, fex Support Vector Machines (SVMs). Fig 47: schematic depicting expected performance of DNNs of different sizes with the number of data samples and compares them to supervised learning algos such as SVMs or ensemble methods.
	\begin{itemize}
		\item When amount of data small: DNNs offer no substantial benefit over these other methods and often performs worse.
		\item But: large DNNs seem to be able to exploit additional data in a way other methods cannot. 
	\end{itemize}
	\item Fact that one doesn't have to hand engineer features makes the DNN even more well suited for handling large datasets. 
	\item Recent theoretical results suggest that as long as a DNN large enough, should generalize well and not overfit \textbf{ARTICLE}.
\end{itemize}

\subsubsection{Neural networks scale up well computationally}
\begin{itemize}
	\item Modern NNs can harness the immense computational capability that has occurred over the last few decades. Architecture of NNs naturally lends itself to parallelization and the exploitation of fast but specialized processors such as graphical processing units (GPUs). 
	\item Google and NVIDIA set on a course to develop TPUs (tensor processing units) which will be specifically designed for the matematical operations underlying deep learning architectures.
	\item The layered architecture of NNs also makes it easy to use modern techniques such as automatic differentiation that make it easy to quickly deploy them.
	\item Algos such as SGD and use of mini-batches make it easy to parallelize code and train much larger DNNs than was thought possible fifteen years ago.
	\item Many of these computational gains are quickly incorporated into modern packages with industrial resources $\rightarrow$ makes it easy to perform numerical experiments on large datasets, leading to further engineering gains.
\end{itemize}

\subsection{Limitations of supervised learning with deep networks}
Supervised learning using NNs has important limitations, like all statistical methods. Especially important when seeking to apply these methods to physics. Often, the same or better performance on a task can be achieved by using a few hand-engineered featuers (or even a collection of random features). Especially important for hard physics problems where data/Monte-Carlo samples maybe hard to come by. Some important limitations:
\begin{itemize}
	\item \textbf{Need labeled data.}
	Like all supervised learning methods. Can be harder to acquire than unlabeled data (e.g. must pay human experts to label images).
	\item \textbf{Supervised neural networks are extremely data intensive.}
	Utility of DNNs extremely limited if data is hard to acquire or the datasets are small (hundreds to a few thousands samples). In this case, performance of other methods that utilize hand-engineered features can exceed that of DNNs.
	\item \textbf{Homogeneous data.}
	Almost all DNNs deal with homogeneous data of one type. Very hard to design architectures that mix and match data types (i.e. some continuous variables, some discrete variables, some time series). In appplications beyond images, video and language, this is often what is required. In contrast, ensemble methods like random forests or gradient-boosted trees have no difficulty handling mixed data types.
	\item \textbf{Many physics problems are not about prediction.}
	Often not interested in solving prediction tasks such as classification. Want to learn something about the underlying distribution that generates the data. Then, often difficult to cast these ideas in a supervised learning setting. While the problems are related, it's possible to make good predictions with a "wrong" model. The model might or might not be useful for understanding physics.
\end{itemize}
Some of these remarks particular to DNNs, other shared by all supervised learning methods. Motivates the use of unsupervised methods which in part circumnavigate these problems.

\section{Dimensional reduction and data visualization}
\begin{itemize}
	\item Will begin our foray into unsupervised learning by way of data visualization = an important tool in ML to identify structures such as
	\begin{itemize}
		\item Correlations
		\item Invariances (symmetries)
		\item Irrelevant features (noise)
	\end{itemize}
	in raw or processed data.
	\item Conceivably, capturing these properties could help us design better predicitve models. In practice, however, the data we deal with is often high-dimensional = its visualization is impossible - daunting at best.
	\item Part of the complication = low-dimensional representation of high-dimensional data necessarily incurs information lost.
	\item Simple way to visualize data: pair-wise correlations (= pairwise scatter plots for all features). Useful in highlighting the important correlations between features when the number of features we are measuring is relatively small. 
	\item In practice, we often have to perform \textit{dimensional reduction} = project the data onto a lower dimensional space = \textit{the latent space}.
	\item We discuss both linear and non-linear methods for dimensional reduction with applications in data visualization. The techniques can be used in many other applications also, inc lossy data compression and feature extraction.
\end{itemize}

\subsection{Some of the challenges of high-dimensional data}
\begin{itemize}
	\item \textit{High-dimensional data lives near the edge of sample space}
	Geometry in high-dimensional space can be counterintuitive. Example pertinent to ML: 
	\begin{itemize}
		\item Consider data distributed uniformely at random in a $D$-dimensional hypercube $\mathcal{C} = [-e/2, e/2]^D$, where $e$= the edge length.
		\item Consider also a $D$-dimensioanl hypersphere $\mathcal{S}$ of radius $e/2$ centered at the origin and contained within $\mathcal{C}$.
		\item The prob that a data point $\bm{x}$ drawn uniformly at random in $\mathcal{C}$ is contained within $\mathcal{S}$ is well approximated by the ratio of the volume of $\mathcal{S}$ to that of $\mathcal{C}$: 
		$p(||\bm{x}||_2 < e/2) \sim (1/2)^D$.
		\item Thus, as the dimension of the feature space $D$ increases, $p$ goes to zero exponentially fast. = most of the data will concentrate outside the hypersphere, in the corners of the hypercube.
		\item In \textbf{physics}, this basic observation underlies many properties of ideal gas such as the Maxwell distribution and the equipartition theorem.
	\end{itemize}
	\item \textit{Real-world data vs. uniform distribution}
	Fortunately, real-world data is not random or uniformally distributed!
	\begin{itemize}
		\item Real data usually lives in a much lower dimensional space than the original space in which the features are being measured (see very helpful fig 48). 
		\item "The blessing of non-uniformity" (vs the curse of dimensionality)
		\item Data will typically be locally smooth (= a local variation of the data will not incur a change in the target variable)
		\item The idea = similar to statistical physics, where roperties of most systems w/many degrees of freedom can often be characterized by low-dimensional 'order parameters'. 
		\item In thermodynamics, bulk properties of a gas of weakly interacting particles can be simply described by the thermodynamic variables that enter equation of states rather than astronomically large dynamical variables (= position and momentum) of each particle in the gas is another instantiation of this idea.
	\end{itemize} 
	\item \textit{The crowding problem}
	\begin{itemize}
		\item When performing dimensional reduction, a common goal is to preserve pairwise distances between the data points from the original space to the latent space.
		\item Can be fairly well achieved if the \textit{intrinsic} dimensionality of the data (=that in the original space) is the same as the dimension of the latent space. Again see fig 48 ('rullekake' eksempelet.)
		\item But, if one attempts to represent data in a space with dimensionality lower than the intrinsic one, the problem of 'overcrowding' can occur. Means low-dimensional embedding of high-dimensional data are often ambiguous. = two points far apart in the data space are mapped to the vicinity of each other in the latent space. 
		\item To alleviate this, one needs to weaken the constraint we impose on our visualization schemes. Fex: in the case of t-distributed stochastic embedding (t-SNE), one prioritizes the preservation of short distances or local ordination(?) rather than that of all pairwise distances.
	\end{itemize}
\end{itemize}


\subsection{Principal component analysis (PCA)}
\begin{itemize}
	\item Goal: perform a linear projection of the data onto a lower-dimensional subspace where the variance is maximized. Inspired by the observation that in many cases, relevant information is often contained in the directions with largest variance. (fig 50, helpfull!)
	\item Intuitively, these dirs encode the large-scale 'signal' as opposed to 'noise' characterized by the dir of small variance. 
	\item PCA also seeks variable dirs while simultaneously reducing the redundancy between new basis vectors. Done by requiring our new basis vectors (=principal components) be orthogonal.
	\item Data then visualized by projecting it onto a subspace spanned by a few principal component basis vectors.
	\item Surprisingly, such PCA-based projections often capture a lot of the large scale structure of many datasets. Fex, fig 51 (cool!): shows the projections of samples drawn from the 2D Ising model at various temperatures on the first two principal components. Despite living in a 1600 dimensional space (the samples are $40 \times 40$ spins), a single component (i.e. a single direction in this 1600 dimensional space) can capture 50\% of the variability contained in our samples. One can actually check that easily that this direction weights all 1600 spins equally and thus \textbf{corresponds to the magnetization parameter}. $\rightarrow$ even without any prior physical knowledge, one can extract relevant order parameters using a  simple PCA-based projection.
	\item PCA is widely employed in biological physics when working with high-dimensional data. Recently, \textbf{a correspondence between PCA and Renormalization Group flows across the phase transition in the 2D Ising model or in a general setting has been proposed}. In stat phys, PCA has also found application in detecting phase transitions, e.g. in the XY model on frustrated triangular and union jack lattices. Also used to classify dislocation patterns in crystals. Physics has also inspired PCA-based algos to infer relevant features in unlabelled data.
	\item Concretely,
	\begin{itemize}
		\item Consider $N$ data points $\{ \bm{x}_1,..., \bm{x}_N \}$ that live in a $D$-dimensional feature space $\mathbb{R}^D$. 
		\item Without loss of generality, we assume the empirical mean $\bar{\bm{x}} = N^{-1} \sum_i \bm{x}_i $ of these data points is zero (we can always center around the mean: $\bar{\bm{x}} = \bm{x}_i - \bar{\bm{x}}$).
		\item Denote $N\times D$ design matrix as $X = [\bar{\bm{x}}_1, \bar{\bm{x}}_2, ..., \bar{\bm{x}}_N]^T$ whose rows = the data points and columns = different features. The $D\times D$ (symmetric) covariance matrix is therefore
		\begin{align}
			\Sigma (X) = \frac{1}{N-1} X^T X
		\end{align}
		Notice 
		\begin{itemize}
			\item The $j$-th diagonal entry of $\Sigma (X)$ = the variance of $j$-th feature 
			\item The $\Sigma (X)_{ij}$ measures the covariance (i.e. \textbf{connected correlation in the language of physics}) between feature $i$ and $j$.
		\end{itemize}
		\item We want: find a new basis for the data that emphasizes highly variable directions while reducing redundancy between basis vectors. In particular, we'll look for \textbf{a linear transformation that reduces the covariance between different features}. To do so, 
		\begin{itemize}
			\item First, perform a singular value decomposition (SVD) on design matrix $X$, namely: $X=USV^T$, where $S$=a diagonal matrix of singular value $s_i$, the orthogonal matrix $U$ contains (as its columns) the left singular vectors of $X$, and similarly $V$ contains (as its columns) the right singular vectors of $X$.
			\item With this, we can rewrite the covariance matrix as
			\begin{align}
				\Sigma (X) =& \frac{1}{N-1} VSU^T USV^T \\
				=& V (\frac{S^2}{N-1}) V^T
				=& V \Lambda V^T
			\end{align}
			where $\Lambda$= a diagonal matrix w/eigenvalues $\lambda_i$ in the decreasing order along the diagonal (i.e. eigendecomposition).
			\item Clear that the right singluar vectors of $X$ (=columns of $V$) are principle directions of $\Sigma (X)$, and 
			\item Singular values of $X$ are related to the eigenvalues of covariance matrix $\Sigma (X)$ via $\lambda_i = s_i^2 / (N-1)$.
			\item To reduce the dimensionality of data from $D$ to $\tilde{D} < D$, first construct the $D\times \tilde{D}$ projection matrix $\tilde{V}_{D'}$ by selecting the singular components with the $\tilde{D}$ largest singular values.
			\item The projection of the data from $D$ to a $\tilde{D}$ dimensional space is simply $\tilde{Y} = X \tilde{V}_{D'}$.
			\item The singular vector w/the largest value (=largest variance) is referred to as the first principal component, the singular vector w/the second largest singular value as the second principal component, etc.
			\item An important quantity: the ratio $\lambda_i / \sum_{i=1}^D \lambda_i$ = the percentage of the explained variance contained in a principal component. (fig 51b)
		\end{itemize}
		\item Common in data visualization to represent data projected on the first few principal components. Valid as long as a large part of the variance is explained in those components. 
		\item Low values of explained variance may imply the intrinsic dimensionality of the data is high or simply that it cannot be captured by a linear representation.
	\end{itemize}
\end{itemize}

\subsection{Multidimensional scaling}
\begin{itemize}
	\item MDS = a non-linear dimensional reduction technique which preserves the pairwise distance/dissimilarity $d_{ij}$ between data points. To types of MDS:
	\begin{itemize}
		\item Metric MDS: the distance matrix is computed under a pre-defined metric and the latent coordinates $\tilde{Y}$ are obtained by minimizing the difference between the distance matrix in the original space ($d_{ij} (X)$) and that in the latent space ($d_{ij} (Y)$):
		\begin{align}
			\tilde{Y} = \text{argmin}_Y \sum_{i<j} w_{ij}|d_{ij} (X) - d_{ij}(Y)|
		\end{align}
		where $w_{ij}$ is a weight value: $w_{ij} \geq 0$. The weight matrix $w_{ij}$ is a set of free parameters that specify the level of confidence (or precision) in the value of $d_{ij} (X)$. If Eucledian metric used, it's the same as PCA and usually called "classical scaling". Thus MDS often considered generalization of PCA. 
		\item Non-metric MDS: $d_{ij}$ can be any distance matrix. The objective function is then to preserve the ordination in the data. I.e. if $d_{12} (X) < d_{13} (X)$ in the original space, then in latent space we should have $d_{12} (X) < d_{13} (X)$.  
	\end{itemize}
	\item Both PCA and MDS can be implemented using standard Python packages such as Scikit. MDS algo typically typically have a scaling of $\mathcal{O} (N^3)$ where $N$=number of data points. 
	\item PCA: if one is only interested in a small fraction of the principal components w/the largest variance (which is the usual case), efficient implementations based on Lanczos methods can achieve scaling of $\mathcal{O} (N^2)$ for dense matrices.
	\item PCA and MDS often among the first data visualization techniques one resorts to.
\end{itemize}

\subsection{t-SNE}
\begin{itemize}
	\item Often desirable to preserve local structures in high-dimensional datset. Typically not possible using linear techniques such as PCA.
	\item Many non-linear techniques such as non-classical MDS, self-organizing map, Isomap and Locally Linear Embedding have been proposed recently. These techniques generally good at preserving local structures in the data, but typically \textit{fail to capture structures at the larger scale such as the clusters in which the data is organized.} 
	\item Recently, $t$-stochastic neighbor embedding (t-SNE) has emerged as one of the go-to methods for visualizing high-dimensional data.
	\item t-SNE = a non-parametric method that utlizes non-linear embeddings. When used appropriately, a powerful technique for unraveling the hidden structures of high-dimensional datasets AND preserving locality.
	\item In \textbf{physics}, t-SNE has recently been used to reduce the dimensionality of and classify spin configs, generated with Monte-Carlo simulations, for the Ising and Fermi-Hubbard models at finite temps. Was also applied to study clustering transitions in random satisfiability problems which bears close resemblance to spin glass models.
	\item Idea of stochastic neighborhood embedding (SNE):
	\begin{itemize}
		\item Associate a prob dist to the neighborhood of each data (note $x \in \mathbb{R}^s$, $s$ the number of features):
		\begin{align}
			p_{i|j} = \frac{e^{-\frac{||x_i - x_j||^2}{2\sigma_i^2}}}{\sum_{k\neq i}e^{-\frac{||x_i - x_k||^2}{2\sigma_i^2}}}
		\end{align}
		where $p_{i|j}$ can be interpreted as the likelihood that $x_j$ is $x_i$'s neighbor. $\sigma_i$ are free parameters that are usually fixed by fixing the local entropy (=the perplexity) of each data point (regions of high denisty therefore have smaller $\sigma_i$). 
		\item Intuitively, the Gaussian likelihood (i.e. short-tailed) means that only points that are nearby $x_i$ contribute to its prob dist. 
		\item A symmetrized prob dist is constructed from the above equation: $p_{ij} \equiv (p_{i|j} + p_{j|i})/(2N)$. The symmetrization ensures even outliers contribute $p_{ij}$ and as such, have meaningful embedding coordinates.
	\end{itemize}
	\item $t$-SNE, on the other hand
	\begin{itemize}
		\item Constructs an equivalent prob dist in a low dimensional latent space (with coordinates $y_i \in \mathbb{R}^t$, $t<s$, with $t$ the dimension of the latent space):
		\begin{align}
			q_{ij} = \frac{(1 + ||y_i -y_j||^2)^{-1}}{\sum_{k\neq j} (1+||y_i - y_k||^2)^{-1}}
		\end{align}
		The crucial point = $q_{ij}$ is chosen to be a long-tail (Cauchy) distribution. This is meant to preserve short distance information (relative neighborhoods) while strongly repelling two points that are far apart in the original space (fig 52). 
		\item In order to find the latent space coordinates $y_i$, t-SNE minimizes the Kullbach-Leibler divergence between $q_{ij}$ and $p_{ij}$:
		\begin{align}
			KL(p||q) \equiv \sum_{ij} p_{ij} \text{log} (\frac{p_{ij}}{q_{ij}})
		\end{align}
		Achieve this by gradient descent. 
	\end{itemize}
	\item When visualizing data with $t$-SNE, important to understand what is plotted. Some important properties to bear in mind when analyzing t-SNE plots:
	\begin{itemize}
		\item \textit{t-SNE can rotate data}
		The KL divergence is invariant under rotations in the latent space, since it only depends on the distance between points. $\rightarrow$ t-SNE plots that are rotations of each other should be considered equivalent.
		\item \textit{t-SNE results are stochastic}
		Although KL divergence is convex in the domain of distributions, it is generally not in the domain of $q_{ij}$ (i.e. latent coordinate $y$). $\rightarrow$ in applying gradient descent the solution will depend on the initial seed. $\rightarrow$ the map obtained may very depending on the seed used and different t-SNE runs will give slightly different results.
		\item \textit{t-SNE generally preserves short distance information}
		Rule of thumb: expect nearby points on the t-SNE points are also closeby in the original space. The reason is the nature of the mapping discussed in fig 52.
		\item \textit{Scales are deformed in t-SNE}
		Since a scale-free distribution is used in the latent space, one should not put too much emphasis on the meaning of the variances of any clusters observed in the latent space.
		\item \textit{t-SNE is computationally intensive}
		A direct implementation of t-SNE has an algo complexity of $\mathcal{O}(N^2)$ which is only applicable to small to medium data sets.
		Improved scaling of the form $\mathcal{O}(N\text{log}N)$ can be achieved at the cost of approximatin $KL(p||q)$ by using Barnes-Hut method.
	\end{itemize}
	\item Illustration:
	\begin{itemize}
		\item Fig 53: t-SNE applied to a model consisting of thirty Gaussians (=a Gaussian mixture model) whose means are uniformly distributed in a forty-dimensional space. Compared the results to a random 2D projection and PCA. 
		\item Clear that unlike more naive dimensional reduction techniques, both PCA and t-SNE can identify the presence of well-formed clusters. The t-SNE visualization cleanly separates all the clusters while certain clusters blend together in the PCA plot. = A direct consequence of the fact that t-SNE keeps nearby points close together while repelling points that are far apart.
		\item Fig 54: t-SNE and PCA plots for MNIST dataset of ten hand-written numerical digits (0-9).
		\item Clear that the non-linear nature of t-SNE makes it much better at capturing and visualizing the complicated correlations between digits than the PCA.
	\end{itemize}
\end{itemize}

\section{Clustering}
\begin{itemize}
	\item We continue discussion of unsupervised learning methods. Unsupervised learning concerned with: discovering structure in unlabeled data (fex learning local structures for data visualization).
	\item More difficult than supervised because unlabeled data. Surprising: still possible to uncover and exploit hidden structure in the data.
	\item Perhaps simplest expample of unsupervised learning = clustering. Aim: group unlabeled data into clusters according to some similarity or distance measure. Informally thought of as: a set of points sharing some pattern or structure.
	\item Many applications of clustering in
	\begin{itemize}
		\item Data mining
		\item Data compression
		\item Signal processing
		\item Can be used to identify coarse features or high level structures in an ulabelled dataset.
		\item Applications in physical sciences:
		\begin{itemize}
			\item Detecting celestial emission sources in astronomical surveys
			\item Inferring groups of genes and proteins with similar functions in biology
			\item Building entaglement classifiers
		\end{itemize}
	\end{itemize}
	\item Clustering=vast field, flurry of methods suited for different purposes. Common considerations when choosing method: 
	\begin{itemize}
		\item the distributions of the clusters (overlapping/noisy clusters vs well-separated clusters), 
		\item the geometry of the data (flat vs non-flat), 
		\item the cluster size distribution (multiple sizes vs uniform sizes), 
		\item dimensionality of the data (low vs high dimensional) and 
		\item the computational efficiency of the desired method (small vs large dataset).
	\end{itemize}
\end{itemize}

\subsection{Practical clustering methods}
Throughout this section focus on Ecluedian distance as similarity measure. See reference for more in depth discussion of different possible similarity measures.
\subsubsection{K-means}
\begin{itemize}
	\item Consider a set of $N$ \textit{unlabeled} observations $\{ \bm{x}_n \}_{n=1}^N$ where $\bm{x}_n \in \mathbb{R}^p$ and where $p$ is the number of features. 
	\item Also consider a set of $K$ cluster centers called the cluster \textit{means}: $\{ \bm{\mu}_k \}_{k=1}^K$ with $\bm{\mu}_k \in \mathbb{R}^p$. The cluster means can be though of as the representatives of each cluster, to which datapoints are assigned (fig 55). 
	\item $K$-means clustering can be formulated as follows:
	\begin{itemize}
		\item Given a fixed integer $K$, find the cluster means $\{\bm{\mu} \}$ and the data point assignments in order to minimize the following objective function:
		\begin{align}
			J(\{ x, \bm{\mu} \}) = \sum_{k=1}^K \sum_{n=1}^N r_{nk} (\bm{x}_n - \bm{\mu}_k)^2
		\end{align}
		where $r_{nk}$=a binary variable ($r_{nk} \in \{ 0,1 \}$) called the assignment.
		\item The assignment $r_{nk}$ is 1 if $x_{n}$ is assigned to cluster $k$ and 0 otherwise. Notice that $\sum_k r_{nk} = 1 \quad \forall \quad n$ and $\sum_n r_{nk} \equiv N_k$, where $N_k$= the number of points assigned to cluster $k$.
		\item The minimization of this objective function can be understood as trying to find the best cluster means such that the variance within each cluster is minimized. 
		\item In physical terms: $J$=the sum of the moments of inertia of every cluster. Indeed, as we'll see below, the cluster means $\bm{\mu}_k$ correspond to the centres of mass of their respective cluster.
	\end{itemize} 
	\item $K$\textbf{-means algorithm} $K$-means algo alternates between two steps:
	\begin{enumerate}
		\item \textit{Expectation}: Given a set of assignments $\{ r_{nk} \}$, minimize $J$ wrt $\bm{\mu}_k$. Taking a simple derivative and setting it to zero yields the update rule:
		\begin{align}
			\bm{\mu}_k = \frac{1}{N_k} \sum_n r_{nk} \bm{x}_n
		\end{align}
		\item \textit{Maximation}: Given a set of cluster means $\{ \bm{\mu}_k \}$, find the assignments $\{ r_{nk} \}$ which minimizes $J$. Clearly, this achieved by assigning each data point to their nearest cluster-mean:
		\[ r_{nk} = \begin{cases}
				 1, & \text{if } k = \text{argmin}_{k'} (\bm{x}_n - \bm{\mu}_{k'})^2 \\
				 0, & \text{otherwise}  
		\end{cases} \]
	\end{enumerate}  
	$K$-means clustering consists in alternating between these two steps until some convergence criterion met. Practically it should terminate when the change in the objective func from one iteration to another becomes smaller than a pre-specified threshold. Fig 55: Simple example of algo.
	\item Nice property: $K$-means is guaranteed to converge. To see this: can verify explicitly (by taking second-order derivatives) that the expectation step always decreases $J$. Also true for the assignment step. $\rightarrow$ since $J$ bound from below, the two-step iteration \textit{always} converges to a local min of $J$. 
	\item $J$ generally a non-convex func $\rightarrow$ in practice one usually needs to run the algo with different initial random seeds and post-select the best local minimum. 
	\item A simple implementation has an average computational complexity which scales linearly in the size of the data set (that is, $\mathcal{O}(KN)$ pr iteration) and is thus scalable to very large datasets.
	\item As we'll see later, $K$-means is a hard-assignment limit of the Gaussian mixture model where all cluster variances are assumed to be the same. 
	\item This highlights a common drawback of $K$-means: if the true clusters have very different variances (spreads), $K$-means can lead to spurious results since the underlying assumption is that the latent model has uniform variances.
\end{itemize}

\subsubsection{Hierarchical clustering: Agglomerative methods}
\begin{itemize}
	\item Agglomerative clustering =A bottom-up approach that starts from small initial clusters which are then progressively merged to form larger clusters.
	\item The merging process generates a hierarchy of clusters that can be visualized in the form of a dendrogram (fig 56). This hierarchy can be useful to analyze the relation between clusters and the subcomponents of individual clusters. 
	\item Agglomerative methods usually specified by defining a distance measure between clusters (the measure need not be metric). Denote the distance between clusters $X$ and $Y$ by $d(X,Y) \in \mathbb{R}$. Different choices of distance result in different clustering algos.   
	At each step, the two clusters that are closest wrt the distance measure are merged until a single cluster is left.
	\item \textbf{Agglomerative clustering algo}:
	\begin{enumerate}
		\item Initialize each point to its own cluster
		\item Given a set of $K$ clusters $X_1, X_2, ..., X_K$, merge clusters until one cluster is left (K=1):
		\begin{enumerate}
			\item Find the closest pair of clusters $(X_i, X_j): \quad (i,j) = \text{argmin}_{(i', j')} d(X_{i'}, X_{j'})$
			\item Merge the pair. Update $K \leftarrow K -1 $
		\end{enumerate}
	\end{enumerate}
	\item A few of the most popular distances used in agglomerative methods (often called linkage methods):
	\begin{enumerate}
		\item Single-linkage: the distance between clusters $i$ and $j$ = the minimum distance between two elements of the different clusters:
		\begin{align}
			d(X_i, X_j) = \text{min}_{\bm{x}_i \in X_i, \bm{x}_j \in X_j} ||\bm{x}_i -\bm{x}_j||_2
		\end{align}
		\item Complete linkage: the distance between clusters $i$ and $j$ = the maximum distance between two elements of the different clusters
		\begin{align}
			d(X_i, X_j) = \text{max}_{\bm{x}_i \in X_i, \bm{x}_j \in X_j} ||\bm{x}_i -\bm{x}_j||_2
		\end{align}
		\item Average linkage: average distance between points of differetn clusters
		\begin{align}
			d(X_i, X_j) = \frac{1}{|X_i|\cdot |X_j|} \sum_{\bm{x}_i \in X_i, \bm{x}_j \in X_j} ||\bm{x}_i -\bm{x}_j||_2
		\end{align}
		\item Ward's linkage: Analogous to the $K$-means method as it seeks to minimize the total inertia. The distance measure is the "error squared" before and after merging which simplifies to:
		\begin{align}
			d(X_i, X_j) = \frac{|X_i||X_j|}{|X_i \cup X_j|} (\bm{\mu}_i - \bm{\mu}_j)
		\end{align}
	\end{enumerate}
	\item Common drawback of hierarchical methods: they don't scale well: at every step, a distance matrix between all clusters must be updated/computed. Efficient implementations achieve a typical computational complexity of $\mathcal{O}(N^2)$, making the method suitable for small to medium datasets.
	\item A simple but major speed-up: initialize the clusters with $K$-means using a large $K$ (but still a small fraction of $N$), then proceed with hierarchical clustering. Advantage: this preserves the large-scale structure of the hierarchy while making use of the linear scaling of $K$-means. In this way hierarchical methods may be applied to very large datasets.
\end{itemize}

\subsubsection{Density-based (DB) clustering}
\begin{itemize}
	\item Makes the intuitive assumption that clusters are defined by regions of space w/higher density of data points. Data points that constitute noise or that are outliers are expected to form regions of low density. This has the advantage of being able to consider clusters of multiple shapes and sizes while identifying outliers. Also suitable for large-scale applications.
	\item Core assumption: \textit{relative} local density estimation of data is possible. = possible to order points according to their densities. Density estimates usually accurate for low-dimensional data but become unreliable for high-dimensional data due to large sampling noise.
	\item Here, we confine our discussion to one of the most widely used density clustering algos DBSCAN. We have also had great success with another recently introduced DB clustering variant which is similar in spirit. Also one of the authors has created a Python package which makes use of accurate density estimates via kernel methods combined with agglomerative clustering to produce fast and accurate density clustering (GitHub).
	\item \textbf{DBSCAN algo} DBSCAN = density-based spatial clustering of applications with noise.
	\begin{itemize}
		\item Consider a set of $N$ data points $X \equiv \{ \bm{x}_n \}_{n=1}^N$.
		\item We start by defining the $\epsilon$-neighborhood of point $\bm{x}_n$ as follows:
		\begin{align}
			N_\epsilon (\bm{x}_n) = \{ \bm{x} \in X | d(\bm{x}, \bm{x}_n) < \epsilon \}
		\end{align}
		$N_\epsilon (\bm{x}_n)$ are the data points at a distance smaller than $\epsilon$ from $x_n$. As before, consider $d(\cdot, \cdot)$ to be the Eucledian metric (which yields spherical neighborohoods, fig 57) but other metrics may be better suited depending on the data. 
		\item $N_\epsilon (\bm{x}_n)$ can be seen as a crude estimate of local density. $\bm{x}_n$ is considered to be a \textit{core-point} if a least \textbf{minPts} are in its $\epsilon$-neighborhood. \textbf{minPts} = a free parameter of the algo that sets the scale of the size of the smallest cluster one should expect. 
		\item Finally, a point $\bm{x}_i$ = \textit{density-reachable} if it is in the $\epsilon$-neighborhood of a \textit{core-point}. 
		\item From these definitions, the algo can be simply formulated:
		\item $\rightarrow$ Until all points in $X$ have been visited; \textbf{do}
		\begin{itemize}
			\item Pick a point $\bm{x}_i$ that has not been visited
			\item Mark $\bm{x}_i$  as a visited point
			\item If $\bm{x}_i$ is a core point; \textbf{then}
			\begin{itemize}
				\item Find the set $\mathcal{C}$ of all points that are \textit{density-reachable} from $\bm{x}_i$.
				\item $\mathcal{C}$ now forms a cluster. Mark all points within that cluster as being visited.
			\end{itemize}
		\end{itemize}
		\item $\rightarrow$ Return the cluster assignments $\mathcal{C}_1,...,\mathcal{C}_k$, with $k$=the number of clusters. Points that have not been assigned to a cluster are considered noise or out-liers.
		\item Note: DBSCAN doesn't require the user to specify the number of clusters but only $\epsilon$ and \textbf{minPts}. While it's common to heuristically fix these parameters, methods such as cross-validation can be used for their determination. 
		\item Finally, note: DBSCAN very efficient since efficient implementations have a computational cost of $\mathcal{O} (N\text{log}N)$.
	\end{itemize}
\end{itemize}

\subsection{Clustering and latent variables via the Gaussian mixture models}
\begin{itemize}
	\item We'll here approach clustering from a more abstract vantage point, and in the process, introduce many of the core ideas underlying supervised learning. 
	\item Central concept in many unsupervised learning techniques: the idea of a latent or hidden variable. Though not directly observable, latent variables still influence the visible structure of the data. Fex: in clustering we can think of the cluster identity of each datapoint (=the cluster the data point belongs to) as a latent variable. Even though can't see the cluster label explicitly, we know points in same cluster tend to be closer together. The latent variables in our data (cluster identity) are a way of representing and abstracting the correlations between datapoints.
	\item In this language: can think of clustering as an algo to learn the most probable value of a latent variable (cluster identity) associated with each datapoint. Calc this latent variable requires additional assumptions about the structure of our data. Like all unsupervised learning algos, in clustering we must make an assumption about the underlying prob dist from which the data was generated. 
	\item Our model for how the data is generated is often called our \textbf{generative model}. In clustering, we
	\begin{itemize}
		\item assume that data points are assigned a cluster, with each cluster characterized by some cluster-specific prob dist (e.g. a Gaussian with some mean and variance that characterizes the cluster).  
		\item We then specify a procedure for finding the value of the latent variable. Often done by choosing the values of the latent variable that minimize some cost func.
	\end{itemize}
	\item One common choice for a class of cost funcs for many unsupervised learning problems = Maximum Likelihood Estimation (MLE): choose the values of the latent variables that maximize the likelihood of the observed data under our generative model (= maximize the prob of getting the observed data under our generative model). Such MLE equations often give rise to the kind of \textbf{Expectation Maximation (EM)} equations that we first encountered in the $K$-means clustering context.
	\item Gaussian Mixture Models (GMM) are a generative model often used in the context of clustering. 
	\begin{itemize}
		\item In GMM, points are drawn from one of $K$ Gaussians, each with its own mean $\bm{\mu}_k$ and covariance matrix $\Sigma_k$,
		\begin{align}
			\mathcal{N(\bm{x}|\bm{\mu},\bm{\Sigma})} \sim e^{( - \frac{(\bm{x}-\bm{\mu}) \bm{\Sigma}^{-1} (\bm{x} - \bm{\mu})^T}{2} )}
		\end{align}
		\item Let's denote the prob a data point is drawn from mixture $k$ by $\pi_k$. Then, the prob of generating a point $\bm{x}$ in a GMM is given by 
		\begin{align}
			p(\bm{x} | \{ \bm{\mu}_k, \bm{\Sigma}_k, \pi_k \}) = \sum_{k=1}^K \mathcal{N} (\bm{x}|\bm{\mu}_k, \bm{Sigma}_k) \pi_k
		\end{align}
		\item Given a dataset $\bm{X} = \{ \bm{x}_1, ..., \bm{x}_N \}$, we can write the likelihood of the dataset as
		\begin{align}
			p(\bm{X} | \{ \bm{\mu}_k, \bm{\Sigma}_k, \pi_k \}) = \prod_{i=1}^N p(\bm{x} | \{ \bm{\mu}_k, \bm{\Sigma}_k, \pi_k \})
		\end{align}
		\item For future reference, let's denote the set of parameters $\{ \bm{\mu}_k, \bm{\Sigma}_k, \pi_k  \}$ by $\bm{\theta}$.
		\item To see how we can use GMM and MLE to perform clustering, we introduce discrete binary $K$-dimensional latent variables $\bm{z}$ for each data point $\bm{x}$ whose $k$-th component is 1 if point $x$ was generated from the $k$-th Gaussian and zero otherwise (often called "one-hot variables"). Fex: if we're considering a Gaussian mixture with $K=3$ we would have three possible values for $\bm{z} \equiv (z_1, z_2, z_3)$: $(1,0,0)$, $(0,1,0)$ and $(0,0,1)$. 
		\item We cannot directly observe $\bm{z}$. It's a latent variable that encodes the cluster identity of point $\bm{x}$. Let's also denote all the $N$ latent variables corresponding to dataset $\bm{X}$ by $\bm{Z}$.
		\item Viewing the GMM as a generative model, we can write the prob $p(\bm{x}|\bm{z})$ of observing a data point $\bm{x}$ given $\bm{z}$ as 
		\begin{align}
			p(\bm{x}|\bm{z}; \{ \bm{\mu}_k, \bm{\Sigma}_k \}) = \prod_{k=1}^K \mathcal{N} (\bm{x}| \mu_k, \Sigma_k)^{z_k}
		\end{align}
		and the prob of observing a given value of latent variable
		\begin{align}
			p(\bm{z} |\{ \pi_k \}) = \prod_{k=1}^K \pi_k^{z_k}
		\end{align}
		\item Using Bayes rule, we can write the joint prob of a clustering assignment $\bm{z}$ and a data point $\bm{x}$ given the GMM parameters as
		\begin{align}
			p(\bm{x}, \bm{z}; \theta) = p(\bm{x}| \bm{z}; \{ \bm{\mu}_k, \bm{\Sigma}_k \}) p(\bm{z} |\{ \pi_k \})
		\end{align}
		\item Can also use Bayes rule to rearrange this expression to give the conditional prob of the data point $\bm{x}$ being in the $k$-th cluster, $\gamma (z_k)$, given model parameters $\theta$ as
		\begin{align}
			\gamma (z_k) \equiv p(z_k = 1| \bm{x} ; \theta) = \frac{\pi_k \mathcal{N}(\bm{x}| \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\bm{x}| \mu_j, \Sigma_j)}
		\end{align}
		$\gamma (z_k)$ are often referred to as the "responsibility" that mixture $k$ takes for explaining $\bm{x}$. Just like in our discussion of soft-max classifiers, this can be made into a "hard-assignment" by assigning each point to the cluster with the largest prob: $\text{argmax}_k \gamma (z_k)$ over the responsibilities.
		\item The complication is of course that we don't know the parameters $\theta$ of the underlying GMM but instead must also learn them from the data $\bm{X}$. As discussed, ideally we could do this by choosing the parameters that maximize the likelihood (or equivalently the log-likelihood) of the data
		\begin{align}
			\hat{\theta}_i = \text{argmax}_{\theta_i} \text{log} p(\bm{X}|\theta)
		\end{align}
		Once we knew the MLEs $\hat{\theta}_i$, we can cal the optimal hard cluster assignment $\text{argmax}_k \hat{\gamma} (z_k)$ where $\hat{\gamma} (z_k) = p(z_k = 1| \bm{x}; \hat{\theta})$.
	\end{itemize}
	\item In practice, due to the complexity of the expression for $p(\bm{X}|\theta)$, it's almost impossible to find the global max of the likelihood func. Instead, must settle for local max. One approach to finding the local max is use a method like SGD on the negative log-likelihood.
	\item Here, we introduce an alternative, powerful approach for finding local minima in latent variable models using an iterative procedure called \textbf{Expectation Maximation (EM)}. Given an initial guess of the parameters $\theta^{(0)}$, the EM algo iteratively generates new estimates for the parameters $\theta^{(1)}, \theta^{(2)}, ...$. Importantly, the likelihood=guaranteed to be non-decreasing under these iterations and hence EM converges to a local max of the likelihood.
	\begin{itemize}
		\item Central observation underlying EM: often much easier to calc the conditional likelihoods of the latent variables 
		$\tilde{p}^{(\bm{Z})} = p(\bm{Z}|\bm{X}; \theta^{(t)})$ given some choice of parameters and the max of the expected log likelihood given an assignment of the latent variables: $\theta^{(t+1)} = \text{argmax}_\theta E_{p(\bm{Z}|\bm{X}; \theta^{(t)})} [\text{log} p(\bm{X}, \bm{Z}; \theta)]$.
		\item To get an intuition for this latter quantity notice that we can write
		\begin{align}
			& E_{\tilde{p}^{(t)}} [\text{log} p(\bm{X}, \bm{Z}; \theta)]
			=& \sum_{i-1}^N \sum_{k=1}^K \gamma_{ik}^{(t)} [\text{log} \mathcal{N} (\bm{x}_i | \bm{\mu}_k, \bm{\Sigma}_k) + \text{log} \pi_k]
		\end{align}
		where we used the shorthand $\gamma_{ik}^{(t)} = p(z_{ik}| \bm{X}; \theta^{(t)})$ with $z_{ik}$ = the $k$-th component of $\bm{z}_i$
		\item Taking the derivative of this eq wrt $\bm{\mu}_k$, $\bm{\Sigma}_k$, and $\pi_k$ (subject to the constraint $\sum_k \pi_k = 1$) and setting this to zero yields the intuitive equations
		\begin{align}
			\bm{\mu}_k^{(t+1)} =& \frac{\sum_i^N \gamma_{ik}^{(t)} x_i}{\sum_i \gamma_{ik}^{(t)}} \\
			\bm{\Sigma}_k^{t+1} =& \frac{\sum_i^N \gamma_{ik}^{(t)} (\bm{x}_i - \bm{\mu}_k)(\bm{x}_i - \bm{\mu}_k)^T}{\sum_i \gamma_{ik}^{(t)}} \\
			\bm{\pi}_k^{(t+1)} =& \frac{1}{N} \sum_k \gamma_{ik}^{(t)}
		\end{align}
		These are just the usual estimates for the mean and variance, with each data point weighed according to our current best guess for the prob that it belongs to cluster $k$. 
		\item We can then use our new estimate $\theta^{(t+1)}$ to calc new memberships $\gamma_{ik}^{(t+1)}$ and repeat the process. This is essentially the $K$-Means algo discussed previously.
	\end{itemize}
	\item This discussion of GMMs introduces several concepts that we'll return to repeatedly in the context of unsupervised learning.
	\begin{itemize}
		\item First, it's often useful to think of the visible correlations between features in the data as resulting from hidden or latent variables.
		\item Second, we'll often posit a generative model that encodes the structure we think exists in the data and then find parameters that maximize the likelihood of the observed data.
		\item Third, often we'll not be able to directly estimate the MLE, and will have to instead look for a computationally efficient way to find a local min of the likelihood.
	\end{itemize}
\end{itemize}

\subsection{Clustering in high-dimension}
\begin{itemize}
	\item One major problem that is aggravated when clustering data in high-dimension = the accumulation of noise coming from spurious features that tends to "blur" distances. Many clustering algos rely on the explicit use of of a similarity measure or distance metrics that weigh all features equally. $\rightarrow$ one must be careful when using an off-the-shelf method in high dimensions.
	\item In order to perform clustering in high-dim data, often useful to denoise the data before proceeding w/using a standard clustering method. 
	\begin{itemize}
		\item Fig 54 (seen earlier): PCA was used to denoise the MNIST dataset by projecting the 748 original dimensions onto the 40 dimensions w/the largest principal components. 
		\item The resulting features were then used to construct a Eucledian distance matrix which was used by $t$-SNE to compute the 2D embedding that is presented. (Using $t$-SNE directly on original data leads to "blurring" of the clusters).
	\end{itemize}
	\item However, simple feature selection or feature denoising (using fex PCA) can sometimes be insufficient for learning clusters due to the presence of large variations in the signal and noise of the features that are relevant for identifying the underlying clusters. (why is this a problem? aren't the features picked out by PCA the ones with large variance? or is there a different type of variance, which PCA somehow avoids (how, if so?)?) Recent promising work suggests one way to overcome these limitations is to learn the latent space \textit{and} the cluster labels at the \textit{same time}.
	\item We end the clustering section with a discussion on cluster validation=can be particularly hard for high-dim data. Often cluster validation (= verifying whether the obtained labels are "valid") is done by direct visual inspection. That is: the data is represented in a low-dim space and the cluster labels obtained are visually inspected to make sure different labels organize into distinct "blobs".
	\item For high-dim data, this is done by performing dim-reduction. But, this can lead to the appearance of spurious clusters since dim-reduction inevitably loses info about the original data. $\rightarrow$ these methods should be used with care when trying to validate clusters (see ref for an interactive discussion on how $t$-SNE can sometime be misleading and how to effectively use it).
	\item There's a lot of work done devising ways of validating clusters based on based on various metrics and measures. Perhaps one of the most intuitive ways of defining a good clustering = measuring how well clusters generalize. Recently, clustering methods based on leveraging powerful classifiers to measure the generalization errors of the clusters have been developed by some of the authors and we believe this represent an especially promising research direction for high-dim clustering.
	\item Finally, see ref for an in-depth survey of the various validation techniques.
\end{itemize}




\section{Variational methods and mean-field theory (MFT)}
\begin{itemize}
	\item Common thread in many supervised learning tasks = accurately representing the underlying prob dist from which a dataset is drawn. Unsupervised learning of high-dim, complex distributions represents a new set of technical and computational challenges that are different from those we encountered in supervised learning.
	\item When dealing with complicated prob dists, often much easier to learn the \textit{relative weights} of different states or data points (ratio of probs), than \textit{absolute} probs.
	\item In \textbf{physics}, this is the familiar statement that the weights of a Boltzmann dist are much easier to calc than the partition func. The relative prob of two configs $\bm{x}_1$ and $\bm{x}_2$ are prop to the difference between their Boltzmann weights
	\begin{align}
		\frac{p(\bm{x}_1)}{p(\bm{x}_2)} = e^{-\beta (E(\bm{x}_1) - E(\bm{x}_2))}
	\end{align}
	where as is usual in stat mech $\beta$= the inverse temp and $E(\bm{x}; \theta)$ =the energy of state $\bm{x}$ given some parameters (couplings) $\theta$.
	\item However, calc the absolute weight of a config requires knowledge of the partition func
	\begin{align}
		Z_p = \text{Tr}_{\bm{x}} e^{-\beta E(\bm{x})}
	\end{align}
	(where the trace is taken over all possible configs $\bm{x}$) since
	\begin{align}
		p(\bm{x}) = \frac{e^{-\beta E(\bm{x}_1)}}{Z_p}
	\end{align}
	In general, calc the partition func $Z_p$ = analytically and computationally intractable.
	\item Fex: for the Ising model with $N$ binary spins, the trace involves calc a sum over $2^N$ terms, whcih is a difficult task for most energy funcs. $\rightarrow$ physicists (and ML scientists) have developed various numerical and computational methods for evaluating such partition funcs. 
	\item One approach: use Monte-Carlo based methods to draw samples from the underlying dist (can be known knowing only the relative probs) and then use these samples to numerically estimate the partition func.
	\item This is the philosophy behind powerful methods inc Markov Chain Monte Carlo (MCMC) and annealed importance sampling which are widely used in both the stat phys and ML communities.
	\item Alternative approach, that we focus on here: approximate the prob dist $p(\bm{x})$ and partition func using a "variational distribution" $q(\bm{x}; \theta_q)$ whose partition func we can calc exactly. The variational parameters $\theta_q$ are chosen to make the variational dist as close to the true dist as possible (how this is done = focus of much of this chapter).
	\item One of the most widely applied examples of a variational method in stat phys = Mean-Field Theory (MFT). MFT can be naturally understood as a procedure for approximating the true dist of the system by a \textbf{factorized distribution}. The deep connection between MFT and variational methods is discussed below. These variational MFT methods have been extended to understand more complicated spin models (aka graphical models in the ML literature) and form the basis of powerful set of techniques called \textbf{Belief Propagation} and \textbf{Survey Propagation}.
	\item Variational methods also widely used in ML to approximate complex probabilistic models. Fex: below we show how the EM procedure, discussed previously in the GMM clustering context, is actually a general method that can be dervied for any latent (hidden) variable model using a variational procedure.
	\item For readers interested in in-depth discussion on \textbf{variational inference for probabilistic graphical models}, we recommend a great treatise + a physics oriented discussion + an outstanding book \textbf{REFERENCES}
\end{itemize}

\subsection{Variational mean-field theory for the Ising model}
\begin{itemize}
	\item Ising models = a major paradigm in stat phys
	\item Historically introduced to study magnetism, it was quickly realized their predictive power applies to a variety of interacting many-particle systems. Ising models are now understood to serve as minimal models for complex phenomena such as certain classes of phase transitions.
	\item In the model, degrees of freedom called spins assume discrete, binary values, e.g. $s_i = \pm 1$. Each spin variable $s_i$ lives on a lattice (or in general, a graph), the sites of which are labeled by $i = 1,2,...,N$.
	\item Despite the extreme simplicity relative to real-world systems, Ising models exhibit a high level of intrinsic complexity, and the degrees of freedom can become correlated in sophisticated ways. Often, spins interact spatially locally, and respond to externally applied magnetic fields.
	\item A spin config $\bm{s}$ specifies the values $s_i$ of the spins at every lattice site. We can assign an "energy" to every such config
	\begin{align}
		E(\bm{s}, \bm{J}) = - \frac{1}{2} \sum_{i,j} J_{ij} s_i s_j - \sum_i h_i s_i
	\end{align}
	where $h_i$=a local magnetic field acting on the spin $s_i$, and $J_{ij}$= the interaction strength between the spin $s_i$ and $s_j$. In textbook examples, the coupling parameters $\bm{J} = (J, h)$ =typically uniform or, in studies of disordered systems, drawn from some prob dist (i.e. quenched disorder).
	\item The prob of finding the system in a given spin config at temp $\beta^{-1}$ is given by 
	\begin{align}
		p(\bm{s}|\bm{J}) =& \frac{1}{Z_p (\bm{J})} e^{-\beta E(\bm{s}, \bm{J})} \\
		Z_p (\bm{J}) =& \sum_{s_i = \pm 1}  e^{-\beta E(\bm{s}, \bm{J})}
	\end{align}
	with $\sum_{s_i = \pm 1} $ denoting the sum over all possible configs of the spin variables. We write $Z_p$ to emphasize that it's the partition func corresponding to the prob dist $p(\bm{s})$ (will become important later).
	\item For a fixed number of lattice sites $N$, there are $2^N$ possible configs, a number that grows exponentially w/the system size. $\rightarrow$ not in general feasible to evaluate the partition func $Z_p (\bm{J})$ in closed form. = a major obstacle for extracting predictions from physical theories since the partition func is directly related to the free-energy through the expression
	\begin{align}
		\beta F_p (\bm{J}) = -\text{log} Z_p (\bm{J}) = \beta \langle E(\bm{s}, \bm{J}) \rangle_p - H_p
	\end{align}
	with
	\begin{align}
		H_p = - \sum_{s_i = \pm 1} p(\bm{s}|\bm{J}) \text{log} p(\bm{s}|\bm{J})
	\end{align}
	the entropy of the prob dist $p(\bm{s}|\bm{J})$.
	\item Even if the true prob $p(\bm{s}|\beta, \bm{J})$ may be a very complicated object, we can still make progress by approximating it by a \textit{variational distribution} $q(\bm{s}, \bm{\theta})$ which captures the essential features of interest, with $\bm{\theta}$ some parameters that define our variational ansatz.
	\item The name variational dist comes from the fact that we're going to vary the parameters $\bm{\theta}$ to make $q(\bm{s}, \bm{\theta})$ as close to $p(\bm{s}|\beta, \bm{J})$ as possible.
	\item The functional form of $q(\bm{s}, \bm{\theta})$ = based on an "educated guess", oftentimes coming from our intuition about the problem.
	\item Can also define the variational free-energy
	\begin{align}
		\beta F_q (\bm{J}, \bm{\theta}) = \beta \langle E(\bm{s}, \bm{J}) \rangle_q - H_q
	\end{align}
	where $\langle E(\bm{s}, \bm{J}) \rangle_q$ = the expectation value of the energy corresponding to the dist $p(\bm{s})$ wrt the dist $q(\bm{s}, \bm{\theta})$, and $H_q$ is the entropy of $q(\bm{s}, \bm{\theta})$. 
	\item Before proceeding further, helpful to introduce a new quantity: the Kullback-Leibler divergence (=KL-divergence=relative entropy) between two dists $p(\bm{x})$ and $q(\bm{x})$. It measures the dissimilarity between the two dists and is given by 
	\begin{align}
		D_{KL}(q||p) = \text{Tr} q(\bm{x}) \text{log} \frac{p(\bm{x})}{q(\bm{x})}
	\end{align}
	= the expectation wrt $q$ of the logarithmic difference between the two dists $p$ and $q$. Two important properties of the KL-divergence are:
	\begin{enumerate}
		\item Positivity: $D_{KL}(p||q) \geq 0 $ with equality iff $p=q$ (in the sense of prob dists)
		\item $D_{KL}(p||q) \neq D_{KL}(q||p)$, that is the KL-divergence is not symmetric in its arguments.
	\end{enumerate}
	\item Variational mean-field theory is a systematic way of constructing such an approximate distribution $q(\bm{s}, \bm{\theta})$. Main idea: choose parameters that minimize the difference between the variational free-energy $F_q(\bm{J}, \bm{\theta})$ and the true free energy $F_q(\bm{J}|\beta)$. Will show in following section that the difference between these two free-energies is actually the KL-divergence:
	\begin{align}
		F_q(\bm{J}, \bm{\theta}) = F_q(\bm{J}|\beta) + D_{KL}(q||p) 
	\end{align}
	This, combined w/the non-negativity of the KL-divergence has important consequences.
	\begin{itemize}
		\item First, it shows the variational free-energy is always larger than the true free-energy, $F_q(\bm{J}, \bm{\theta}) \geq F_p(\bm{J})$, with equality only if $q=p$ (the latter inequality is found in many physics textbooks and is known as the \textbf{Gibbs inequality}).
		\item Second, finding the best variational free-energy is equivalent to minimizing the KL divergence $D_{KL}(q||p)$.  
	\end{itemize} 
	\item \textbf{Armed with these observations, let's derive a MFT of the Ising model using variational methods}. In the simplest MFT of the Ising model, the variational dist is chosen so that all spins are independent:
	\begin{align}
		q(\bm{s}, \bm{\theta}) = \frac{1}{Z_q} e^{\sum_i \theta_i s_i} = \prod_i \frac{e^{\theta_i s_i}}{2\text{cosh} \theta_i}
	\end{align}
	Aka we've chosen a dist $q$ which factorizes on every lattice site. An important property of this functional form = we can analytically find a closed-form expression for the variational partition func $Z_q$. This simplicity also comes at a cost= ignoring correlations between spins. These correlations become less and less important in higher dims, and the MFT ansatz becomes more accurate.
	\item To evaluate the variational free-energy, we 
	\begin{itemize}
		\item First need the entropy $H_q$. Since $q$ factorizes over the lattice sites, the entropy separates into a sum of one-body terms
		\begin{align}
			H_q (\bm{\theta}) =& - \sum_{s_i = \pm 1} q(\bm{s}, \bm{\theta}) \text{log} q(\bm{s}, \bm{\theta}) \\
			&= - \sum_i q_i \text{log} q_i + (1-q_i)\text{log} (1-q_i)
		\end{align}
		where $q_i = \frac{e^{\theta_i}}{2\text{cosh}\theta_i}$ is the prob that spin $s_i$ is in the $+1$ state.
		\item Next, need to evaluate the average of the Ising energy $E(\bm{s}, \bm{J})$ wrt the variational dist $q$. Although the energy contains bilinear terms, we can still evaluate this average easily, because the spins are independent (uncorrelated) in the $q$ dist. The mean value of spin $s_i$ in the $q$ dist, or on the on-site magnetization, is given by
		\begin{align}
			m_i = \langle s_i \rangle_q = \sum_{s_i = \pm 1} s_i \frac{e^{\theta_i s_i}}{2\text{cosh} \theta_i} = \text{tanh}(\theta_i)
		\end{align} 
		Since the spins are independent, we have
		\begin{align}
			\langle E(\bm{s}, \bm{J}) \rangle_q = - \frac{1}{2} \sum_{i,j} J_{ij}m_i m_j - \sum_i h_i m_i
		\end{align}
	\end{itemize}
	The total variational free-energy is
	\begin{align}
		\beta F_q (\bm{J}, \bm{\theta}) = \beta \langle E(\bm{s}, \bm{J}) \rangle_q - H_q
	\end{align}
	and minimizing wrt the variational parameters $\bm{\theta}$, we obtain
	\begin{align}
		\frac{\partial}{\partial \theta_i} \beta F_q (\bm{J}, \bm{\theta}) = 2\frac{d q_i}{d \theta_i} (-\beta [\sum_j J_{ij} m_j + h_i] + \theta_i)
	\end{align}
	Setting this eq to zero, we arrive at
	\begin{align}
		\theta_i = \beta \sum_j J_{ij} m_j (\theta_j) + h_i
	\end{align}
	\item For the special case of a uniform field $h_i = h$ and uniform nearest neighbor couplings $J_ij = J$, by symmetry the variational parameters for all the spins are identical, with $\theta_i = \theta$ for all $i$. Then, the mean-field equations reduce to their familiar textbook form, $m=\text{tanh}(\theta)$ and $\theta = \beta (z J m(\theta) + h)$, where $z$ is the coordination number of the lattice (i.e. the number of nearest neighbors).
	\item The equations for $m_i$ and $\theta_i$ form a closed system = the mean-field equations for the Ising model. To solve them, one method is to iterate through and update each $\theta_i$, once at a time, in an asynchronous fashion. To see the relationship of this approach to solving MFT equations to EM, it's helpful to explicitly spell out the iterative procedure to find the solutions to the eq for $\theta_i$. 
	\begin{itemize}
		\item We start by initializing our variational parameters to some $\bm{\theta}^{(0)}$ and repeat the following until convergence:
		\begin{enumerate}
			\item \textit{Expectation}: 
			Given a set of assignments at iteration $t$, $\bm{\theta}^{(t)}$, calc the corresponding magnetizations $\bm{m}^{(t)}$.
			\item \textit{Maximation}:
			Given a set of magnetizations $m_t$, find new assignments $\theta^{(t+1)}$ which maximize the variational free-energy $F_q$. From our found expression for $\theta_i$, this is just
			\begin{align}
				\theta_i^{(t+1)} = \beta \sum_j J_{ij} m_j^{(t)} + h_i
			\end{align}
		\end{enumerate}
	\end{itemize}
	From these equations, clear that we can think of the MFT of the Ising model as an EM like procedure similar to the one we used for k-means clustering and GMMs.
	\item As is well known in physics, even though MFT not exact, it can often yield qualitatively and even quantitatively precise predictions (especially in high dims). The discrepancy between the true physics and MFT predictions stems from the fact that the variational dist $q$ we chose doesn't model the correlations between spins. Fex: it predicts the wrong value for the critical temp for the 2D Ising model. Even erroneously predicts the existence of a phase transition in 1D at a non-zero temp. 
	\item But we emphasize that the failure of any particular ansatz doesn't compromize the power of the approach. In some cases, one can consider changing the variational ansatz to improve the predictive properties of the corresponding variational MFT.
\end{itemize}

\subsection{Expectation Maximation (EM)}
\begin{itemize}
	\item Ideas along the lines of variational MFT have been independently developed in statistics and imported into ML to perform maximum likelihood estimation (MLE).
	\item Here, we explicitly derive the EM algo and demonstrate further its close relation to MFT (\textbf{article} by Hinton).
	\item We'll focus on latent variable models where some of the variables=hidden=cannot be directly observed. Often makes MLE difficult to implement. EM gets around this difficulty by using an iterative two-step procedure, closely related to variational free-energy based approximation schemes in stat phys.
	\item Let $\bm{x}$ = the set of visible variables we can directly observe and $\bm{z}$ = the set of latent/hidden variables we cannot directly observe.
	\item Denote underlying prob dist from which $\bm{x}$ and $\bm{z}$ drawn by $p(\bm{z}, \bm{x}| \bm{\theta})$, with $\bm{\theta}$ = all relevant parameters.
	\item Given a dataset $\bm{x}$, we want to find the MLE of the parameters $\bm{\theta}$ that maximizes the prob of the observed data.
	\item As in variational MFT, we view $\bm{\theta}$ as variational parameters chosen to maximize the log-likelihood $L(\bm{\theta}) = \text{log} p(\bm{x}|\bm{\theta})$. Algorithmically, this can be done by iterating the variational parameters $\bm{\theta}^{(t)}$ in a series of steps $(t=1,2,...)$ starting from some arbitrary initial value $\bm{\theta}^{(0)}$ :
	\begin{enumerate}
		\item \textbf{Expectation step (E step)}:
		Given the known values of observed variable $\bm{x}$ and the current estimate of parameter $\bm{\theta}_{t-1}$, find the prob dist of the latent variable $\bm{z}$:
		\begin{align}
			q_{t-1} (\bm{z}) = p(\bm{z}| \bm{\theta}^{(t-1)}, \bm{x})
		\end{align}
		\item \textbf{Maximation step (M step)}:
		Re-estimate the parameter $\bm{\theta}^{(t)}$ to be those with max likelihood, assuming $q_{t-1} (\bm{z})$ found in the prev step is the true dist of hidden variable $\bm{z}$:
		\begin{align}
			\bm{\theta}_t = \text{argmax}_\theta \langle \text{log} p(\bm{z}, \bm{x}| \bm{\theta}) \rangle_{g_{t-1}}
		\end{align}
	\end{enumerate}
	It's been shown that each EM iteration increases the true log-likelihood $L(\bm{\theta})$, or at worst leaves it unchanged. In most models, this iteration procedure converges to a \textit{local maximum} of $L(\bm{\theta})$.
	\item To see how EM is actually performed and related to variational MFT, we make use of the KL-divergence. Recall our goal =to maximize the log-likelihood $L(\bm{\theta}) = \text{log}p(\bm{x}|\bm{\theta}) $. With data $\bm{z}$ missing, we surely cannot just maximize $L(\bm{\theta})$ directly since parameter $\bm{\theta}$ might couple both $\bm{z}$ and $\bm{x}$. EM circumvents this by optimizing another objective func, $F_q(\bm{\theta})$, constructed based on estimates of the hidden variable dist $q(\bm{z}|\bm{x})$. Indeed, the func optimized is none other than the \textit{variational free energy} encountered in the prev section:
	\begin{align}
		F_q(\bm{\theta}) := - \langle \text{log} p(\bm{z}, \bm{x}|\bm{\theta}) \rangle_q - H_q
	\end{align}
	where $H_q$ is the SHannon entopy of $q(\bm{z})$. Can define the true free-energy $F_p(\bm{\theta})$ as the negative log-likelihood of the observed data:
	\begin{align}
		-F_p(\bm{\theta}) = L(\bm{\theta}) = \text{log} p(\bm{x}|\bm{\theta})
	\end{align}
	In the language of stat phys, $F_p(\bm{\theta})$ =the \textit{true} free-energy while $F_q(\bm{\theta})$=the variational free-energy we would like to minimize. Note we have employed a physics sign convention here of defining the free-energy as minus log of the partition func. In the ML literature, this minus often omitted, can lead to some confusion.
	\item Our goal: choose $\bm{\theta}$ so that our variational free-energy $F_q(\bm{\theta})$=as close to the true free-energy $F_p(\bm{\theta})$ as possible. The difference of these free-energies can be written as
	\begin{align}
		& F_q(\bm{\theta}) - F_p(\bm{\theta}) \\
		=& \text{log}  p(\bm{x}|\bm{\theta}) - \sum_z q(\bm{z}|\bm{x}) \text{log} p(\bm{z}, \bm{x}| \bm{\theta}) + \sum_z q(\bm{z}|\bm{x}) \text{log}q(\bm{z}|\bm{x}) \\
		=& \sum_z q(\bm{z}|\bm{x}) \text{log}p(\bm{x}|\bm{\theta}) - \sum_z  q(\bm{z}|\bm{x}) \text{log} p(\bm{z}, \bm{x}| \bm{\theta}) + \sum_z q(\bm{z}|\bm{x}) \text{log}q(\bm{z}|\bm{x}) \\
		=& - \sum_z q(\bm{z}|\bm{x}) \text{log} \frac{p(\bm{z}, \bm{x}| \bm{\theta})}{p(\bm{x}| \bm{\theta})} + \sum_z q(\bm{z}|\bm{x}) \text{log} \tilde{p}(\bm{z}) \\
		=& \sum_z q(\bm{z}|\bm{x}) \text{log} \frac{q(\bm{z}|\bm{x})}{p(\bm{z}|\bm{x}, \bm{\theta})} \\
		=& D_{KL} (q(\bm{z}|\bm{x}) || p(\bm{z}|\bm{x}, \bm{\theta})) \geq 0
	\end{align}
	where we've used Baye's theorem $p(\bm{z}|\bm{x}, \bm{\theta})= p(\bm{z},\bm{x} | \bm{\theta}) / p(\bm{x} | \bm{\theta})$. Since KL-divergence always positive, this shows the variational free-energy $F_q$ always an upper bound of the true free-energy $F_p$. In physics: \textbf{Gibb's inequality}.
	\item From eq for $F_q(\bm{\theta})$ and the fact that the entropy term there doesn't depend on $\bm{theta}$, we can immediately see that the M-step=minimizing the variational free-energy $F_q(\bm{\theta})$.
	\item Surprisingly, the E-step can also be viewed as the optimization of the variational free-energy. Concretely, one can show that the dist of hidden variables $\bm{z}$ given the observed variable $\bm{x}$ and the current estimate of parameter $\bm{\theta}$ is the \textit{unique} prob $q(\bm{z})$ that minimizes $F_q(\bm{\theta})$ (now seen as a functional of $q$). This can be proved by: taking the functional derivative of the equation for $F_q(\bm{\theta})$, plus a Langrange multiplier that encodes $\sum_z g(\bm{z}) = 1$, wrt $q(\bm{z})$.
	\item Summing things up, we can rewrite EM as (same ref to Hinton):
	\begin{enumerate}
		\item \textit{Expectation step}: Construct the approximating prob dist of unobserved \textbf{z} given the values of observed variable $\bm{z}$ given the values of observed variable $\bm{x}$ and parameter estimate $\bm{\theta}^{(t-1)}$:
		\begin{align}
			q_{t-1} (\bm{z}) = \text{argmin}_q F_q (\bm{\theta}^{(t-1)})
		\end{align} 
		\item \textit{Maximation step}: Fix $q$, update the variational parameters:
		\begin{align}
			\bm{\theta}^{(t)} = \text{argmax}_{\bm{\theta}} - F_{q_{t-1}} (\bm{\theta}) 
		\end{align}
	\end{enumerate} 
	\item \textbf{To recapitulate}: 
	\begin{itemize}
		\item EM implements MLE even with missing or hidden variables through optimizing a lower bound of the true log-likelihood. 
		\item In statistical physics, this reminicent of optimizing a variational free-energy which is a lower bound of true free-energy due to Gibbs inequality. 
	\end{itemize}
	\item Fig 59: picture of how EM works
	\begin{itemize}
		\item The E-step can be seen as representing the unobserved variable $\bm{z}$ by a prob dist $q(\bm{z})$. This prob used to construct an alternative objective func $-F_q(\bm{\theta})$, which is then maximized wrt $\bm{\theta}$ in the M-step.
	\end{itemize}
	\item By construction, maximizing the negative variational free-energy is equivalent to doing MLE on the joint data (=both observed and unobserved). 
	\item "M-step" name = intuitive since $\bm{\theta}$ found by maximizing $-F_q(\bm{\theta})$. "E-step" comes from the fact that one ususally doesn't need to construct the prob of missing datas explicitly, but rather need only to compute the "expected" sufficient statistics over these data.
	\item Practical side: Em demonstrated as extremely useful in parameter estimation (but we're doing prediction in ML...?), particularly in hidden Markov models and Bayesian networks. Striking advantage: conceptually simple + easy to implement. In many cases, implementation of EM guaranteed to increase the likelihood monotonically = could be a perk during debugging.
	\item For readers familiar with physics of disordered systems: it's possible to construct a one-to-one dictionary between EM for latent variable models and the MFT of spin systems with quenched disorder. In disordered spin systems, the Ising couplings $\bm{J}$ commonly taken to be quenched random variables drawn from some underlying prob dist. In the EM procedure, the quenched disorder provided by the observed data $\bm{x}$ which are drawn from some underlying prob dist characterizing the data. The spins $\bm{s}$ are like the hidden or latent variables $\bm{z}$. Similar analogies can be found for all the variational MFT quantities (table 1):
	\begin{itemize}
		\item \textbf{Statistical physics} - \textbf{Variational EM}
		\item spins/d.o.f: $\bm{s}$ $\Leftrightarrow$ Hidden/latent variables $\bm{z}$
		\item couplings/quenched disorder: $\bm{J}$ $\Leftrightarrow$ Data observations: $\bm{x}$
		\item Boltzmann factor $e^{-\beta E (\bm{s}, \bm{J})}$ $\Leftrightarrow$ Complete probability: $p(\bm{x}, \bm{z}|\bm{\theta})$
		\item Partition function: $Z(bm{J})$ $\Leftrightarrow$ Marginal likelihood $P(\bm{x}|\theta)$
		\item Energy: $\beta E(\bm{s}, \bm{J})$ $\Leftrightarrow$ Negative log-complete data likelihod: $-\text{log} p(\bm{x}, \bm{z}|\bm{\theta}, m)$
		\item Free energy: $\beta F_p (\bm{J}|\beta)$ $\Leftrightarrow$ negative log-marginal likelihood: $-\text{log} p(\bm{x}|m)$
		\item Variational distribution: $q(\bm{s})$ $\Leftrightarrow$ Variational distribution: $q(\bm{z}|\bm{x})$
		\item Variational free-energy: $F_q(\bm{J}, \bm{\theta})$ $\Leftrightarrow$ Variational free-energy: $F_q(\bm{x}, \bm{\theta})$
	\end{itemize}
\end{itemize}



\section{Energy based models: Maximium Entropy (MaxEnt) Principle, Generative models, and Boltzmann Learning}
\begin{itemize}
	\item \textit{Discriminative} models: 
	\begin{itemize}
		\item Most of models we have discussed so far, inc linear and logistic regression, ensemble models, supervised neural networks
		\item Designed to perceive differences between groups/categories of data.
		\item Fex: recognizing differences between images of cats and images of dogs allows a discriminative model to label an image as "cat" or "dog".
		\item Form the core techniques of most supervised learning methods.
		\item Several limitations: 
		\begin{enumerate}
			\item Like all supervised learning, require labeled data.
			\item There are tasks that descriminative approaches simply cannot accomplish, fex: drawing new examples from an unknown prob dist. 
		\end{enumerate}
	\end{itemize}
	\item \textit{Generative} models:
	\begin{itemize}
		\item A model that can learn to represent and sample from a prob dist
		\item Fex: A gen model for images would learn to draw new examples of cats and dogs given a dataset of images of cat and dog. And: Given samples generated from one phase of an Ising model we may want to generate new samples from that phase.
		\item Such tasks clearly beyond the scope of discriminative models like the ensemble methods and DNNs discussed so far.
	\end{itemize}
	\item \textit{Energy-based} generative models:
	\begin{itemize}
		\item Closely related to the kinds of models commonly encountered in statistical physics $\rightarrow$ we'll draw on many techniques that have their origin in stat mech (e.g. Monte-Carlo methods)
	\end{itemize}
	\item Chapter overview:
	\begin{itemize}
		\item Overview of generative models, highlighting similiarities and differences with the supervised learning methods previously encountered
		\item Introduce perhaps simples kind of generative model: Maximum Entropy (MaxEnt) models. Have no latent/hidden variables=ideal for introducing key concpets underlying energy-based generative models.
		\item Extended discussion on how to train energy-based models. Much of it also applicable to more complicated energy-based models such as the RBM and the deep models discussed later.
	\end{itemize}
\end{itemize}

\subsection{An overview of energy-based generative models}
\begin{itemize}
	\item Generative models = ML methods that learn to generate new examples similar to those found in a training dataset. Core idea for most: learn a parametric model for the prob dist from which the data was drawn. Can then generate new examples by sampling from the learned model. 
	\item In stat phys, this sampling often done using Markov Chain Monte Carlo (MCMC) methods. \textbf{A concise and beautiful intro to MCMC-inspired methods that bridges both stat phys and ML is REFs (book and a review)}.
	\item Added complexity of learning models directly from samples introduces many of the same fundamental tensions encountered when discussing discriminative models:
	\begin{itemize}
		\item Model must be able to "generalize" beyond the examples they have been trained on = generate new samples not in the training set
		\item $\rightarrow$ must be expressive enouh to capture complex correlations present in the underlying data distribution
		\item $\rightarrow$ amount of data we have is finite, giving rise to overfitting
	\end{itemize}
	\item In practice: most generative models used in ML flexible enough that, with sufficient number of parameters, they can approximate any prob dist $\rightarrow$ there are thus \textbf{three axes on whcih we can differentiate classes of generative models}:
	\begin{enumerate}
		\item How easy the model is to train - in terms of
		\begin{itemize}
			\item Computational time
			\item The complexity of writing code for the algo
		\end{itemize}
		\item How well the model generalizes from the training set to the test set
		\item \textbf{Which characteristics of the data dist the model is capable of and focuses on captureing}
	\end{enumerate}
	\item One of fundamental reasons energy-based models less widely-employed than their discriminative counterpart is the training procedures of these models differ significantly from those for supervised NN models:
	\begin{itemize}
		\item Both employ gradient-descent based procedures for minimizing a cost function (one common choice for generative models=the negative log-likelihood func)
		\item But: energy-based models do not use backprop and automatic differentiation for computing gradients
		\item They use: ideas inspired by MCMC based methods in physics and statistics, sometimes named "Boltzmann learning"
		\item $\rightarrow$ Training them requires additional tools not immediately available in packages like PyTorch and TensorFlow
		\item Paysage, built on top of PyTorch bridges this gap, provides training for methods like RBM and stacked RBMS. Can be employed on GPUs. Maintained by a company affiliated with the authors.
	\end{itemize}
	\item Finally: generative models at their most basic level = complex parametrizations of the prob dist the data is drawn from. $\rightarrow$ can do much more than just generate new examples. They can be used to perform a multitude of other tasks that require sampling from a complex prob dist includin:
	\begin{itemize}
		\item "de-noising"
		\item filling in missing data
		\item and even discrimination (Hinton reference).
	\end{itemize}
	The versatility is one of the major appeals of generative models.
\end{itemize}

\subsection{Maximum entropy models: the simplest energy-based generative models}
\begin{itemize}
	\item Have their origin in a series of beautiful papers by Jaynes that reformulated stat mech in information theoretic terms \textbf{REF}. 
	\item Often presented as the class of generative models that make the least assumptions about the underlying data - BUT all models make assumptions.
\end{itemize}

\subsubsection{MaxEnt models in statistical mechanics}
\begin{itemize}
	\item Introduced by E. T. Jaynes in a two-part paper in 1957 entitled "\textbf{Information theory and statistical mechanics}". In these incredible papers, Jaynes
	\begin{itemize}
		\item Showed it was possible to rederive the Boltzmann dist (and the idea of generalized ensembles) entirely from information theoretic arguments.
		\item Quoting from the abstact, Jaynes consdiered "\textbf{stat mech as a form of stat inference rather than as a physical theory}" (portending the close connection between statistical physics and ML).
		\item Showed the Boltzmann dist could be viewed as resulting from a stat inference procedure for learning prob dists describing physical systems where one only has partial information about the system (usually the average energy).
	\end{itemize}
	\item The key quantity in MaxEnt models = the information theoretic entropy = Shannon entropy = a concept introduced by Shannon in his landmark treatise on information theory (1949).
	\begin{itemize}
		\item It quantifies the stat uncertainty one has about the value of a random variable $\bm{x}$ drawn from a prob dist $p(\bm{x})$.
		\item It's defined as
		\begin{align}
			S_p = -\text{Tr}_x p(\bm{x}) \text{log} p(\bm{x})
		\end{align}
		where the trace = a sum/integral over all possible values a variable can take.
	\end{itemize}
	\item Janyes showed the Boltzmann dist follows from the Principle of Maximum Entropy. 
	\begin{itemize}
		\item A physical system should be described by the prob dist w/the largest entropy subject to certain constraints (often provided by measuring the average value of conserved, extensive quantities such as the energy, particle number, etc.)
		\item The principle uniquely specifies a procedure for parameterizing the functional form of the prob dist. 
		\item Having specified and learned this form we can, ofc, generate new examples by sampling this dist.
	\end{itemize}
	\item More details on the workings:
	\begin{itemize} 
		\item $\{ f_i(\bm{x}) \}$ = set of funcs we have chosen whose average value we want to fix to some observed values $\langle f_i \rangle_{obs}$.
		\item Principle of Max Ent states we should choose the dist $p(\bm{x})$ w/the largest uncertainty (i.e. largest Shannon entropy $S_p$), subject to the constraints that the model averages match the observed averages:
		\begin{align}
			\langle f_i \rangle_{model} := \int \text{d} \bm{x} f_i(\bm{x})p(\bm{x}) = \langle f_i \rangle_{obs}
		\end{align}
		\item Can formulate the Principle of Max Ent as an optimization problem using the method of Lagrange multipliers (=the $\lambda_i$ below) by minimizing:
		\begin{align}
			\mathcal{L}[p] = -S_p + \sum_i \lambda_i (\langle f_i \rangle_{obs} - \int \text{d} \bm{x} f_i(\bm{x})p(\bm{x})) + \gamma (1 - \int \text{d} \bm{x} p(\bm{x}))
		\end{align}
		where the first set of constraints=enforce the requirement for the averages and the last constraint = enforces the normalization that the trace over the prob dist equals one.
		\item Can solve for $p(\bm{x})$ by taking the functional derivative and setting it to zero
		\begin{align}
			0 = \frac{\delta \mathcal{L}}{\delta p} = (\text{log} p(\bm{x}) + 1) - \sum_i \gamma_i f_i(\bm{x}) - \gamma
		\end{align}
		The general form of the max entropy dist then given by
		\begin{align}
			p(\bm{x}) = \frac{1}{Z} e^{\sum_i \gamma_i f_i (\bm{x})}
		\end{align}
		where $Z(\gamma_i) = \int \text{d} \bm{x} e^{\sum_i \lambda_i f_i (\bm{x})}$ = the partition function.
		\item The max ent dist = clearly just the usual Boltzmann dist, with energy $E(\bm{x}) = -\sum_i \gamma_i f_i (\bm{x})$.
		\item The values of the Lagrange multipliers are chosen to match the observed averages for the set of funcs $\{ f_i(\bm{x}) \}$ whose average value is being fixed:
		\begin{align}
			\langle f_i \rangle_{model} = \int \text{d} \bm{x} p(\bm{x}) f_i(\bm{x}) = \frac{\partial \text{log} Z}{\partial \lambda_i} = \langle f_i \rangle_{obs}
			\label{MaxEnt}
		\end{align}
		aka, the parameters of the dist can be chosen such that
		\begin{align}
			\partial_{\lambda_i} \text{log} Z = \langle f_i \rangle_{data}
		\end{align}
	\end{itemize}
	\item To gain more intuition for the MaxEnt dist, helpful to relate the Lagrange multipliers to the familiar thermodynamic quantities we use to describe physical systems.
	\begin{itemize}
		\item Our $\bm{x}$ = the microscopic state of the system = the MaxEnt dist is a prob dist over microscopic states.
		\item But, in thermodynamics we only have access to average quantities. If we know only the average energy $\langle E(\bm{x}) \rangle_{obs}$, the MaxEnt procedure tells us to maximize  the ent subject to the average energy constraint. This yields
		\begin{align}
			p(\bm{x}) = \frac{1}{Z} e^{-\beta E(\bm{x})}
		\end{align}
		where we have identified the Lagrange multiplier conjugate to the energy $\lambda_1 = -\beta = 1/k_B T$ with the (negative) inverse temperature. 
		\item Suppose we also constrain the particle number $\langle N(x) \rangle_{obs}$. Then, an almost identical calc yields a MaxEnt dist of the functional form 
		\begin{align}
			p(\bm{x}) = \frac{1}{Z} e^{-\beta (E(\bm{x}) - \mu N(\bm{x}))}
		\end{align}
		where we have written our Langrange multipliers in the familiar thermodynamic notation $\lambda_1 = -\beta$ and $\lambda_2 = \mu / \beta$.
		\item Since this is just the Boltzmann dist, we can also relate the partition func in our MaxEnt model to the thermodynamic free-energy via $F=-\beta^{-1} \text{log} Z$.
		\item The choice of which quantities to constrain = equivalent to working in different thermodynamic ensembles.
	\end{itemize}
\end{itemize}
\subsubsection{From statistical mechanics to machine learning}
\begin{itemize}
	\item The MaxEnt idea also provides a general procedure for learning a generative model from \textit{data}. Key difference between MaxEnt models in (theoretical) physics and ML:
	\begin{itemize} 
		\item In ML we have no direct access to observed average values $\langle f_i \rangle_{obs}$.
		\item Instead, these averages must be directly estimated from data (samples). 
		\item To denote this differeence, will call empirical averages calc from data as $\langle f_i \rangle_{data}$.
	\end{itemize}
	Can think of MaxEnt as a stat inference procedure simply by replacing $\langle f_i \rangle_{obs}$ by $\langle f_i \rangle_{data}$ above.
	\item This subtle change has important implications for training MaxEnt models.
	\begin{enumerate}
		\item Since we don't know these averages exactly, but must estimate from the data, our training procedure must be careful not to overfit to the observations (our samples might not be reflective of the true values of these statistics).
		\item The averages of certain functions $f_i$ are easier to estimate from limited data than others. Often an important consideration when formulating which MaxEnt model to fit to the data.
		\item Unlike in physics where conservation laws often guide the funcs $f_i$ whose averages we hold fixed, ML offers no comparable guide for how to choose the $f_i$ we care about. 
	\end{enumerate}
	For these reasons, choosing the $\{ f_i \}$ is often far from straightforward. 
	\item We have presented a physics based perspective of the MaxEnt procedure. The MaxEnt in ML is also closely related to Bayesian inference ideas and this latter point of view is more common in discussions in the stat and ML literature.
\end{itemize}

\subsubsection{Generalized Ising Models from MaxEnt}
\begin{itemize}
	\item Form of a MaxEnt = completely specified once we choose the averages $\{ f_i \}$ we wish to constrain. 
	\item A common choice = constrain the first two moments of a dist. 
	\item When our random variables $\bm{x}$=continuous, the corresponding MaxEnt dist = a multi-dim Gaussian.  
	\item If $\bm{x}$=binary (discrete), corresponding MaxEnt= a generalized Ising (Potts) model with all-to-all couplings.
	\item To see this, consider
	\begin{itemize}
		\item $\bm{x}$ = a random variable with first and second moments $\langle x_i \rangle_{data}$ and $\langle x_i x_j \rangle_{data}$, respectively.
		\item According to the Principle of Max Ent, we should choose to model this variable using a Boltzmann dist w/constraints on the first and second moments.
		\item Let $a_i$ be the Langrange multiplier associated with $\langle x_i \rangle_{data}$ and $J_{ij}/2$ be the L multipilier associated with $\langle x_i x_j \rangle_{data}$ .
		\item Using equation \ref{MaxEnt}, it's easy to verfiy that the energy function
		\begin{align}
			E(\bm{x}) = -\sum_i a_i x_i - \frac{1}{2} \sum_{ij} J_{ij} x_i x_j
		\end{align}
		satisfies the above constraints.
	\end{itemize}
	\item Partition funcs for MaxEnt models=often intractable to compute. $\rightarrow$ helpful to consider two special cases where $\bm{x}$ has different support (different kinds of data). 
	\begin{enumerate}
		\item Consider the case that the random variables $\bm{x} \in \mathbb{R}^n$ = real numbers. In this case, can compute the partition func directly:
		\begin{align}
			Z = \int \text{d} \bm{x} e^{\bm{a}^T \bm{x} + \frac{1}{2} \bm{x}^T \bm{J}\bm{x} } = \sqrt{(2\pi)^n \text{det} J^{-1}} e^{-\frac{1}{2} \bm{a}^T \bm{J}^{-1} \bm{a}}
		\end{align}
		The resulting prob density func is
		\begin{align}
			p(\bm{x}) =& Z^{-1} e^{-E(\bm{x})} \nonumber \\
			&= \frac{1}{\sqrt{(2\pi)^n \text{det} J^{-1}}} e^{-\frac{1}{2} \bm{a}^T \bm{J}^{-1} \bm{a} + \bm{a}^T \bm{x} + \frac{1}{2} \bm{x}^T \bm{J}\bm{x} } \nonumber \\
			=& \frac{1}{\sqrt{(2\pi)^n \text{det} \Sigma}} e^{-\frac{1}{2} (\bm{x} - \bm{\mu})^T \Sigma^{-1} (\bm{x} - \bm{\mu})}
		\end{align}
		where $\bm{\mu} = -J^{-1} \bm{a}$ and $\Sigma = -J^{-1}$. This, ofc = the normalized, multi-dimensional Gaussian distribution.
		\item Consider the case that the random variable $\bm{x}$ = binary with $x_i \in \{ -1, +1 \}$. The energy func takes same form as the one above, but partition function can no longer be computed in a closed form. This model=the \textbf{Ising model} in physics and a \textbf{Markov Random Field} in ML. Since energy func intractable, the best we can do is estimate it using numerical techniques such as MCMC methods or approximate methods like variational MFT methods. Note: in ML common to use binary variables which take on values in $x_i \in \{ 0, 1 \}$, rather than $x_i \in \{ -1, +1 \}$. Can sometimes be a source of confusion when translating between ML and physics literatures and can lead to confusion when using ML packages for physics problems.
	\end{enumerate}
\end{itemize}

\subsection{Cost functions for training energy-based models}
\begin{itemize}
	\item MaxEnt procedures gives us a way of parameterizing an energy-based generative model.
	\item \textbf{For any energy-based generative model}, the energy function $E(\bm{x}, \{ \theta_i \})$ depends on some parameters $\theta_i$ (=couplings in stat phys language) that must be inferred directly from the data.
	\begin{itemize}
		\item Fex: For MaxEnt models the $\{ \theta_i \}$=just the Langrange multipliers $\{ \lambda_i \}$ introduced in the last section.
	\end{itemize}
	The goal of the training procedure is to use the available data to fit these parameters.
	\item We fit the parameters by minimizing a cost func using stochastic gradient descent. The procedure naturally separates into two parts: choosing an appropriate cost func and calc the gradient of the cost func wrt the model parameters. 
	\item Formulating a cost func for generative models = a little bit trickier than for supervised, discriminative models. Objective of discriminative models=straightforward: predict the label from the features. What we mean by a "good" generative model =much harder to define using a cost func. 
	\begin{itemize}
		\item Would like the model to generate examples similar to those found in the training data
		\item But would also like it to be able to generalize - we don't want it to reproduce "spurious details" that are particular to the training data
		\item \textbf{Unlike for discriminative models, no straightforward idea like cross-validation on the data labels} that neatly adresses this issue.
	\end{itemize}
	\item Calc the gradients of energy-based models also different from discriminative models such as deep NNs. Rather than rely on automatic differentiation techniques and backprop, calc the gradient requires drawing on intuitions from MCMC methods. 
	\item Below provide an in-depth discussion of Boltzmann learning for energy-based generative models, focusing on MaxEnt models. Put emphasis on training procedures that generalize to more complicated generative models w/latent variables such as RBMs. Thus, largely ignore the incredibly rich physics-based literature on fitting Ising-like MaxEnt models.
\end{itemize}

\subsubsection{Maximum likelihood}
\begin{itemize}
	\item By far most common approach for training a generative model = maximize the log-likelihood of the training data.
	\item Recall: the log-likelihood characterizes the log-prob of generating observed data using our generative model. By choosing the negative log-likelihood as the cost func, the learning procedure tries to find parameters that maximize the prob of the data. 
	\item This cost func=intuitive, has therefore been the work-horse of most generative modeling. But, note the MLE has some important limitations we'll return to in chapter 12.
	\item We now employ a general notation applicable to all energy-based models, not just the MaxEnt. Reason being much of the discussion does not rely on the specific form of the energy func,, only the fact that our generative model takes a Boltzmann form. 
	\begin{itemize}
		\item $p_\theta(\bm{x})$= the prob dist by which we denote the generative model. $\text{log} Z(\{ \theta_i \})$ = its corresponding partition func.
		\item In MLE, the parameters are fit by maximizing the log-likelihood:
		\begin{align}
			\mathcal{L}(\{ \theta_i \}) :=& \langle \text{log} (p_\theta (\bm{x})) \rangle_{data} \\
			=& - \rangle E(\bm{x}; \{ \theta_i \}) \rangle_{data} - \text{log} Z (\{ \theta_i \})
		\end{align}
		where we set $\beta=1$. Made use of the fact that our generative dist is of the Boltzmann form and that our partition func does not depend on the dat:
		\begin{align}
			\langle \text{log} Z(\{ \theta_i \}) \rangle_{data} = \text{log} Z (\{ \theta_i \})
		\end{align}
	\end{itemize}
\end{itemize}

\subsubsection{Regularization}
\begin{itemize}
	\item Just as for discriminative models like linear and logistic regression, it's common to supplement the log-likelihood w/additional regularization terms that prevent overfitting. 
	\item Instead of minimizing the log-likelihood, minimize a cost func of the form
	\begin{align}
		- \mathcal{L}(\{ \theta_i \}) + E_{reg} (\{ \theta_i \})
	\end{align}
	where $E_{reg} (\{ \theta_i \})$ = an additional regularization term that prevents overfitting.
	\item \textbf{From Bayesian perspective}, this new term can be viewed as encoding a (negative) log-prior on model parameters and performing a maximium-a-posteriori (MAP) estimate instead of a MLE.
	\item As seen with regression, different forms of regularization give rise to different kinds of properties. A common choice for the regularization func are the sums of the $L_1$ or $L_2$ norms of the parameters
	\begin{align}
		E_{reg} (\{ \theta_i \}) = \Lambda \sum_i | \theta_i |^\alpha , \quad \alpha = 1,2 
	\end{align}
	with $\Lambda$=parameters that control the regularization strenght. $\Lambda=0$=no regularization, simply performing MLE. Large $\Lambda$=will force many parameters to be close to or exactly zero.
	\item Just as in regression, an $L_1$ penalty enforces sparsity, w/many of the $\theta_i$ set to zero, and $L_2$ regularization shrinks the size of the parameters towards zero.
	\item One challenge of generative models: often hard to choose the reg strength $\Lambda$.
	\begin{itemize}
		\item Recall: for linear and logistic regression $\Lambda$ is chosen to maximize out-of-sample performance on a validation dataset (i.e. cross-validation).
		\item But, for generative models data usually unlabeled. $\rightarrow$ choosing $\Lambda$ more subtle, no universal procedure.
		\item One common strategy: divide the data into a training set and a validation set and monitor a summary statistic such as the
		\begin{itemize}
			\item log-likelihood
			\item energy distance (ref)
			\item variational free-energy of the generative model
		\end{itemize}
		on the training and validation sets. If the gap between them starts growing, one is probably overfitting the model even if the log-likelihood of the training dataset is still increasing. 
		\item This also gives a procedure for "early stopping" - a regularization procedure we introduced in the context of discriminative models.
		\item In practice when using such regularizers: important to try many different values of $\Lambda$ and then try to use a proxy statistic for overfitting to evaluate the optimal choice of $\Lambda$.
	\end{itemize} 
\end{itemize}

\subsection{Computing gradients}
\begin{itemize}
	\item Now: procedure for minimizing the cost func. Powerful and common choice = SGD.
	\item Performing MLE using SGD requires calc the gradient of the log-likelihood wrt the parameters $\theta_i$. 
	\item To simplify notation and gain intuition, helpful to define "operators" $O_i(\bm{x})$, conjugate to the parameters $\theta_i$
	\begin{align}
		O_i(\bm{x}) := \frac{\partial E(\bm{x}; \theta_i)}{\partial \theta_i}
	\end{align}
	\item Since the partition func just the cumulant generative func for the Boltzmann dist, we know the usual stat mech relationships between expectation values and derivatives of the log-partition func hold:
	\begin{align}
		\langle O_i(\bm{x}) \rangle_{model} = \text{Tr}_x p_\theta (\bm{x}) O_i (\bm{x}) = - \frac{\partial \text{log} Z(\{ \theta_i \})}{\partial \theta_i}
	\end{align}
	\item In terms of the operators $\{ O_i(\bm{x}) \}$, the gradient of the log-likelihood takes the form
	\begin{align}
		- \frac{\partial \mathcal{L} (\{ \theta_i \})}{\partial \theta_i}
		&= \langle \frac{\partial E(\bm{x}; \theta_i)}{\partial \theta_i} \rangle_{data} + \frac{\partial \text{log} Z(\{ \theta_i \})}{\partial \theta_i} \nonumber \\
		&=  \langle O_i(\bm{x}) \rangle_{data} - \langle O_i (\bm{x}) \rangle_{model}
	\end{align}
	These eqs have a simple and beautiful interpretation:
	\begin{itemize}
		\item The grad of the log-likelihood wrt to a model parameter is a \textbf{difference of moments} - one calc directly from the data, one calc from our model using the current model parameters.
		\item \textit{The positive phase} of the gradient = the data dependent term. 
		\item \textit{The negative phase} of the gradient = the model dependent term.
	\end{itemize}
	This derivation also gives an intuitive explanation for likelihood-based training procedures: The gradient acts on the model to lower the energy of configurations that are near observed data points while raising the energy of configs far from observed data points. 
	\item Note: All info about the data only enters the training procedure through the expectations $\langle O_i(\bm{x}) \rangle_{data}$ and our generative model is blind to information beyond what is contained in these expectations.
	\item Now still need to calc the expectation values in the above eq. The positive gradient phase/ expec vals wrt the data is easily calc using samples from the training data. But, the negative phase/the expec vals wrt the model generally much harder to compute. In almost all cases will have to resort to numerical or approximate methods. Fundemental reason: impossible to calc the partition func exactly for most interesting problems.
	\item There are exceptional cases where we can calc expec vals analytically. The generative model then said to have a \textit{Tractable Likelihood}.
	One example:
	\begin{itemize}
		\item The Gaussian MaxEnt model for real valued data discussed before. The parameters/Lagrange multipliers for this model = the local fields $\bm{a}$ and the pairwise coupling matrix $J$. 
		\item Here, the usual manipulations involving Gaussian integrals allow us to exactly find the parameters $\mu = -J^{-1} \bm{a}$ and $\Sigma = -J^{-1}$, yielding the familiar expressions $\mu = \langle \bm{x} \rangle_{data}$ and $\Sigma = \langle (\bm{x} - \langle \bm{x} \rangle_{data})(\bm{x} - \langle \bm{x} \rangle_{data} )^T \rangle_{data}$. These are the standard estimates for the sample mean and covariance matrix. 
		\item Converting back to the Lagrange multiplier yields
		\begin{align}
			J = - \langle (\bm{x} - \langle \bm{x} \rangle_{data})(\bm{x} - \langle \bm{x} \rangle_{data} )^T \rangle_{data}^{-1}
		\end{align}
		SPM: men vil dette være overfittet...??? hvis det er perfekt til dataene? eller tar likelihood og skiller perfekt på hvor denne grensen går? hvordan?
	\end{itemize}  
	\item In the generic case with \textit{intractable likelihoods}, we must estimate expec vals numerically. One way to do this:
	\begin{itemize}
		\item Draw samples $\mathcal{S}_{model} = \{ \bm{x}_i' \}$ from the model $p_\theta (\bm{x})$ and evaluate expec vals using these samples:
		\begin{align}
			\langle h(\bm{x}) \rangle_{model} = \int \text{d} \bm{x} p_\theta (\bm{x}) h(\bm{x}) \approx \sum_{\bm{x}_i' \in \mathcal{S}_{model}} g(\bm{x}_i')
		\end{align}
		The samples from the model $\bm{x}_i' \in \mathcal{S}_{model}$ are often referred to as \textit{fantasy particles} in the ML literature and can be generated using simple MCMC algos such as Metropolis-Hastings.
	\end{itemize}
	\item Once we have the fantasy particles (samples) from the model, we can also easily calc the grad of an arbitrary expec val $\langle g(\bm{x}) \rangle_{model}$ using what is commonly called the "log-derivative trick" in ML:
	\begin{align}
		\frac{\partial }{\partial \theta_i} \langle g(\bm{x}) \rangle_{model}
		&= \int \text{d} \bm{x} \frac{\partial p_\theta (\bm{x})}{\partial \theta_i} g(\bm{x}) \\
		&= \langle \frac{\partial \text{log} p_\theta (\bm{x})}{\partial \theta_i} \rangle_{model} \\
		&= \langle O_i (\bm{x}) g(\bm{x}) \rangle_{model} \\
		\approx \sum_{\bm{x}_i' \in \mathcal{S}_{model}} O_i (\bm{x}) g(\bm{x}_i')
	\end{align}
	This expression allows us to take gradients of more complex cost funcs beyond the MLE procedure discussed here.
\end{itemize}


\subsection{Summary of the training procedure}
\begin{itemize}
	\item Now summarize and present a general procedure for training an energy based model using SGD o the cost func.
	\item Goal: fit the parameters of a model 
	$p_\lambda (\{ \theta_i \}) = Z^{-1} e^{- E(\bm{x}, \{ \theta_i \})}$
	\item Training the model involves the following steps
	\begin{enumerate}
		\item Read a minibatch of data, $\{ \bm{x} \}$
		\item Generate a random sample (fantasy particles)
		$\{ \bm{x}' \} \sim p_\lambda$ using an MCMC algo (e.g. Metropolis-Hastings)
		\item Compute the grad of log-likelihood using these samples and the eq found earlier, where the averages are taken over the minibatch of data and the fantasy particles/samples from the model, respectively.
		\item Use the grad as input to one of the grad based optimizers discussed in chapter four.
	\end{enumerate}
	\item In practice: helpful to supplement this with some practical tricks that help training. As with discriminative NNs, important to initialize the parameters properly and print summary statistics during the training on the training and validation sets to prevent overfitting. \textbf{These and many other little practical tricks have been nicely summarized in a short note from the Hinton group REF}
	\item A major computational and practical limitation: often hard to draw samples from generative models. MCMC methods often have long mixing-times (=the time you have to run the Markov chain to get uncorrelated samples) $\rightarrow$ can result in biased sampling. Luckily, often don't need to know the gradients exactly for trainng ML models (recall: noisy gradient estimates often help the convergence of gradient descent algos) $\rightarrow$ can significantly reduce the computational expense by running MCMC for a reasonable time window. Will exploit this extensively in the next section discussing how to train more complex energy-based models with hidden variables.
\end{itemize}


\section{Deep generative models: Latent variables and Restricted Boltzmann Machines (RBMs)}
\begin{itemize}
	\item Last section: core ideas behind energy-based generative models. Now: energy-based models that include latent/hidden variables.
	\item Including latent variables = greatly enhances expressive power, allows model to represent sophisticated correlations between visible features without sacrificing trainability. Multiple latent layers: can construct powerful deep generative models that possesses many of the same desirable properties as the deep, discriminative NNs.
\end{itemize}

\subsection{Why hidden (latent) variables?}
\begin{itemize}
	\item Latent variables = powerful yet elegant way to encode sophisticated correlations between observable features. Underlying reason: 
	\begin{itemize}
		\item Marginalizing over a subset of variables (="integreating out" degrees of freedom in physics language) induces complex interactions between remaining variables.
		\item The idea that integrating out variables can lead to complex correlations = familiar component of many physical theories. Fex: when considering free electrons living on a lattice, integrating out phonons gives rise to higher-order electron-electron interactions (e.g. superconducting or magnetic correlations).
		\item More generally, in the Wilsonian renormalization group paradigm, all effective field theories can be thought of as arising from integrating out high-energy degrees of freedom \textbf{REF}.
	\end{itemize}
	\item Generative models w/latent variables run this logic in reverse: 
	\begin{itemize}
		\item Encode complex interactions between visible variables by introducing additional, hidden variables that interact w/visible degrees of freedom in a simple manner, yet still reproduce the complex correlations between visible degrees in the data once marginalized over (integrated out).
		\item This trick is \textbf{also widely exploited in physics}, e.g. in the Hubbard-Stratonovich transformation or in the introduction of ghost fields in gauge theory.
	\end{itemize}
	\item To make these ideas more concrete, let's revisit the pairwise Ising model introduced in discussion of MaxEnt models:
	\begin{itemize}
		\item The model is described by a Boltzmann distribution w/energy
		\begin{align}
			E(\bm{v}) = - \sum_i a_i v_i - \frac{1}{2} \sum_{ij} v_i J_{ij} v_j
		\end{align}
		where $J_{ij}$ = a symmetric coupling matrix that encodes the pairwise constraints and $a_i$ enforce the single-variable constraint (single-variable constraint?).
		\item Goal: replace the complicated interactions between the visible variables $v_i$, encoded by $J_{ij}$, by interactions with a new set of latent variables $h_\mu$. 
		\item In order to do this, helpful to rewrite the coupling matrix in a slightly different form. Using SVD, we can always express the coupling matrix $J_{ij} = \sum_{\mu=1}^N W_{i\mu} W_{j\mu}$, where $\{ W_{i\mu} \}_i$ = appropriately normalized singular vectors.
		\item In terms of $W_{i\mu}$, the energy takes the form
		\begin{align}
			E_{Hop} (\bm{v}) = - \sum_i a_i v_i -\frac{1}{2} \sum_{ij\mu} v_i W_{i\mu} W_{j\mu} v_j
		\end{align}
		\item Note: in the special case when both the $v_i \in \{ -1, +1 \}$ and $W_{i\mu} \in \{ -1, +1 \}$ are binary variables, a model with this form of the energy function is known as the \textbf{Hopfield model REF}. The Hopfield model has played an estremely important role in stat phys, computational neuroscience, and ML, see \textbf{REF} for a beautiful discussion combining all  these properties.
		\item We therefore refer to all energy functions of the form above as \textbf{(generalized) Hopfield models}, even for the case when the $W_{i\mu}$=continuous variables.
		\item Now "decouple" the $v_i$ by introducing a set of normally, distributed continuous latent variables $h_\mu$ (\textbf{in condensed matter physics this called a Hubbard-Stratonovich transformation} as mentioned above). 
		\item Using the usual identity for Gaussian integrals, we can rewrite the Boltzmann dist for the generalized Hopfield model as (\textbf{cool!! this could be used to show how to go from wave func sqaured to something involving hidden variables})
		\begin{align}
			p(\bm{v}) =& \frac{e^{\sum_i a_i v_i +\frac{1}{2} \sum_{ij\mu} v_i W_{i\mu} W_{j\mu} v_j}}{Z} \\
			=& \frac{e^{\sum_i a_i v_i} \prod_\mu \int \text{d} h_\mu e^{- \frac{1}{2}\sum_\mu h_\mu^2 - \sum_i v_i W_{i\mu} h_\mu}}{Z} \\
			=& \frac{\int \text{d}\bm{h} e^{-E(\bm{v}, \bm{h})}}{Z}
		\end{align}
		where $E(\bm{v}, \bm{h})$ is \textbf{a joint energy functional} of both the latent and visible variables of the form
		\begin{align}
			E(\bm{v}, \bm{h}) = - \sum_i a_i v_i + \frac{1}{2} \sum_\mu h_\mu^2 - \sum_{i\mu} v_i W_{i\mu} h_\mu 
		\end{align}
		\item We can also use the energy func $E(\bm{v}, \bm{h})$ to define a new energy-based model $p(\bm{v}, \bm{h})$ on both the latent and visible variables
		\begin{align}
			p(\bm{v}, \bm{h}) = \frac{e^{-E(\bm{v}, \bm{h})}}{Z'}
		\end{align}
		\item Marginalizing over latent variables of course gives us back the generalized Hopfield model (\textbf{REF}).
		\begin{align}
			p(\bm{v}) = \int \text{d}\bm{h} p(\bm{v}, \bm{h}) = \frac{-E_{Hop}(\bm{v})}{Z}
		\end{align}
		\item Notice: $E(\bm{v}, \bm{h})$ contains no direct interactions between visible degrees of freedom (or between hidden degrees of freedom). Instead: the complex correlations between the $v_i$ are encoded in the interaction between the visible $v_i$ and latent variables $h_\mu$.
		\item It turns out the model presented here = a special case of a more general class of powerful energy-based models called Restrected Boltzmann Machines (RBMs).
	\end{itemize}
\end{itemize}

\subsection{Restricted Boltzmann machines (RBMs)}
\begin{itemize}
	\item RBM = an energy-based model w/both visible and hidden units where the visible and hidden units interact with each other but don't interact among themselves. 
	\item Energy func of an RBM takes the general functional form
	\begin{align}
		E(\bm{v}, \bm{h}) = -\sum_i a_i(v_i) - \sum_\mu b_\mu (h_\mu) - \sum_{i\mu} W_{i\mu} W_{i\mu} v_i h_\mu
	\end{align}
	where $a_i(\cdot)$ and $b_\mu(\cdot)$ are funcs that we are free to choose. The most common choice is:
	\begin{equation}
        a_i(v_i):=
        \begin{cases}
            a_i v_i,    &\text{if $v_i \in \{ 0,1 \}$ is binary} \\
            \frac{v_i^2}{2 \sigma_i^2} &\text{if $v_i \in \mathbb{R}$ is continuous}
        \end{cases}
    \end{equation}
    and 
    \begin{equation}
        b_\mu(h_\mu):=
        \begin{cases}
            b_\mu h_\mu,    &\text{if $h_\mu \in \{ 0,1 \}$ is binary} \\
            \frac{h_\mu^2}{2 \sigma_\mu^2} &\text{if $h_\mu \in \mathbb{R}$ is continuous}
        \end{cases}
    \end{equation}
	For this choice of $a_i(\cdot)$ and $b_i(\cdot)$, layers consisting of discrete binary units often called Bernoulli layers and layers consisting of continuous variables often called Gaussian (WHAT happened to subtracting biases in the Gaussian version????).
	\item The \textbf{basic pipartite structure} of an RBM (=visible and hidden layer that interact with each other but not among themselves) often depicted using a graph as in fig 61.
	\item Different types:
	\begin{itemize}
		\item Bernoulli-Bernoulli = most common choice
		\item Continuous-continuous = \textbf{the RBM reduces to a multi-dimensional Gaussian with a very paritcular correlation structure}.
		\item discrete (visible) - continuous (hidden) = the RBM is equivalent to a generalized Hopfield model.
		\item continuous (visible) - discrete (hidden) = often called a Gaussian Bernoulli RBM (\textbf{REF, Hinton}).
		\item Even possible to perform multi-modal learning w/mixture of cont and discrete variables.
		\item Remember for all these: only interactions between hidden and visible, no intralayer interactions. \textbf{This is analogous to Quantum Electrodynamics}, where a free fermion and a free photon interact with one anotherbut not among themselves.
	\end{itemize}
	\item Specifying a generative model w/this bipartite interaction structure has two major advantages:
	\begin{itemize}
		\item Enables capturing both pairwise \textit{and higher-order} correlations between visible units
		\item Makes it easier to sample from the model using an MCMC method known as block Gibbs sampling, in turn making the model easier to train.
	\end{itemize}
	\item \textbf{It's worth better understanding the kind of correlations that can be captured using an RBM}. 
	\begin{itemize}
		\item To do so, can marginalize over the hidden units and ask about the resulting distribution over just the visible units
		\begin{align}
			p(\bm{v}) = \int \text{d} \bm{h} p(\bm{v}, \bm{h}) = \int \text{d}\bm{h} \frac{e^{-E(\bm{v}, \bm{h})}}{Z}
		\end{align}
		(where the integral should be replaced by a trace in all expressions for discrete units).
		\item Can also define a marginal energy using the expression
		\begin{align}
			p(\bm{v}) := \frac{e^E(\bm{v})}{Z} 
		\end{align}
		\item Combining these equations (how did you get rid of $Z$?),
		\begin{align}
			E(\bm{v}) =& -\text{log} \int \text{d}\bm{h} e^{-E(\bm{v}, \bm{h})} \\
			=& -\sum_i a_i (v_i) - \sum_\mu \text{log} \int \text{d} h_\mu e^{b_\mu (h_\mu) + \sum_i v_i W_{i\mu} h_\mu}
		\end{align}
		\item \textbf{To understand what correlations are captured by} $p(\bm{v})$ it is helpful to introduce the distribution 
		\begin{align}
			q_\mu (h_\mu) = \frac{e^{b_\mu(h_\mu)}}{Z}
		\end{align}
		of hidden units $h_\mu$, ignoring the interactions between $\bm{v}$ and $\bm{h}$, and the \textbf{cumulant generating function} (????)
		\begin{align}
			K_\mu (t) := \text{log} \int \text{d} h_\mu q_\mu (h_\mu) e^{th_\mu} = \sum_n \kappa_\mu^{(n)} \frac{t^n}{n!}
		\end{align}
		$K_\mu (t)$ is defined such that the $n^{th}$ cumulant is $\kappa_\mu^{(n)} = \partial_t^n K_\mu |_{t=0} $.
		\item The cumulant generating func appears in the \textbf{marginal free-energy of the visible units}, which can be rewritten (up to a constant term) as:
		\begin{align}
			E(\bm{v}) =& - \sum_i a_i (v_i) - \sum_\mu K_\mu (\sum_i W_{i\mu} v_i) \\
			=& -\sum_i a_i(v_i) - \sum_\mu \sum_n \kappa_\mu^{(n)} \frac{(\sum_i W_{i\mu}v_i)^n}{n!}  \\
			=& -\sum_i a_i (v_i) - \sum_i (\sum_\mu \kappa_\mu^{(1)} W_{i\mu}) v_i  \\
			&- \frac{1}{2} \sum_{ij} (\sum_\mu \kappa_\mu^{(2)} W_{i\mu}W_{j\mu} )v_i v_j + ...
		\end{align}
		We see that
		\begin{itemize}
			\item The marginal energy includes all orders of interactions between the visible units, with the $n$-th order cumulants of $q_\mu(h_\mu)$ weighting the $n$-th order interactions between the visible units. (!!!!!!)
			\item In the case of the Hopfield model we discussed previously, $q_\mu (h_\mu)$ = a standard Gaussian distribution where the mean is $\kappa_\mu^{(1)} =0$, the variance is $\kappa_\mu^{(2)} = 1$, and all higher order cumulants are zero. Plugging these cumulants (weren't these called moments earlier???? same thing or different?) into the above equation recovers the expression we first got for the energy $E(\bm{v}, \bm{h})$ after having introduced the hidden variables before (one section back).
		\end{itemize}
	\end{itemize}
	\item These calc make clear the underlying reason for the incredible representational power of RBMs with a Bernoulli hidden layer (did we assume the type of the hidden layer above?).
	\begin{itemize}
		\item \textbf{Each hidden unit can encode very complex interactions at all orders.}
		\item We can learn which order of correlations/interactions are important directly from the data instead of having to specify them ahead of time as we did in the MaxEnt models.
		\item This highlights the power of generative models with even the simplest interactions between visible and latent variables to encode, learn, and represent complex correlations present in the data.
	\end{itemize}
\end{itemize}

\subsection{Training RBMs}
\begin{itemize}
	\item RBMs=a special case of energy-based generative models, which can be trained using the MLE procedure, as described in prev chapter. Brief recap:
	\begin{itemize}
		\item First, choose a cost func: For MLE, this just the negative log-likelihood with or without an additional regularization term to prevent overfitting. 
		\item Then, minimize the cost func using one of the SGD methods. The gradient itself can be calc using the equation from prev chapter. Fex: for the Bernoulli-Bernoulli RBM we have
		\begin{align}
			\frac{\partial \mathcal{L}(\{ W_{i\mu}, a_i, b_\mu \})}{\partial W_{i\mu}} & = \langle v_i h_\mu \rangle_{data} - \langle v_i h_\mu \rangle_{model} \\
			\frac{\partial \mathcal{L}(\{ W_{i\mu}, a_i, b_\mu \})}{\partial a_i} & = \langle  v_i \rangle_{data} - \langle v_i \rangle_{model} \\
			\frac{\partial \mathcal{L}(\{ W_{i\mu}, a_i, b_\mu \})}{\partial b_\mu} & = \langle h_\mu \rangle_{data} - \langle h_\mu \rangle_{model} \\
		\end{align}
		where 
		\begin{itemize}
			\item The postive expec wrt data = sampling from the model while clamping the visible units to their observed values in the data.
			\item As before, calc the negative phase of the gradient (the expec val wrt the model) requires we draw samples from the model. Luckily, the bipartite form of the interactions in RBMs were specifically chosen with this in mind.
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Gibbs sampling and contrastive divergence (CD)}
\begin{itemize}
	\item The \textbf{bipartite} interaction structure of the RBM makes it possible to calc expec vals using a MCMC method called Gibbs sampling.
	\item Key reason for this: since no interactions of visible units with themselves or hidden with themselves; the visible and hidden units are conditionally independent:
	\begin{align}
		p(\bm{v}|\bm{h}) =& \prod_i p(v_i | \bm{h}) \\
		p(\bm{h}|\bm{v}) =& \prod_\mu p(h_\mu| \bm{v})
	\end{align}
	with
	\begin{align}
		p(v_i = 1| \bm{h}) =& \sigma(a_i + \sum_\mu W_{i\mu} h_\mu) \\
		p(h_\mu = 1| \bm{v}) =& \sigma(b_\mu + \sum_i W_{i\mu} v_i)
	\end{align}
	and where $\sigma(x) = 1/(1+e^{-x})$= the sigmoid function.
	\item Using these expressions, easy to compute expec vals wrt the data. 
	\begin{itemize}
		\item Input to grad descent= a minibatch of observed data. 
		\item For each sample in the minibatch, we simply clamp the visible units to the observed vals and apply the above equation using the probability of teh hidden variables.
		\item Then average over all samples in the minibatch to calc expec vals wrt the data.
	\end{itemize}
	\item To calc expec vals wrt the model, we use (block) Gibbs sampling. Idea: 
	\begin{itemize}
		\item Iteratively sample from the conditional distributions $\bm{h}_{t+1} \sim p(\bm{h}| \bm{v}_t)$ and $\bm{v}_{t+1} \sim p(\bm{v}|\bm{h}_{t+1})$.
		\item Since the units conditionally independent, each step of this iteration can be performed by simply drawing random numbers.
		\item The samples are guaranteed to converge to the equilibrium distribution of the model in the limit that $t\rightarrow \infty$.
		\item At the end of the Gibbs sampling, one ends up with a minibatch of samples (fantasy particles).
	\end{itemize}
	\item One drawback of Gibbs sampling: may take many back and forth iterations to draw an independent samples. $\rightarrow$ the Hinton group introduced an approximate Gibbs sampling technique called Contrastive Divergence (CD). 
	\begin{itemize}
		\item In CD-$n$, we just perform $n$ iterations of (block) Gibbs sampling, with $n$ often taken to be as amll as 1.
		\item Price for this truncation: we're not drawing samples from the true model distribution.
		\item But for our purpose - using the expecs to estimate the gradient for SGD - CD-$n$ has been proven to work reasonably well. As long as the approximate gradients are reasonably correlated with the true gradient, SGD will move in a reasonable direction. 
		\item CD-$n$ of course does come at a price. Truncating the Gibbs sampler prevents sampling far away from the starting point, which for CD-$n$ are the data points in the minibatch. $\rightarrow$ our generative model will be much more accurate around regions of feature space close to our training data. $\rightarrow$ as is often the case in ML, CD-$n$ sacrifices ability to generalize to some extent in order to make the model easier to train.
		\item Some of these undesirable features can be tempered by using a slightly different variant of CD called Persistent Contrastive Divergence (PCD). 
		\begin{itemize}
			\item Rather than restarting the Gibbs sampler from the data at each gradient descent step, we start the Gibbs sampling at the fantasy particles (samples from the model) in the last grad descent step.
			\item Since parameters change slowly compared to the Gibbs sampling, samples that are high prob at one step of the SGD are also likely to be high prob at the next step $\rightarrow$ ensures PCD does not introduce large errors in the estimation of the gradients. 
			\item Advantage of using fantasy particles to initialize the Gibbs sampler = to allow PCD to explore parts of the feature space much further from the training data than one could reach with ordinary CD. 
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsubsection{Practical considerations}
"Tricks-of-the-trade" training of RBM summary published in note by Hinton \textbf{REF}. Some of the important points:
\begin{itemize}
	\item \textbf{Initialization}
	The model must be initialized. 
	\begin{itemize}
		\item Hinton suggests taking the weights $W_{i\mu}$ from a Gaussian with a mean zero and std $\sigma=0.01$.
		\item Alternative, proposed by Glorot and Bengio: choose std to scale with the layer sizes: $\sigma = 2/\sqrt{N_v + N_h}$ where $N_v$ and $N_h$=number of visible and hidden units respectively.
		\item Bias of the hidden units initialized to zero
		\item Bias of the visible units typically taken to be inversely proportional to the mean activation, $a_i = \langle v_i \rangle_{data}^{-1}$
	\end{itemize} 
	\item \textbf{Regularization}
	\begin{itemize}
		\item Can use an $L_1$ or $L_2$ penalty, typically only on the weight parameters, not the biases.
		\item Alternatively, Dropout has been shown to decrease overfitting when training with CD and PCD and to result in more interpretable learned features.
	\end{itemize}
	\item \textbf{Learning Rates}
	Typically, helpful to reduce the learning rate in later stages of training.
	\item \textbf{Updates for CD and PCD}
	Several computational tricks can be used for speeding up the alternating updates in CD and PCD.
\end{itemize}

\subsection{Deep Boltzmann Machine}
\begin{itemize}
	\item Possess multiple hidden layers and were the first models rebranded as "deep learning" (technically, these were Deep Belief Networks where only the top layer was undirected).
	\item Motivation:
	\begin{itemize}
		\item An RBM is composed of two layers of neurons, connected via an \textbf{undirected graph}. As a result, possible to perform sampling $\bm{v} \sim p(\bm{v}|\bm{h})$ and inference $\bm{h} \sim p(\bm{h}|\bm{v})$ w/the same model.
		\item As w/the Hopfield model, can view \textbf{each hidden unit as representative of a pattern, or feature, that could be present in the data (in general, should think of activity patterns of hidden units representing features in the data)}. The inference step involves assigning a prob to each of these features that expresses \textbf{the degree to which each feature is present in the data sample}.
		\item In an RBM, hidden units don't influence each other during the inference step, i.e. hidden units are conditionally independent given the visible units. There are a number of reasons why this is unsatisfactory. One is:
		\begin{itemize}
			\item The desire for sparse, distributed representations, where each observed visible vector will strongly activate a few (i.e. more than one but only a very small fraction) of the hidden units. 
		\end{itemize}
		In the brain, this is thought to be achieved by inhibitory lateral connections between neurons. But, adding lateral intra-layer connections between the hidden units makes the dist difficult to sample from so we need to come up with another way of creating connections between the hidden units.
		\item With the Hopfield model, we saw that pairwise linear connections between neurons can be mediated through another layer. $\rightarrow$ a simple way to allow for effective connections between the hidden units is to add another layer of hidden units. 
		\item Rather than just having two layers, a visible and hidden, we can add additional layers of latent variables to account for the correlations between hidden units. 
		\item Ideally, as one adds more and more layers, one might hope that the correlations between hidden variables become smaller and smaller deeper into the network. This basic logic is \textbf{reminiscent of renormalization procedures that seek to decorrelate layers at each step REF}. 
		\item Price of adding additional layers = models become harder to train.
	\end{itemize}
	\item Training DBMs more subtle than RBMs due to the difficulty of propagating information from visible to hidden units. But, Hinton and co realized some of these problems could be alleviated via a layerwise procedure. 
	\begin{itemize}
		\item Rather than attempting to train the whole DBM at once, can think of the DBM as a stack of RBMs (see fig 63)
		\item One first trains the bottom two layers of the DBM - treating it as if it is a standalone RBM.
		\item Once bottom trained, can generate "samples" from the hidden layer and use these samples as an input to the next RBM (consisting of the first and second hidden layer - purple hexagons and green squares in fig 63).
		\item This procedure can be repeated to pretrain all layers of the DBM.
		\item This pretraining initializes the weights so that SGD can be used effectively when the network is trained in a supervised (???) fashion.
		\item In particular, the pretraining helps the grads to stay well behaved rather than vanish or blow up - a problem we discussed extensively in the section on DNNs.
		\item Worth noting: once pretrained we can use the usual Boltzmann learning rules (presented in the prev chapter) to fine-tune the weights and improve the performance of the DBM \textbf{REF}.
		\item As shown in next section, Paysage package presented here can be used to both construct and train DBNs using such a pretraining procedure.
	\end{itemize}
\end{itemize}

\subsection{Example: Using Paysage for MNIST}
\begin{itemize}
	\item Here demonstrate how to use the new open source package Paysage (French for landscape) for training unsupervised energy-based models on the MNIST dataset. (Paysage documentation link to github).
	\item We'll show how to build and train four different models:
	\begin{itemize}
		\item  A "Hopfield" type RBM with Gaussian hidden and Bernoulli (binary) visible units.
		\item A conventional Bernoulli-Bernoulli RBM
		\item A conventional RBM with an additional $L_1$-penalty that enforces sparsity
		\item A DBM with three Bernoulli layers with $L_1$ penalty each.
	\end{itemize}
	\item Note Paysage requires Python 3.6 or higher. We fix the seed of the rand num generator to ensure reproducibility.
	\item Procedure:
	\begin{itemize}
		\item Downloard a preprocessed version of the MNIST data built into Paysage
		\item If the first time using it, need to shuffle it. Necessary since we shall employ SGD-based algos which requires using small minibatches of data to compute the grad at each step. If the data ordered, then the estimates for the grads computed from the minibatches will be biased. Shuffling ensures the grad estimates ubiased (though still noisy).
		\item Create a python generator, which splits the \texttt{data} into a training and validation sets, and separates them into minibatches of size \texttt{batch\_size}. Before we begin training, we set \texttt{data} to training mode.
		\item To monitor progress of performance metrics during training, define the variable \texttt{performance} which tells Paysage to measure the reconstruction error from the validation set. \textbf{Possible metrics include}
		\begin{itemize} 
			\item The reconstruction error (used in this example)
			\item Metrics related to difference in energy of random samples from the model
			\item See \texttt{metrics.md} in Paysage documentation for a complete list.
		\end{itemize}
		\item Now move on to construct a \texttt{hopfield} model.
		\begin{itemize}
			\item Use the \texttt{Model} class and with a visible \texttt{BernoulliLayer} and a hidden \texttt{GaussianLayer}.
			\item Also standardize the mean and variance of the Gaussian layer setting them to zero and unity, respectively (the nomenclature of Paysage here is inspired by the terminology in Variational Autoencoders).
			\item We chose to train the model with the \texttt{Adam} optimizer. To ensure convergence, attenuate the \texttt{learning\_rate} hyperparameter according to a \texttt{PowerLawDecay} schedule: \texttt{learning\_rate}(t) = \texttt{initial}/(1 + \texttt{coefficient}$\times t$). It will prove convenient to define the function \texttt{Adam\_optimizer} for this purpose.
			\item Next, have to create the model. First, initialize the \texttt{model} using the \texttt{initialize} function attribute which accepts the \texttt{data} as a required argument. We choose the initialization routine \texttt{glorot} (as mentioned in the pracitcal tricks section in this chapter).
			\item Second, define an optimizer calling the func \texttt{Adam\_optimizer}, and store the object under the name \texttt{opt}. 
			\item To create an MCMC \texttt{sampler}, we use the method \texttt{from\_batch} of the \texttt{SequentialMC} class, passing the \texttt{model} and the \texttt{data}. 
			\item Last, create an \texttt{SGD} object called \texttt{trainer} to train the model using Persistent Contrastive Divergence (\texttt{pcd}) with a fixed number of \texttt{monte\_carlo\_steps}.
			\item We can also \texttt{monitor} the reconstruction error during training.
			\item Last, we train the model in epochs (cf. variable \texttt{num\_epochs}), calling the \texttt{train} method of \texttt{trainer}.
			\item These steps=universal for shallow generative \texttt{models}, and it's convenient to combine them in the func \texttt{train\_model}, which we shall use repeatedly.
		\end{itemize}
		\item Can easily build a Bernoulli RBM and train it using the funcs defined above as follows. (shows code.)
		\item Constructing a Bernoulli RBM with $L_1$ regularization also straightforward, using the \texttt{add\_penalty} method which accepts a dictionary as input. Some layers may have multiple properties (such as the location and scale parameters of a Gaussian layer) so the dictionary key specifies to which property the penalty should be applied.
		\item To define a DBM, 
		\begin{itemize}
			\item Just add more layers, and an $L_1$ penalty for every layer.
			\item Recalling the essential trick with layer-wise pre-training to prepare the weights of the DBM, we define a \texttt{pretrainer} as an object of the \texttt{LayerwisePretrain} class (see code snippet below). This results in a slight modifiaction of the function \texttt{train\_model}. 
		\end{itemize}
	\end{itemize}
	\item Having trained our models, let's see how they perform by computing some reconstructions and fantasy particles from the validation data.
	\begin{itemize}
		\item Recall: a reconstruction $\bm{v}'$ of a given data point $\bm{x}$ is computed in two steps:
		\begin{itemize}
			\item Fix the visible layer $\bm{v} = \bm{x}$ to be the data, use MCMC sampling to find the state of the hidden layer $\bm{h}$ which maximizes the prob dist $p(\bm{h}|\bm{v})$.
			\item Fixing the same obtained state $\bm{h}$, we find the reconstruction $\bm{v}'$ of the original data point which maximizes the prob $p(\bm{v}' |\bm{h})$.
		\end{itemize}
		in the case of a DBM, the forward pass continues until we reach at the last of the hidden layers, and the backward pass goes in reverse.
		\item A config sampled from an RBM needs to specify the values of both the visible and hidden units. Since the data only specify the visible units, we need to initialize some hidden values. The visible and hidden units stored in a \texttt{State} object. 
		\item To compute reconstructions, we define an MCMC \texttt{sampler} based on the trained \texttt{model}. The starting point for the MCMC sampler is set using the \texttt{set\_state()} method.
		\item To compute reconstructions, need to keep the prob dist learned by the generative \texttt{model} fixed which is done by the help of the \texttt{deterministic\_iteration} function method, that  takes in its first argument the number of passes (\texttt{1} for a single $\bm{v}\rightarrow \bm{h} \rightarrow \bm{v}'$ pass), and the state of the sampler \texttt{sampler.state} as required arguments. 
		\item Can combine these steps in the func \texttt{compute\_reconstructions}.
		\item See fig 60 for results.
	\end{itemize}
	\item Once we have the trained models ready, can use MCMC to draw samples from the corresponding prob dist, called "fantasy particles". To this end, let's
	\begin{itemize}
		\item Draw a \texttt{random\_sample} from the validation data and compute the \texttt{model\_state}. 
		\item Next, we define an MCMC \texttt{sampler} based on the \texttt{model}, and set its state to \texttt{model\_state}.
		\item To compute fantasy particles, we do layer-wise Gibbs sampling for a total of \texttt{n\_steps} equilibration steps.
		\item The last step (controlled by the boolean \texttt{mean\_field}) is a final mean-field iteration (see tricks discussed in \textbf{Hinton REF}).
		\item Result in fig 64.
	\end{itemize}
	\item One can use generative models to reduce noise in images (\textbf{de-noising}). Let's randomly flip a fraction, \texttt{fraction\_to\_flip}, of the black\&white bits in the validation data, and use the models defined above to reconstruct (de-noise) the digit images. Result in fig 65.
\end{itemize}

\subsection{Example: Using Paysage for the Ising Model}
\begin{itemize}
	\item Can also use Paysage to analyze the 2D Ising data. 
	\begin{itemize}
		\item In prev sections, we used our knowledge of the critical point at $T_c/J \approx 2.26$ to label the spin configs and study the problem of classifying the states according to their phase of matter.
		\item But, in more complicated models, where the precise position of $T_c$ is not known, one cannot label the states with such an accuracy, if at all. 
		\item As we explained, \textbf{generative models can be used to learn a variational approximation for the probability distribution that generated the data points}.
		\item By using only the 2D spin configs, we now want to train a Bernoulli RBM, the fantasy particles of which are thermal Ising configs. 
		\item Unlike previous studies of the Ising dataset, here we perform the analysis at a fixed temperature $T$. We can then apply our model at three different values $T=1.75, 2.25, 2.75$ in the ordered, critical and disordered regions, repsectively.
		\item Define a DBM with two hidden layers of $N_{hidden}$ and $N_{hidden}/10$ units, respectively, and apply $L_1$ regularization to all weights.
		\item As in the MNIST problem above, we apply layer-wise pre-training, and deploy Persistent Contrastive Divergence to train the DBM using ADAM.
		\item One lesson from this problem=similar to real-life problems, this task is computationally intensive. The training time on present-day laptops easily exceeds that of previous studies from this review. $\rightarrow$ encourage the reader to try GPU-based training and study the resulting speed-up.
	\end{itemize}
	\item See figs 66, 67, 68 for the results of the numerical experiment at $T/J = 1.75, 2.25, 2.75$ respectively, for a DBM with $N_{hidden} = 800$. Looking at the reconstructions and the fantasy particles, we see that our DBM works well in the disordered and critical regions. But, the chosen layer architecture not optimal for the ordered phase.
\end{itemize}

\subsection{Generative models in physics}
\begin{itemize}
	\item Examples of generative models studied in physics:
	\begin{itemize}
		\item In biophysics, dynamic Boltzmann dists have been used as effective models in chemical kinetics.
		\item In stat phys, they were used to identify the criticality in the Ising model.
		\item In parallel, tools from stat phys have been applied to analyze the learning ability of RBMs \textbf{REF}, characterizing the sparsity of the weights, the effective temperature, the nonlinearities in the activation functions of hidden units, and the adaptation of fields maintaining the activity in the visible layer (?? what does this last thing mean?).
		\item Spin glass theory motivated a deterministic framework for the training, evaluation, and use of RBMs;
		\item It was demonstrated that the training process in RBMs itself exhibits phase transitions
		\item Learning in RBMs was studied in the context of nonequilibrium thermodynamics and spectral dynamics
		\item Mean-field theory found application in analyzing DBMs \textbf{REF}
		\item Using generative models to improve Monte Carlo algos
		\item Ideas from quantum mechanics have been put forward to introduce improved speed-up in certain parts of the learning algos for Helmholtz machines
		\item \textbf{WATCH OUT VILDE:}
		\item Generative models have applications in the study of quantum systems too. Most notably, RBM-inspired variational asatzes were used to learn the prob dist associated with the absolute square of a quantum state \textbf{REF}, and
		\item In this context, RBMs are sometimes called Born machines \textbf{REF}.
		\item Further applications include the detection of order in low-energy product states, and 
		\item Learning Einstein-Podolsky-Rosen correlations on an RBM.
		\item Inspired by the success of tensor networks in physics, the latter have been used as a basis for RBMs to extract the spatial geometry from entangelement \textbf{REF}, and
		\item Generative models based on matrix product states have been developed.
		\item Quantum entaglement was studied using RBM-encoded states \textbf{REF} and
		\item Tensor product based generative models have been used to understand MNIST and other ML datasets.
	\end{itemize}
\end{itemize}


\section{Variational AutoEncoders (VAEs) and Generative Adversarial Networks (GANs)}
\begin{itemize}
	\item We previously considered energy-based generative models. Here, we extend to two new gernative model frameworks that have gained wide appeal:
	\begin{itemize}
		\item Generative adversarial networks (GANs)
		\item Variational autoencoders (VAEs)
	\end{itemize}
	\item Unlike energy-based models, both these frameworks are based on differentiable neural networks and consequently can be trained using \textbf{backprop} methods. VAEs in particular can be easily implemented and trained using high-level packages such as Keras making them an easy-to-deploy generative framework.
	\item They also differ from energy-based models in that they \textbf{don't directly seek to maximize likelihood}. Fex: GANs employ a novel cost func based on adversarial learning. 
	\item VAEs and GANs are starting to make their way into physics (refs). More generally they've found important applications in many artistic and image manipulation tasks.
\end{itemize}

\subsection{The limitations of maximizing Likelihood}
\begin{itemize}
	\item The Kullback-Leibler (KL)-divergence plays a central role in many generative models. Developing an intuition about KL-divergences = one of the keys to understanding why adversarial learning has proved such a powerful method for generative modeling.
	\item The KL divergence measures the similarity between two prob dists $p(\bm{x})$ and $q(\bm{x})$. Strictly speaking it's \textbf{not a metric because it is not symmetric and does not satisfy the triangle inequality}.
	\item Given two distributions, there are two distinct KL-divergences we can construct:
	\begin{align}
		D_{KL} (p||q) =& \int \text{d} \bm{x} p(\bm{x}) \text{log} \frac{p(\bm{x})}{q(\bm{x})} \\
		D_{KL} (q||p) =& \int \text{d} \bm{x} q(\bm{x}) \text{log} \frac{q(\bm{x})}{p(\bm{x})}
	\end{align}
	A related quantity called the Jensen-Shannon divergence,
	\begin{align}
		JS(p, q) = \frac{1}{2} [D_{KL}(p|| \frac{p+q}{2}) + D_{KL}(q||\frac{p+q}{2})]
	\end{align}
	does satisfy all the properties of a squared metric (i.e. the square root of the JS divergence is a metric).
	\item An important property of the KL-divergence we'll make use of repeatedly is its positivity: $D_{KL} (p||q) \geq 0$ with equality iff $p(\bm{x}) = q(\bm{x})$ almost everywhere.
	\item In generative models in ML, the two dists we're usually concerned with = the model dist $p_\theta (\bm{x})$ and the data dist $p_{data} (\bm{x})$. 
	\item We of course want these models to be as similar as possible. But, \textbf{there are many subtelties about how we measure similarities that can have large consequences for the behaviour of training procedures}.
	\item Maximizing the log-likelihood of the data under the model = minimizing the KL divergence between the data dist and the model dist $D_{KL}(p_{data}|| p_\theta)$. To see this, we can rewrite the KL divergence as:
	\begin{align}
		D_{KL} (p_{data}||p_\theta) =& \int \text{d} \bm{x} p_{data}(\bm{x}) \text{log} p_{data} (\bm{x}) \\
		&- \int \text{d} \bm{x} p_{data}(\bm{x}) \text{log} p_\theta (\bm{x}) \\
		=& - S[p_{data}] - \langle \text{log} p_\theta (\bm{x}) \rangle_{data}
	\end{align}
	Rearranging this eq, we have
	\begin{align}
		\langle \text{log} p_\theta (\bm{x}) \rangle_{data} = - S[p_{data}] - D_{KL} (p_{data}||p_\theta)
	\end{align}
	(hva er S her?? JS divergence?? No dont think so, thats what GANs use) The equivalence follows from the positivity of KL-divergence and the fact that the entropy of the data dist is constant. 
	\item In contrast, the original formulation of GANs minimizes \textbf{an upper bound on the Jensen-Shannon divergence} between the model dist $p_\theta(\bm{x})$ and the data dist $p_{data} (\bm{x})$. 
	\item This difference in objectives underlies the difference in behavior between GANs and likelihood based generative models. To see this, we can compare the behavior  of the two KL-divergences $D_{KL}(p_{data}|| p_\theta)$ and $D_{KL}(p_\theta || p_{data})$. As illustrated in fig 69 and 70 (\textbf{useful}), though both of these KL-divergences measure similarities between the two dists, they are sensitive to very different things.
	\begin{itemize}
		\item $D_{KL}(p_\theta || p_{data})$ is insensitive to setting $p_\theta \approx 0$ even when $p_{data} \neq 0$,  
		\item Whereas $D_{KL}(p_{data}|| p_\theta)$ punishes this harshly.
		\item In contrast, $D_{KL}(p_{data}|| p_\theta)$ prefers models that have a high prob in regions with lots of training data points 
		\item Whereas $D_{KL}(p_\theta || p_{data})$ punishes models for putting high prob where there is no data.
	\end{itemize} 
	\item In the context of the above discussion, this suggests that the way likelihood-based methods are most likely to fail, is by improperly "filling in" any low-prob density regions between peaks in the data dist.
	\item In contrast, at least in principle, the Jensen-Shannon dist which underlies GANs is sensitive to both 
	\begin{itemize}
		\item Placing weight where there is data since it has information about $D_{KL}(p_{data}|| p_\theta)$ and
		\item Not placing weight where no data has been observed (i.e. low-prob density regions) since it has info about $D_{KL}(p_\theta || p_{data})$.
	\end{itemize}
	\item In practice:
	\begin{itemize}
		\item $D_{KL}(p_{data}|| p_\theta)$ can be calc easily directly from the data using sampling
		\item $D_{KL}(p_\theta || p_{data})$ is impossible to compute since we don't know $p_{data} (\bm{x})$. In particular, this integral cannot be calc using sampling since we cannot evaluate $p_{data}(\bm{x})$ at the locations of the fantasy particles. 
	\end{itemize} 
	\item The idea of adversarial learning - circumnavigate this difficulty by using an adversarial learning procedure. 
	\item Recall, $D_{KL}(p_\theta || p_{data})$ large when the model artificially over-weighs low-density regions near real peaks. Adversarial learning accomplishes this same task by teaching a discriminator network to distinguish between real data points and samples generated from the model.
	\item By punishing the model for generating points that can be easily discriminated from the data, arversarial learning decreases the weight of regions in the model space that are frar away data points - regions that inevitably arise when maximizing likelihood.
	\item This core intuition implicitly underlies many adversarial training algos (though it has been recently suggested this may not be the entire story.)
\end{itemize}

\subsection{Generative models and adversarial learning}

\subsection{Variational Autoencoders (VAEs)}
\subsubsection{VAEs as variational models}
\subsubsection{Training via the reparametrization trick}
\subsubsection{Connection to the information bottleneck}

\subsection{VAE with Gaussian latent variables and Gaussian encoder}
\subsubsection{Implementing the Gaussian VAE}
\subsubsection{VAEs for the MNIST dataset}
\subsubsection{VAEs for the 2D Ising model}



\section{Outlook}
\subsection{Research at the intersection of physics and ML}
\subsection{Topics not covered in review}
\subsection{Rebranding Machine Learning as "Artifical Intelligence"}
\subsection{Social Implications of Machine Learning}

\end{document}