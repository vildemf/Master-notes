\relax 
\bibstyle{plain}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Why is machine learning difficult?}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Basics of statistical learning theory}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient descent and its generalizations}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Overview of Bayesian inference}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Bayes rule}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Bayesian decisions}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameters}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Linear regression}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Least-square regression}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ridge-regression}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}LASSO and sparse regression}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Using linear regression to learn the Ising Hamiltonian}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Convexity of a regularizer}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Bayesian formulation of linear regression}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7}Recap and a general perspective on regularizers}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Logistic regression}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}The cross-entropy as a cost function for logistic regression}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Minimizing the cross entropy}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Examples of binary classification}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Identifying the phases of the 2D Ising model}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}SUSY}{35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Softmax regression}{38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}An example of SoftMax classification: MNIST digit classification}{38}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Combining models}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Revisiting the bias-variance tradeoff for ensembles}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Bias-variance decomposition for ensembles}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Summarizing the theory and intuitions behind ensembles}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Bagging}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Boosting}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Random forests}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5}Gradient boosted trees and XGBoost}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.6}Application to the Ising model and Supersymmetry Datasets}{52}}
\@writefile{toc}{\contentsline {section}{\numberline {9}An introduction to Feed-Forward Deep Neural Networks (DNNs)}{54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Neural network basics}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}The basic building block: neurons}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Layering neurons to build deep networks: network architecture}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Training deep networks}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}High-level specification of a neural network using Keras}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}The backpropagation algorithm}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Deriving and implementing the backpropagation equations}{63}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.2}Computing gradients in deep networks: what can go wrong with backprop?}{64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Regularizing neural networks and other practical considerations}{65}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.1}Implicit regularization using SGD: initialization, hyper-parameter tuning, and early stopping}{66}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.2}Dropout}{67}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.5.3}Batch Normalization}{67}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Deep neural networks in practice: examples}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.1}Deep learning packages}{68}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.2}Approaching the learning problem}{69}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.3}SUSY dataset}{71}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.6.4}Phases of the 2D Ising model}{71}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Convolutional Neural Networks (CNN)}{72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}The structure of convolutional neural networks}{73}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Example: CNNs for the 2D Ising model}{74}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Pre-trained CNNs and transfer learning}{75}}
\@writefile{toc}{\contentsline {section}{\numberline {11}High-level concepts in Deep Neural Networks}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Organizing deep learning workflows using the bias-variance tradeoff}{76}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Why neural networks are so successful: three high-level perspectives on neural networks}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}Neural networks as representation learning}{78}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2}Neural networks can exploit large amounts of data}{79}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.3}Neural networks scale up well computationally}{80}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Limitations of supervised learning with deep networks}{80}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Dimensional reduction and data visualization}{81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Some of the challenges of high-dimensional data}{82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Principal component analysis (PCA)}{83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Multidimensional scaling}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}t-SNE}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Clustering}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Practical clustering methods}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.1}K-means}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.2}Hierarchical clustering: Agglomerative methods}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.3}Density-based (DB) clustering}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Clustering and latent variables via the Gaussian mixture models}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Clustering in high-dimension}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Variational methods and mean-field theory (MFT)}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Variational mean-field theory for the Ising model}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Expectation Maximation (EM)}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Energy based models: Maximium Entropy (MaxEnt) Principle, Generative models, and Boltzmann Learning}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}An overview of energy-based generative models}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Maximum entropy models: the simplest energy-based generative models}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.2.1}MaxEnt models in statistical mechanics}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.2.2}From statistical mechanics to machine learning}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.2.3}Generalized Ising Models from MaxEnt}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Cost functions for training energy-based models}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.3.1}Maximum likelihood}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.3.2}Regularization}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Computing gradients}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5}Summary of the training procedure}{86}}
\@writefile{toc}{\contentsline {section}{\numberline {16}Deep generative models: Latent variables and Restricted Boltzmann Machines (RBMs)}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1}Why hidden (latent) variables?}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.2}Restricted Boltzmann machines (RBMs)}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.3}Training RBMs}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.3.1}Gibbs sampling and contrastive divergence (CD)}{86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {16.3.2}Practical considerations}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.4}Deep Boltzmann Machine}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.5}Example: Using Paysage for MNIST}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.6}Example: Using Paysage for the Ising Model}{86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.7}Generative models in physics}{86}}
